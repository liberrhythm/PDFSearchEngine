Mixing Time Estimation in Reversible Markov
Chains from a Single Sample Path

Daniel Hsu
Columbia University

Aryeh Kontorovich
Ben-Gurion University

Csaba SzepesvaÌri
University of Alberta

djhsu@cs.columbia.edu

karyeh@cs.bgu.ac.il

szepesva@cs.ualberta.ca

Abstract
This article provides the first procedure for computing a fully data-dependent interval that traps the mixing time tmix of a finite reversible ergodic Markov chain at
a prescribed confidence level. The interval is computed from a single finite-length
sample path from the Markov chain, and does not require the knowledge of any
parameters of the chain. This stands in contrast to previous approaches, which either only provide point estimates, or require a reset mechanism, or additional prior
knowledge. The interval is constructed around the relaxation time trelax , which is
strongly related
âˆš to the mixing time, and the width of the interval converges to zero
roughly at a n rate, where n is the length of the sample path. Upper and lower
bounds are given on the number of samples required to achieve constant-factor
multiplicative accuracy. The lower bounds indicate that, unless further restrictions are placed on the chain, no procedure can achieve this accuracy level before
seeing each state at least â„¦(trelax ) times on the average. Finally, future directions
of research are identified.

1

Introduction

This work tackles the challenge of constructing fully empirical bounds on the mixing time of
Markov chains based on a single sample path. Let (Xt )t=1,2,... be an irreducible, aperiodic timehomogeneous Markov chain on a finite state space [d] := {1, 2, . . . , d} with transition matrix P .
Under this assumption, the chain converges to its unique stationary distribution Ï€ = (Ï€i )di=1 regardless of the initial state distribution q:
lim Prq (Xt = i) = lim (qP t )i = Ï€i

tâ†’âˆ

tâ†’âˆ

for each i âˆˆ [d].

The mixing time tmix of the Markov chain is the number of time steps required for the chain to be
within a fixed threshold of its stationary distribution:


tmix := min t âˆˆ N : sup max |Prq (Xt âˆˆ A) âˆ’ Ï€(A)| â‰¤ 1/4 .
(1)
q

AâŠ‚[d]

P

Here, Ï€(A) = iâˆˆA Ï€i is the probability assigned to set A by Ï€, and the supremum is over all
possible initial distributions q. The problem studied in this work is the construction of a non-trivial
confidence interval Cn = Cn (X1 , X2 , . . . , Xn , Î´) âŠ‚ [0, âˆ], based only on the observed sample
path (X1 , X2 , . . . , Xn ) and Î´ âˆˆ (0, 1), that succeeds with probability 1 âˆ’ Î´ in trapping the value of
the mixing time tmix .
This problem is motivated by the numerous scientificPapplications and machine learning tasks in
which the quantity of interest is the mean Ï€(f ) =
i Ï€i f (i) for some function f of the states
of a Markov chain. This is the setting of the celebrated Markov Chain Monte Carlo (MCMC)
paradigm [1], but the problem also arises in performance prediction involving time-correlated data,
as is common in reinforcement learning [2]. Observable bounds on mixing times are useful in the
1

design and diagnostics of these methods; they yield effective approaches to assessing the estimation
quality, even when a priori knowledge of the mixing time or correlation structure is unavailable.
Main results. We develop the first procedure for constructing non-trivial and fully empirical confidence intervals for Markov mixing time. Consider a reversible ergodic Markov chain on d states
with absolute spectral gap Î³â‹† and stationary distribution minorized by Ï€â‹† . As is well-known [3,
Theorems 12.3 and 12.4],
(trelax âˆ’ 1) ln 2 â‰¤ tmix â‰¤ trelax ln

4
Ï€â‹†

(2)

where trelax := 1/Î³â‹† is the relaxation time. Hence, it suffices to estimate Î³â‹† and Ï€â‹† . Our main
results are summarized as follows.
1. In Section 3.1, we show that in some problems n = â„¦((d log d)/Î³â‹† + 1/Ï€â‹† ) observations
are necessary for any procedure to guarantee constant multiplicative accuracy in estimating
Î³â‹† (Theorems 1 and 2). Essentially, in some problems every state may need to be visited
about log(d)/Î³â‹† times, on average, before an accurate estimate of the mixing time can be
provided, regardless of the actual estimation procedure used.
2. In Section 3.2, we give a point-estimator for Î³â‹† , and prove in Theorem 3 that it achieves
multiplicative accuracy from a single sample path of length OÌƒ(1/(Ï€â‹† Î³â‹†3 )).1 We also provide a point-estimator for Ï€â‹† that requires a sample path of length OÌƒ(1/(Ï€â‹† Î³â‹† )). This
establishes the feasibility of estimating the mixing time in this setting. However, the valid
confidence intervals suggested by Theorem 3 depend on the unknown quantities Ï€â‹† and
Î³â‹† . We also discuss the importance of reversibility, and some possible extensions to nonreversible chains.
3. In Section 4, the construction of valid fully empirical confidence intervals for Ï€â‹† and Î³â‹†
are considered. First, the difficulty of the task is explained, i.e., why the standard approach
of turning the finite time confidence intervals of Theorem 3 into a fully empirical one fails.
Combining several results from perturbation theory in a novel fashion we propose a new
procedure and prove that it avoids slow convergence (Theorem 4). We also explain how
to combine the empirical confidence intervals from Algorithm 1 with the non-empirical
bounds from Theorem 3 to produce valid empirical confidence intervals. We prove in
Theorem 5 that the width of these new intervals converge to zero asymptotically at least as
fast as those from either Theorem 3 and Theorem 4.
Related work. There is a vast statistical literature on estimation in Markov chains. For instance, it
is known that under the assumptions
Pn on (Xt )t from above, the law of large numbers guarantees that
the sample mean Ï€ n (f ) := n1 t=1 f (Xt ) converges almost surely to Ï€(f ) [4], while the central
âˆš
limit theorem tells us that as n â†’ âˆ, the distribution of the deviation n(Ï€ n (f ) âˆ’ Ï€(f )) will be
normal with mean zero and asymptotic variance limnâ†’âˆ n Var (Ï€ n (f )) [5].
Although these asymptotic results help us understand the limiting behavior of the sample mean
over a Markov chain, they say little about the finite-time non-asymptotic behavior, which is often
needed for the prudent evaluation of a method or even its algorithmic design [6â€“13]. To address
this need, numerous works have developed Chernoff-type bounds on Pr(|Ï€ n (f ) âˆ’ Ï€(f )| > Ç«), thus
providing valuable tools for non-asymptotic probabilistic analysis [6, 14â€“16]. These probability
bounds are larger than corresponding bounds for independent and identically distributed (iid) data
due to the temporal dependence; intuitively, for the Markov chain to yield a fresh draw Xtâ€² that
behaves as if it was independent of Xt , one must wait Î˜(tmix ) time steps. Note that the bounds
generally depend on distribution-specific properties of the Markov chain (e.g., P , tmix , Î³â‹† ), which
are often unknown a priori in practice. Consequently, much effort has been put towards estimating
these unknown quantities, especially in the context of MCMC diagnostics, in order to provide datadependent assessments of estimation accuracy [e.g., 11, 12, 17â€“19]. However, these approaches
generally only provide asymptotic guarantees, and hence fall short of our goal of empirical bounds
that are valid with any finite-length sample path.
Learning with dependent data is another main motivation to our work. Many results from statistical learning and empirical process theory have been extended to sufficiently fast mixing, dependent
1

The OÌƒ(Â·) notation suppresses logarithmic factors.

2

data [e.g., 20â€“26], providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some
cases [27]. However, the convergence rates of the estimates from [27], which are needed to derive
confidence bounds, are given in terms of unknown mixing coefficients. When the data comes from a
Markov chain, these mixing coefficients can often be bounded in terms of mixing times, and hence
our main results provide a way to make them fully empirical, at least in the limited setting we study.
It is possible to eliminate many of the difficulties presented above when allowed more flexible access
to the Markov chain. For example, given a sampling oracle that generates independent transitions
from any given state (akin to a â€œresetâ€ device), the mixing time becomes an efficiently testable
property in the sense studied in [28, 29]. On the other hand, when one only has a circuit-based
description of the transition probabilities of a Markov chain over an exponentially-large state space,
there are complexity-theoretic barriers for many MCMC diagnostic problems [30].

2
2.1

Preliminaries
Notations

We denote the set of positive integers by N, and the set of the first d positive integers {1, 2, . . . , d}
by [d]. The non-negative part of a real number x is [x]+ := max{0, x}, and âŒˆxâŒ‰+ := max{0, âŒˆxâŒ‰}.
We use ln(Â·) for natural logarithm, and log(Â·) for logarithm with an arbitrary constant base. Boldface symbols are used for vectors and matrices (e.g., v, M ), and their entries are referenced by
subindexing (e.g., vi , Mi,j ). For a vector v, kvk denotes its Euclidean norm; for a matrix M , kM k
denotes its spectral norm. We use Diag(v) to denote the diagonal matrix whose (i, i)-th entry is vi .
Pd
The probability simplex is denoted by âˆ†dâˆ’1 = {p âˆˆ [0, 1]d : i=1 pi = 1}, and we regard vectors
in âˆ†dâˆ’1 as row vectors.
2.2

Setting

Let P âˆˆ (âˆ†dâˆ’1 )d âŠ‚ [0, 1]dÃ—d be a d Ã— d row-stochastic matrix for an ergodic (i.e., irreducible
and aperiodic) Markov chain. This implies there is a unique stationary distribution Ï€ âˆˆ âˆ†dâˆ’1 with
Ï€i > 0 for all i âˆˆ [d] [3, Corollary 1.17]. We also assume that P is reversible (with respect to Ï€):
Ï€i Pi,j = Ï€j Pj,i ,

i, j âˆˆ [d].

(3)

The minimum stationary probability is denoted by Ï€â‹† := miniâˆˆ[d] Ï€i .
Define the matrices
M := Diag(Ï€)P

and

L := Diag(Ï€)âˆ’1/2 M Diag(Ï€)âˆ’1/2 .

The (i, j)th entry of the matrix Mi,j contains the doublet probabilities associated with P : Mi,j =
Ï€i Pi,j is the probability of seeing state i followed by state j when the chain is started from its
stationary distribution. The matrix M is symmetric on account of the reversibility of P , and hence
it follows that L is also symmetric. (We will strongly exploit the symmetry in our results.) Further,
L = Diag(Ï€)1/2 P Diag(Ï€)âˆ’1/2 , hence L and P are similar and thus their eigenvalue systems are
identical. Ergodicity and reversibility imply that the eigenvalues of L are contained in the interval
(âˆ’1, 1], and that 1 is an eigenvalue of L with multiplicity 1 [3, Lemmas 12.1 and 12.2]. Denote and
order the eigenvalues of L as
1 = Î»1 > Î»2 â‰¥ Â· Â· Â· â‰¥ Î»d > âˆ’1.

Let Î»â‹† := max{Î»2 , |Î»d |}, and define the (absolute) spectral gap to be Î³â‹† := 1âˆ’Î»â‹† , which is strictly
positive on account of ergodicity.
Let (Xt )tâˆˆN be a Markov chain whose transition probabilities are governed by P . For each t âˆˆ N,
let Ï€ (t) âˆˆ âˆ†dâˆ’1 denote the marginal distribution of Xt , so
Ï€ (t+1) = Ï€ (t) P ,

Note that the initial distribution Ï€

(1)

t âˆˆ N.

is arbitrary, and need not be the stationary distribution Ï€.

The goal is to estimate Ï€â‹† and Î³â‹† from the length n sample path (Xt )tâˆˆ[n] , and also to construct fully
empirical confidence intervals that Ï€â‹† and Î³â‹† with high probability; in particular, the construction
3

of the intervals should not depend on any unobservable quantities, including Ï€â‹† and Î³â‹† themselves.
As mentioned in the introduction, it is well-known that the mixing time of the Markov chain tmix
(defined in Eq. 1) is bounded in terms of Ï€â‹† and Î³â‹† , as shown in Eq. (2). Moreover, convergence
rates for empirical processes on Markov chain sequences are also often given in terms of mixing
coefficients that can ultimately be bounded in terms of Ï€â‹† and Î³â‹† (as we will show in the proof of
our first result). Therefore, valid confidence intervals for Ï€â‹† and Î³â‹† can be used to make these rates
fully observable.

3

Point estimation

In this section, we present lower and upper bounds on achievable rates for estimating the spectral
gap as a function of the length of the sample path n.
3.1

Lower bounds

The purpose of this section is to show lower bounds on the number of observations necessary to
achieve a fixed multiplicative (or even just additive) accuracy in estimating the spectral gap Î³â‹† . By
Eq. (2), the multiplicative accuracy lower bound for Î³â‹† gives the same lower bound for estimating
the mixing time. Our first result holds even for two state Markov chains and shows that a sequence
length of â„¦(1/Ï€â‹† ) is necessary to achieve even a constant additive accuracy in estimating Î³â‹† .
Theorem 1. Pick any Ï€Ì„ âˆˆ (0, 1/4). Consider any estimator Î³Ì‚â‹† that takes as input a random
sample path of length n â‰¤ 1/(4Ï€Ì„) from a Markov chain starting from any desired initial state
distribution. There exists a two-state ergodic and reversible Markov chain distribution with spectral
gap Î³â‹† â‰¥ 1/2 and minimum stationary probability Ï€â‹† â‰¥ Ï€Ì„ such that
Pr [|Î³Ì‚â‹† âˆ’ Î³â‹† | â‰¥ 1/8] â‰¥ 3/8.
Next, considering d state chains, we show that a sequence of length â„¦(d log(d)/Î³â‹† ) is required to
estimate Î³â‹† up to a constant multiplicative accuracy. Essentially, the sequence may have to visit all
d states at least log(d)/Î³â‹† times each, on average. This holds even if Ï€â‹† is within a factor of two of
the largest possible value of 1/d that it can take, i.e., when Ï€ is nearly uniform.
Theorem 2. There is an absolute constant c > 0 such that the following holds. Pick any positive
integer d â‰¥ 3 and any Î³Ì„ âˆˆ (0, 1/2). Consider any estimator Î³Ì‚â‹† that takes as input a random sample
path of length n < cd log(d)/Î³Ì„ from a d-state reversible Markov chain starting from any desired
initial state distribution. There is an ergodic and reversible Markov chain distribution with spectral
gap Î³â‹† âˆˆ [Î³Ì„, 2Î³Ì„] and minimum stationary probability Ï€â‹† â‰¥ 1/(2d) such that
Pr [|Î³Ì‚â‹† âˆ’ Î³â‹† | â‰¥ Î³Ì„/2] â‰¥ 1/4.
The proofs of Theorems 1 and 2 are given in Appendix A.2
3.2

A plug-in based point estimator and its accuracy

Let us now consider the problem of estimating Î³â‹† . For this, we construct a natural plug-in estimator.
Along the way, we also provide an estimator for the minimum stationary probability, allowing one
to use the bounds from Eq. (2) to trap the mixing time.
c âˆˆ [0, 1]dÃ—d and random vector Ï€Ì‚ âˆˆ âˆ†dâˆ’1 by
Define the random matrix M
ci,j := |{t âˆˆ [n âˆ’ 1] : (Xt , Xt+1 ) = (i, j)}| ,
M
nâˆ’1
|{t âˆˆ [n] : Xt = i}|
Ï€Ì‚i :=
, i âˆˆ [d] .
n
Furthermore, define
b := 1 (L
b +L
b âŠ¤)
Sym(L)
2
2

A full version of this paper, with appendices, is available on arXiv [31].

4

i, j âˆˆ [d] ,

to be the symmetrized version of the (possibly non-symmetric) matrix
b := Diag(Ï€Ì‚)âˆ’1/2 M
c Diag(Ï€Ì‚)âˆ’1/2 .
L

b Our estimator of the minimum staLet Î»Ì‚1 â‰¥ Î»Ì‚2 â‰¥ Â· Â· Â· â‰¥ Î»Ì‚d be the eigenvalues of Sym(L).
tionary probability Ï€â‹† is Ï€Ì‚â‹† := miniâˆˆ[d] Ï€Ì‚i , and our estimator of the spectral gap Î³â‹† is Î³Ì‚â‹† :=
1 âˆ’ max{Î»Ì‚2 , |Î»Ì‚d |}.
These estimators have the following accuracy guarantees:
Theorem 3. There exists an absolute constant C > 0 such that the following holds. Assume the
estimators Ï€Ì‚â‹† and Î³Ì‚â‹† described above are formed from a sample path of length n from an ergodic and
reversible Markov chain. Let Î³â‹† > 0 denote the spectral gap and Ï€â‹† > 0 the minimum stationary
probability. For any Î´ âˆˆ (0, 1), with probability at least 1 âˆ’ Î´,
ï£«s
ï£¶
log Ï€dâ‹† Î´
Ï€â‹† log Ï€dâ‹† Î´
ï£¸
|Ï€Ì‚â‹† âˆ’ Ï€â‹† | â‰¤ C ï£­
(4)
+
Î³â‹† n
Î³â‹† n
and

ï£«s

|Î³Ì‚â‹† âˆ’ Î³â‹† | â‰¤ C ï£­

log

d
Î´

Â· log

Ï€â‹† Î³ â‹† n

n
Ï€â‹† Î´

+

log

1
Î³â‹†

Î³â‹† n

ï£¶

ï£¸.

(5)

Theorem 3 implies that the sequence lengths
required
Ï€â‹† and Î³â‹† to within constant


to estimate

1
1
multiplicative factors are, respectively, OÌƒ Ï€â‹† Î³â‹† and OÌƒ Ï€â‹† Î³ 3 . By Eq. (2), the second of these is
â‹†
also a bound on the required sequence length to estimate tmix .
c and Ï€Ì‚
The proof of Theorem 3 is based on analyzing the convergence of the sample averages M
to their expectation, and then using perturbation bounds for eigenvalues to derive a bound on the
error of Î³Ì‚â‹† . However, since these averages are formed using a single sample path from a (possibly)
non-stationary Markov chain, we cannot use standard large deviation bounds; moreover applying
c would result in a significantly worse
Chernoff-type bounds for Markov chains to each entry of M
sequence length requirement, roughly a factor of d larger. Instead, we adapt probability tail bounds
for sums of independent random matrices [32] to our non-iid setting by directly applying a blocking
technique of [33] as described in the article of [20]. Due to ergodicity, the convergence rate can be
bounded without any dependence on the initial state distribution Ï€ (1) . The proof of Theorem 3 is
given in Appendix B.
Note that because the eigenvalues of L are the same as that of the transition probability matrix P ,
we could have instead opted to estimate P , say, using simple frequency estimates obtained from
b.
the sample path, and then computing the second largest eigenvalue of this empirical estimate P
In fact, this approach is a way to extend to non-reversible chains, as we would no longer rely on
the symmetry of M or L. The difficulty with this approach is that P lacks the structure required
by certain strong eigenvalue perturbation results. One could instead invoke the Ostrowski-Elsner
theorem [cf. Theorem 1.4 on Page 170 of 34], which bounds the matching distance between the
b âˆ’ P k is expected
eigenvalues of a matrix A and its perturbation A + E by O(kEk1/d ). Since kP
to be of size O(nâˆ’1/2 ), this approach will give a confidence interval for Î³â‹† whose width shrinks
at a rate of O(nâˆ’1/(2d) )â€”an exponential slow-down compared to the rate from Theorem 3. As
demonstrated through an example from [34], the dependence on the d-th root of the norm of the
perturbation cannot be avoided in general. Our approach based on estimating a symmetric matrix
affords us the use of perturbation results that exploit more structure.
Returning to the question of obtaining a fully empirical confidence interval for Î³â‹† and Ï€â‹† , we notice
that, unfortunately, Theorem 3 falls short of being directly suitable for this, at least without further
assumptions. This is because the deviation terms themselves depend inversely both on Î³â‹† and Ï€â‹† ,
and hence can never rule out 0 (or an arbitrarily small positive value) as a possibility for Î³â‹† or Ï€â‹† .3
In effect, the fact that the Markov chain could be slow mixing and the long-term frequency of some
3
Using Theorem 3, it is possible to trap Î³â‹† in the union of two empirical confidence intervalsâ€”one around
Î³Ì‚â‹† and the other around zero, both of which shrink in width as the sequence length increases.

5

Algorithm 1 Empirical confidence intervals
Input: Sample path (X1 , X2 , . . . , Xn ), confidence parameter Î´ âˆˆ (0, 1).
1: Compute state visit counts and smoothed transition probability estimates:
Ni := |{t âˆˆ [n âˆ’ 1] : Xt = i}| ,

i âˆˆ [d];

Ni,j := |{t âˆˆ [n âˆ’ 1] : (Xt , Xt+1 ) = (i, j)}| ,
2:
3:
4:
5:

Ni,j + 1/d
,
Pbi,j :=
Ni + 1

(i, j) âˆˆ [d]2 .

b # be the group inverse of A
b := I âˆ’ P
b.
Let A
b.
Let Ï€Ì‚ âˆˆ âˆ†dâˆ’1 be the unique stationary distribution for P
b Diag(Ï€Ì‚)âˆ’1/2 .
b
b := Diag(Ï€Ì‚)1/2 P
Compute eigenvalues Î»Ì‚1 â‰¥Î»Ì‚2 â‰¥ Â· Â· Â· â‰¥Î»Ì‚d of Sym(L), where L
Spectral gap estimate:
Î³Ì‚â‹† := 1 âˆ’ max{Î»Ì‚2 , |Î»Ì‚d |}.

6: Empirical bounds for |Pbi,j âˆ’Pi,j | for (i, j) âˆˆ [d]2 : c := 1.01, Ï„n,Î´ := inf{t â‰¥ 0 : 2d2 (1 +

âŒˆlogc

and

2n
âˆ’t
t âŒ‰+ )e

ï£«

bi,j := ï£¬
B
ï£­

â‰¤ Î´},

r

v
ï£¶2
s
u
u
b
b
b
(5/3)Ï„n,Î´ + |Pi,j âˆ’ 1/d| ï£·
cÏ„n,Î´ t cÏ„n,Î´
2cPi,j (1 âˆ’ Pi,j )Ï„n,Î´
+
+
+
ï£¸ .
2Ni
2Ni
Ni
Ni

7: Relative sensitivity of Ï€:

o
o
n
n
1
b# : i âˆˆ [d] : j âˆˆ [d] .
b# âˆ’ min A
max A
i,j
j,j
2
p
p
S
8: Empirical bounds for maxiâˆˆ[d] |Ï€Ì‚i âˆ’ Ï€i | and max iâˆˆ[d] {| Ï€i /Ï€Ì‚i âˆ’ 1|, | Ï€Ì‚i /Ï€i âˆ’ 1|}:
(
)
n
o
[
bÌ‚
bÌ‚
1
bi,j : (i, j) âˆˆ [d]2 ,
,
bÌ‚ := ÎºÌ‚ max B
ÏÌ‚ := max
.
2
Ï€Ì‚i [Ï€Ì‚i âˆ’ bÌ‚]+
iâˆˆ[d]
ÎºÌ‚ :=

9: Empirical bounds for |Î³Ì‚â‹† âˆ’ Î³â‹† |:

wÌ‚ := 2ÏÌ‚ + ÏÌ‚2 + (1 + 2ÏÌ‚ + ÏÌ‚2 )

X

(i,j)âˆˆ[d]2

Ï€Ì‚i 2
BÌ‚
Ï€Ì‚j i,j

!1/2

.

states could be small makes it difficult to be confident in the estimates provided by Î³Ì‚â‹† and Ï€Ì‚â‹† . This
suggests that in order to obtain fully empirical confidence intervals, we need an estimator that is not
subject to such effectsâ€”we pursue this in Section 4. Theorem 3 thus primarily serves as a point
of comparison for what is achievable in terms of estimation accuracy when one does not need to
provide empirical confidence bounds.

4

Fully empirical confidence intervals

In this section, we address the shortcoming of Theorem 3 and give fully empirical confidence intervals for the stationary probabilities and the spectral gap Î³â‹† . The main idea is to use the Markov
property to eliminate the dependence of the confidence intervals on the unknown quantities (including Ï€â‹† and Î³â‹† ). Specifically, we estimate the transition probabilities from the sample path using
simple frequency estimates: as a consequence of the Markov property, for each state, the frequency
estimates converge at a rate that depends only on the number of visits to the state, and in particular
the rate (given the visit count of the state) is independent of the mixing time of the chain.
6

As discussed in Section 3, it is possible to form a confidence interval for Î³â‹† based on the eigenvalues of an estimated transition probability matrix by appealing to the Ostrowski-Elsner theorem.
However, as explained earlier, this would lead to a slow O(nâˆ’1/(2d) ) rate. We avoid this slow rate
by using an estimate of the symmetric matrix L, so that we can use a stronger perturbation result
(namely Weylâ€™s inequality, as in the proof of Theorem 3) available for symmetric matrices.
To form an estimate of L based on an estimate of the transition probabilities, one possibility is
to estimate Ï€ using a frequency-based estimate for Ï€ as was done in Section 3, and appeal to
the relation L = Diag(Ï€)1/2 P Diag(Ï€)âˆ’1/2 to form a plug-in estimate. However, as noted in
Section 3.2, confidence intervals for the entries of Ï€ formed this way may depend on the mixing
time. Indeed, such an estimate of Ï€ does not exploit the Markov property.
We adopt a different strategy for estimating Ï€, which leads to our construction of empirical confib using smoothed frequency estimates
dence intervals, detailed in Algorithm 1. We form the matrix P
#
b
b = I âˆ’P
b (Step 2), followed by
of P (Step 1), then compute the so-called group inverse A of A
b
finding the unique stationary distribution Ï€Ì‚ of P (Step 3), this way decoupling the bound on the
b # of A
b is uniquely defined; and if P
b
accuracy of Ï€Ì‚ from the mixing time. The group inverse A
b # can
defines an ergodic chain (which is the case here due to the use of the smoothed estimates), A
be computed at the cost of inverting an (dâˆ’1)Ã—(dâˆ’1) matrix [35, Theorem 5.2].4 Further, once
b # , the unique stationary distribution Ï€Ì‚ of P
b can be read out from the last row of A
b # [35,
given A
b,
Theorem 5.3]. The group inverse is also be used to compute the sensitivity of Ï€. Based on Ï€Ì‚ and P
b
we construct the plug-in estimate L of L, and use the eigenvalues of its symmetrization to form the
estimate Î³Ì‚â‹† of the spectral gap (Steps 4 and 5). In the remaining steps, we use perturbation analyses
b ; and also to relate Î³Ì‚â‹† and Î³â‹† , viewing L as a
to relate Ï€Ì‚ and Ï€, viewing P as the perturbation of P
b Both analyses give error bounds entirely in terms of observable quantities
perturbation of Sym(L).
(e.g., ÎºÌ‚), tracing back to empirical error bounds for the smoothed frequency estimates of P .

The most computationally expensive step in Algorithm 1 is the computation of the group inverse
b # , which, as noted reduces to matrix inversion. Thus, with a standard implementation of matrix
A
inversion, the algorithmâ€™s time complexity is O(n + d3 ), while its space complexity is O(d2 ).

To state our main theorem concerning Algorithm 1, we first define Îº to be analogous to ÎºÌ‚ from
b # replaced by the group inverse A# of A := I âˆ’ P . The result is as follows.
Step 7, with A
Theorem 4. Suppose Algorithm 1 is given as input a sample path of length n from an ergodic and
reversible Markov chain and confidence parameter Î´ âˆˆ (0, 1). Let Î³â‹† > 0 denote the spectral gap,
Ï€ the unique stationary distribution, and Ï€â‹† > 0 the minimum stationary probability. Then, on an
event of probability at least 1 âˆ’ Î´,
Ï€i âˆˆ [Ï€Ì‚i âˆ’ bÌ‚, Ï€Ì‚i + bÌ‚]

for all i âˆˆ [d],

Moreover, bÌ‚ and wÌ‚ almost surely satisfy (as n â†’ âˆ)
!
r
Pi,j log log n
bÌ‚ = O
max Îº
, wÌ‚ = O
Ï€i n
(i,j)âˆˆ[d]2

Î³â‹† âˆˆ [Î³Ì‚â‹† âˆ’ wÌ‚, Î³Ì‚â‹† + wÌ‚].

and

Îº
Ï€â‹†

r

log log n
+
Ï€â‹† n

r

d log log n
Ï€â‹† n

!

.5

The proof of Theorem 4 is given in Appendix C. As mentioned above, the obstacle encountered in
We establish fully observable upper and
Theorem 3 is avoided by exploiting the Markov property.
p
lower bounds on the entries of P that converge at a n/ log log n rate using standard martingale tail
inequalities; this justifies the validity of the bounds from Step 6. Properties of the group inverse [35,
36] and eigenvalue perturbation theory [34] are used to validate the empirical bounds on Ï€i and Î³â‹†
developed in the remaining steps of the algorithm.
The first part of Theorem 4 provides valid empirical confidence intervals for each Ï€i and for Î³â‹† ,
which are simultaneously valid at confidence level Î´. The second part of Theorem 4 shows that the
4
The group inverse of a square matrix A, a special case of the Drazin inverse, is the unique matrix A#
satisfying AA# A = A, A# AA# = A# and A# A = AA# .
5
In Theorems 4 and 5, our use of big-O notation is as follows. For a random sequence (Yn )n and a (nonrandom) positive sequence (ÎµÎ¸,n )n parameterized by Î¸, we say â€œYn = O(ÎµÎ¸,n ) holds almost surely as n â†’ âˆâ€
if there is some universal constant C > 0 such that for all Î¸, lim supnâ†’âˆ Yn /ÎµÎ¸,n â‰¤ C holds almost surely.

7

width of the intervals decrease
length increases.
We show in Appendix C.5 that
 as the sequence

q
q


Pi,j log log n
log log n
d
,
wÌ‚
=
O
.
Îº â‰¤ d/Î³â‹† , and hence bÌ‚ = O max(i,j)âˆˆ[d]2 Î³dâ‹†
Ï€i n
Ï€â‹† Î³â‹†
Ï€â‹† n

It is easy to combine Theorems 3 and 4 to yield intervals whose widths shrink at least as fast
as both the non-empirical intervals from Theorem 3 and the empirical intervals from Theorem 4.
Specifically, determine lower bounds on Ï€â‹† and Î³â‹† using Algorithm 1, Ï€â‹† â‰¥ miniâˆˆ[d] [Ï€Ì‚i âˆ’ bÌ‚]+ ,
Î³â‹† â‰¥ [Î³Ì‚â‹† âˆ’ wÌ‚]+ ; then plug-in these lower bounds for Ï€â‹† and Î³â‹† in the deviation bounds in Eq. (5)
from Theorem 3. This yields a new interval centered around the estimate of Î³â‹† from Theorem 3,
and it no longer depends on unknown quantities. The interval is a valid 1 âˆ’ 2Î´ probability confidence interval for Î³â‹† , and for sufficiently large n, the width shrinks at the rate given in Eq. (5). We
can similarly construct an empirical confidence interval for Ï€â‹† using Eq. (4), which is valid on the
same 1 âˆ’ 2Î´ probability event.6 Finally, we can take the intersection of these new intervals with the
corresponding intervals from Algorithm 1. This is summarized in the following theorem, which we
prove in Appendix D.
Theorem 5. The following holds under the same conditions as Theorem 4. For any Î´ âˆˆ (0, 1),
b and Vb described above for Ï€â‹† and Î³â‹† , respectively, satisfy Ï€â‹† âˆˆ U
b and
the confidence intervals U
b
Î³â‹† âˆˆ V with probability at leastr
1 âˆ’ 2Î´. Furthermore,
the widths of these intervals almost surely
!

q

d
Ï€â‹† log Ï€â‹† Î´
log d
Î´ Â·log(n)
b | = O min
b| = O
,
|
V
,
wÌ‚
, where wÌ‚ is
satisfy (as n â†’ âˆ) |U
Î³â‹† n
Ï€â‹† Î³ â‹† n
the width from Algorithm 1.

5

Discussion

The construction used in Theorem 5 applies more generally: Given a confidence interval of the
form In = In (Î³â‹† , Ï€â‹† , Î´) for some confidence level Î´ and a fully empirical confidence set En (Î´)
for (Î³â‹† , Ï€â‹† ) for the same level, Inâ€² = En (Î´) âˆ© âˆª(Î³,Ï€)âˆˆEn (Î´) In (Î³, Ï€, Î´) is a valid fully empirical 2Î´level confidence interval whose asymptotic width matches that of In up to lower order terms under
reasonable assumptions on En and In . In particular, this suggests that future work should focus on
closing the gap between the lower and upper bounds on the accuracy of point-estimation. Another
interesting direction is to reduce the computation cost: The current cubic cost in the number of states
can be too high even when the number of states is only moderately large.
Perhaps more important, however, is to extend our results to large state space Markov chains: In
most practical applications the state space is continuous or is exponentially large in some natural parameters. As follows from our lower bounds, without further assumptions, the problem of fully data
dependent estimation of the mixing time is intractable for information theoretical reasons. Interesting directions for future work thus must consider Markov chains with specific structure. Parametric
classes of Markov chains, including but not limited to Markov chains with factored transition kernels
with a few factors, are a promising candidate for such future investigations. The results presented
here are a first step in the ambitious research agenda outlined above, and we hope that they will
serve as a point of departure for further insights in the area of fully empirical estimation of Markov
chain parameters based on a single sample path.
References
[1] J. S. Liu. Monte Carlo Strategies in Scientific Computing. Springer Series in Statistics. Springer-Verlag,
2001.
[2] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction (Adaptive Computation and
Machine Learning). A Bradford Book, 1998.
[3] D. Levin, Y. Peres, and E. Wilmer. Markov Chains and Mixing Times. AMS, 2008.
[4] S. P. Meyn and R. L. Tweedie. Markov Chains and Stochastic Stability. Springer, 1993.
[5] C. Kipnis and S. R. S. Varadhan. Central limit theorem for additive functionals of reversible markov
processes and applications to simple exclusions. Comm. Math. Phys., 104(1):1â€“19, 1986.
6
For the Ï€â‹† interval, we only plug-in lower bounds on Ï€â‹† and Î³â‹† only where these quantities appear as 1/Ï€â‹†
and 1/Î³â‹† in Eq. (4). It is then possible to â€œsolveâ€ for observable bounds on Ï€â‹† . See Appendix D for details.

8

[6] I. Kontoyiannis, L. A. Lastras-MontanÌƒo, and S. P. Meyn. Exponential bounds and stopping rules for
MCMC and general Markov chains. In VALUETOOLS, page 45, 2006.
[7] M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In ICML, pages 65â€“72, 2006.
[8] V. Mnih, Cs. SzepesvaÌri, and J.-Y. Audibert. Empirical Bernstein stopping. In ICML, pages 672â€“679,
2008.
[9] A. Maurer and M. Pontil. Empirical Bernstein bounds and sample-variance penalization. In COLT, 2009.
[10] L. Li, M. L. Littman, T. J. Walsh, and A. L. Strehl. Knows what it knows: a framework for self-aware
learning. Machine Learning, 82(3):399â€“443, 2011.
[11] J. M. Flegal and G. L. Jones. Implementing MCMC: estimating with confidence. In Handbook of Markov
chain Monte Carlo, pages 175â€“197. Chapman & Hall/CRC, 2011.
[12] B. M. Gyori and D. Paulin. Non-asymptotic confidence intervals for MCMC in practice. arXiv:1212.2016,
2014.
[13] A. Swaminathan and T. Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. In ICML, 2015.
[14] D. Gillman. A Chernoff bound for random walks on expander graphs. SIAM Journal on Computing,
27(4):1203â€“1220, 1998.
[15] C. A. LeoÌn and F. Perron. Optimal Hoeffding bounds for discrete reversible Markov chains. Annals of
Applied Probability, pages 958â€“970, 2004.
[16] D. Paulin. Concentration inequalities for Markov chains by Marton couplings and spectral methods.
Electronic Journal of Probability, 20:1â€“32, 2015.
[17] S. T. Garren and R. L. Smith. Estimating the second largest eigenvalue of a Markov transition matrix.
Bernoulli, 6:215â€“242, 2000.
[18] G. L. Jones and J. P. Hobert. Honest exploration of intractable probability distributions via markov chain
monte carlo. Statist. Sci., 16(4):312â€“334, 11 2001.
[19] Y. AtchadeÌ. Markov Chain Monte Carlo confidence intervals. Bernoulli, 2015. (to appear).
[20] B. Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals of
Probability, 22(1):94â€“116, January 1994.
[21] R. L. Karandikar and M. Vidyasagar. Rates of uniform convergence of empirical means with mixing
processes. Statistics and Probability Letters, 58(3):297â€“307, 2002.
[22] D. Gamarnik. Extension of the PAC framework to finite and countable Markov chains. IEEE Transactions
on Information Theory, 49(1):338â€“345, 2003.
[23] M. Mohri and A. Rostamizadeh. Stability bounds for non-iid processes. In NIPS, 2008.
[24] M. Mohri and A. Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes. In NIPS, 2009.
[25] I. Steinwart and A. Christmann. Fast learning from non-i.i.d. observations. In NIPS, 2009.
[26] I. Steinwart, D. Hush, and C. Scovel. Learning from dependent observations. Journal of Multivariate
Analysis, 100(1):175â€“194, 2009.
[27] D. McDonald, C. Shalizi, and M. Schervish. Estimating beta-mixing coefficients. In AISTATS, pages
516â€“524, 2011.
[28] T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing that distributions are close. In
FOCS, pages 259â€“269. IEEE, 2000.
[29] T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing closeness of discrete distributions.
Journal of the ACM (JACM), 60(1):4:2â€“4:25, 2013.
[30] N. Bhatnagar, A. Bogdanov, and E. Mossel. The computational complexity of estimating MCMC convergence time. In RANDOM, pages 424â€“435. Springer, 2011.
[31] D. Hsu, A. Kontorovich, and C. SzepesvaÌri. Mixing time estimation in reversible Markov chains from a
single sample path. CoRR, abs/1506.02903, 2015.
[32] J. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends in Machine
Learning, 2015.
[33] S. Bernstein. Sur lâ€™extension du theoreme limite du calcul des probabilites aux sommes de quantites
dependantes. Mathematische Annalen, 97:1â€“59, 1927.
[34] G. W. Stewart and J. Sun. Matrix perturbation theory. Academic Press, Boston, 1990.
[35] C. D. Meyer Jr. The role of the group generalized inverse in the theory of finite Markov chains. SIAM
Review, 17(3):443â€“464, 1975.
[36] G. Cho and C. Meyer. Comparison of perturbation bounds for the stationary distribution of a Markov
chain. Linear Algebra and its Applications, 335:137â€“150, 2001.

9

