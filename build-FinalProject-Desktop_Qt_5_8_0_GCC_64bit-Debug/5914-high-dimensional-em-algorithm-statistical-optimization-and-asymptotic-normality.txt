High Dimensional EM Algorithm:
Statistical Optimization and Asymptotic Normality⇤
Zhaoran Wang
Princeton University

Quanquan Gu
University of Virginia

Yang Ning
Princeton University

Han Liu
Princeton University

Abstract
We provide a general theory of the expectation-maximization (EM) algorithm for
inferring high dimensional latent variable models. In particular, we make two contributions: (i) For parameter estimation, we propose a novel high dimensional EM
algorithm which naturally incorporates sparsity structure into parameter estimation.
With an appropriate initialization, this algorithm converges at a geometric rate
and attains an estimator with the (near-)optimal statistical rate of convergence. (ii)
Based on the obtained estimator, we propose a new inferential procedure for testing
hypotheses for low dimensional components of high dimensional parameters. For
a broad family of statistical models, our framework establishes the first computationally feasible approach for optimal estimation and asymptotic inference in high
dimensions.

1

Introduction

The expectation-maximization (EM) algorithm [12] is the most popular approach for calculating the
maximum likelihood estimator of latent variable models. Nevertheless, due to the nonconcavity of
the likelihood function of latent variable models, the EM algorithm generally only converges to a
local maximum rather than the global one [30]. On the other hand, existing statistical guarantees
for latent variable models are only established for global optima [3]. Therefore, there exists a gap
between computation and statistics.
Significant progress has been made toward closing the gap between the local maximum attained
by the EM algorithm and the maximum likelihood estimator [2, 18, 25, 30]. In particular, [30] first
establish general sufficient conditions for the convergence of the EM algorithm. [25] further improve
this result by viewing the EM algorithm as a proximal point method applied to the Kullback-Leibler
divergence. See [18] for a detailed survey. More recently, [2] establish the first result that characterizes
explicit statistical and computational rates of convergence for the EM algorithm. They prove that,
given a suitable initialization, the EM algorithm converges at a geometric rate to a local maximum
close to the maximum likelihood estimator. All these results are established in the low dimensional
regime where the dimension d is much smaller than the sample size n.
In high dimensional regimes where the dimension d is much larger than the sample size n, there
exists no theoretical guarantee for the EM algorithm. In fact, when d
n, the maximum likelihood
estimator is in general not well defined, unless the models are carefully regularized by sparsity-type
assumptions. Furthermore, even if a regularized maximum likelihood estimator can be obtained in a
computationally tractable manner, establishing the corresponding statistical properties, especially
asymptotic normality, can still be challenging because of the existence of high dimensional nuisance
parameters. To address such a challenge, we develop a general inferential theory of the EM algorithm
for parameter estimation and uncertainty assessment of high dimensional latent variable models. In
particular, we make two contributions in this paper:
• For high dimensional parameter estimation, we propose a novel high dimensional EM algorithm by
attaching a truncation step to the expectation step (E-step) and maximization step (M-step). Such a
⇤
Research supported by NSF IIS1116730, NSF IIS1332109, NSF IIS1408910, NSF IIS1546482-BIGDATA,
NSF DMS1454377-CAREER, NIH R01GM083084, NIH R01HG06841, NIH R01MH102339, and FDA
HHSF223201000072C.

1

truncation step effectively enforces the sparsity of the attained estimator and allows us to establish
significantly improved statistical rate of convergence.
• Based upon the estimator attained by the high dimensional EM algorithm, we propose a decorrelated
score statistic for testing hypotheses related to low dimensional components of the high dimensional
parameter.
Under a unified analytic framework, we establish simultaneous statistical and computational guarantees for the proposed high dimensional EM algorithm and the respective uncertainty assessment
(t) T
procedure. Let ⇤ 2 Rd be the true parameter, s⇤ be its sparsity level and
be the iterative
t=0
solution sequence of the high dimensional EM algorithm with T being the total number of iterations.
In particular, we prove that:
• Given an appropriate initialization init with relative error upper bounded by a constant  2 (0, 1),
⇤
(t) T
i.e., init
/k ⇤ k2  , the iterative solution sequence
satisfies
2
t=0
p
(t)
⇤
t/2

·⇢
+ 2 · s⇤ · log d/n
(1.1)
2
| 1 {z }
|
{z
}
Optimization Error

Statistical Error: Optimal Rate

with high probability. Here ⇢ 2 (0, 1), and 1 , 2 are quantities that possibly depend on ⇢,  and
⇤
. As the optimization error term in (1.1)pdecreases to zero at a geometric rate with respect to
t, the overall estimation error achieves the s⇤ · log d/n statistical rate of convergence (up to an
extra factor of log n), which is (near-)minimax-optimal. See Theorem 3.4 for details.
• The proposed decorrelated score statistic is asymptotically normal. Moreover, its limiting variance
is optimal in the sense that it attains the semiparametric information bound for the low dimensional
components of interest in the presence of high dimensional nuisance parameters. See Theorem 4.6
for details.
Our framework allows two implementations of the M-step: the exact maximization versus approximate
maximization. The former one calculates the maximizer exactly, while the latter one conducts an
approximate maximization through a gradient ascent step. Our framework is quite general. We
illustrate its effectiveness by applying it to two high dimensional latent variable models, that is,
Gaussian mixture model and mixture of regression model.
Comparison with Related Work: A closely related work is by [2], which considers the low dimensional regime where d is much smaller than n. Under certain initialization conditions, theyp
prove
that the EM algorithm converges at a geometric rate to some local optimum that attains the d/n
statistical rate of convergence. They cover both maximization and gradient ascent implementations of
the M-step, and establish the consequences for the two latent variable models considered in our paper
under low dimensional settings. Our framework adopts their view of treating the EM algorithm as
a perturbed version of gradient methods. However, to handle the challenge of high dimensionality,
the key ingredient of our framework is the truncation step that enforces the sparsity structure along
the solution path. Such a truncation operation poses significant challenges for both computational
and statistical analysis. In detail, for computational analysis we need to carefully characterize the
evolution of each intermediate solution’s support and its effects on the evolution of the entire iterative
solution sequence. For statistical analysis, we need to establish a fine-grained characterization of the
entrywise statistical error, which is technically more challenging than just establishing
the `2 -norm
p
error employed by [2]. In high dimensional regimes, we p
need to establish the s⇤ · log d/n statistical
rate of convergence, which is much sharper than their d/n rate when d
n. In addition to point
estimation, we further construct hypothesis tests for latent variable models in the high dimensional
regime, which have not been established before.
High dimensionality poses significant challenges for assessing the uncertainty (e.g., testing hypotheses) of the constructed estimators. For example, [15] show that the limiting distribution of the Lasso
estimator is not Gaussian even in the low dimensional regime. A variety of approaches have been
proposed to correct the Lasso estimator to attain asymptotic normality, including the debiasing method
[13], the desparsification methods [26, 32] as well as instrumental variable-based methods [4]. Meanwhile, [16, 17, 24] propose the post-selection procedures for exact inference. In addition, several
authors propose methods based on data splitting [20, 29], stability selection [19] and `2 -confidence
sets [22]. However, these approaches mainly focus on generalized linear models rather than latent
variable models. In addition, their results heavily rely on the fact that the estimator is a global optimum
of a convex program. In comparison, our approach applies to a much broader family of statistical
models with latent structures. For these latent variable models, it is computationally infeasible to

2

obtain the global maximum of the penalized likelihood due to the nonconcavity of the likelihood
function. Unlike existing approaches, our inferential theory is developed for the estimator attained
by the proposed high dimensional EM algorithm, which is not necessarily a global optimum to any
optimization formulation.
Another line of research for the estimation of latent variable models is the tensor method, which
exploits the structures of third or higher order moments. See [1] and the references therein. However,
existing tensor methods primarily focus on the low dimensional regime where d ⌧ n. In addition,
since the high order sample moments generally have a slow statistical rate of convergence, the
estimators obtained by the tensor
p methods usually have a suboptimal statistical rate even for d ⌧ n.
For example, [9] establish the d6 /n statistical
rate of convergence for mixture of regression model,
p
which is suboptimal compared with the d/n minimax lower bound. Similarly, in high dimensional
settings, the statistical rates of convergence attained by tensor methods are significantly slower than
the statistical rate obtained in this paper.
The latent variable models considered in this paper have been well studied. Nevertheless, only a
few works establish theoretical guarantees for the EM algorithm. In particular, for Gaussian mixture
model, [10, 11] establish parameter estimation guarantees for the EM algorithm and its extensions. For
mixture of regression model, [31] establish exact parameter recovery guarantees for the EM algorithm
under a noiseless setting. For high dimensional mixture of regression model, [23] analyze the gradient
EM algorithm for the `1 -penalized log-likelihood. They establish support recovery guarantees for the
attained local optimum but have no parameter estimation guarantees. In comparison with existing
works, this paper establishes a general inferential framework for simultaneous parameter estimation
and uncertainty assessment based on a novel high dimensional EM algorithm. Our analysis provides
the first theoretical guarantee of parameter estimation and asymptotic inference in high dimensional
regimes for the EM algorithm and its applications to a broad family of latent variable models.
Notation: The matrix (p, q)-norm, i.e., k · kp,q , is obtained by taking the `p -norm of each row and
then taking the `q -norm of the obtained row norms. We use C, C 0 , . . . to denote generic constants.
Their values may vary from line to line. We will introduce more notations in §2.2.

2

Methodology

We first introduce the high dimensional EM Algorithm and then the respective inferential procedure.
As examples, we consider their applications to Gaussian mixture model and mixture of regression
model. For compactness, we defer the details to §A of the appendix. More models are included in the
longer version of this paper.
Algorithm 1 High Dimensional EM Algorithm
1: Parameter: Sparsity Parameter sb, Maximum Number of Iterations T
(0)
2: Initialization: Sbinit
supp init , sb ,
trunc init , Sbinit
supp(·, ·) and trunc(·, ·) are defined in (2.2) and (2.3)
3: For t = 0 to T 1
4:
E-step: Evaluate Qn ; (t)
5:
M-step: (t+0.5)
Mn (t)
Mn (·) is implemented as in Algorithm 2 or 3
(t+0.5)
(t+1)
6:
T-step: Sb
supp (t+0.5) , sb ,
trunc (t+0.5) , Sb(t+0.5)
7: End For
(T )
8: Output: b

Algorithm 2 Maximization Implementation of the M-step
1: Input:

(t)

, Qn

;

Output: Mn

(t)

(t)

argmax Qn

;

(t)

Algorithm 3 Gradient Ascent Implementation of the M-step
1: Input: (t) , Qn ;
2: Output: Mn (t)

2.1

(t)
(t)

Parameter: Stepsize ⌘ > 0
+ ⌘ · rQn (t) ; (t)

High Dimensional EM Algorithm

Before we introduce the proposed high dimensional EM Algorithm (Algorithm 1), we briefly review
the classical EM algorithm. Let h (y) be the probability density function of Y 2 Y, where 2 Rd is
the model parameter. For latent variable models, we assume
R that h (y) is obtained by marginalizing
over an unobserved latent variable Z 2 Z, i.e., h (y) = Z f (y, z) dz. Let k (z | y) be the density
3

of Z conditioning on the observed variable Y = y, i.e., k (z | y) = f (y, z)/h (y). We define
n Z
1X
Qn ( ; 0 ) =
k 0 (z | yi ) · log f (yi , z) dz.
(2.1)
n i=1 Z

See §B of the appendix for a detailed derivation. At the t-th iteration of the classical EM algorithm, we
evaluate Qn ; (t) at the E-step and then perform max Qn ; (t) at the M-step. The proposed
high dimensional EM algorithm (Algorithm 1) is built upon the E-step and M-step (lines 4 and 5)
of the classical EM algorithm. In addition to the exact maximization implementation of the M-step
(Algorithm 2), we allow the gradient ascent implementation of the M-step (Algorithm 3), which
performs an approximate maximization via a gradient ascent step. To handle the challenge of high
dimensionality, in line 6 of Algorithm 1 we perform a truncation step (T-step) to enforce the sparsity
structure. In detail, we define
supp( , s): The set of index j’s corresponding to the top s largest | j |’s.
(2.2)
Also, for an index set S ✓ {1, . . . , d}, we define the trunc(·, ·) function in line 6 as
⇥
⇤
trunc( , S) j = j · 1{j 2 S}.
(2.3)

Note that (t+0.5) is the output of the M-step (line 5) at the t-th iteration of the high dimensional
EM algorithm. To obtain (t+1) , the T-step (line 6) preserves the entries of (t+0.5) with the top sb
large magnitudes and sets the rest to zero. Here sb is a tuning parameter that controls the sparsity level
(line 1). By iteratively performing the E-step, M-step and T-step, the high dimensional EM algorithm
attains an sb-sparse estimator b = (T ) (line 8). Here T is the total number of iterations.
2.2

Asymptotic Inference

Notation: Let r1 Q( ; 0 ) be the gradient with respect to and r2 Q( ; 0 ) be the gradient with
respect to 0 . If there is no confusion, we simply denote rQ( ; 0 ) = r1 Q( ; 0 ) as in the previous
sections. We define the higher order derivatives in the same manner, e.g., r21,2 Q( ; 0 ) is calculated
>

by first taking derivative with respect to and then with respect to 0 . For = 1> , 2> 2 Rd with
d1
d2
and d1 + d2 = d, we use notations such as v 1 2 Rd1 and A 1 , 2 2 Rd1 ⇥d2
1 2 R , 2 2 R
to denote the corresponding subvector of v 2 Rd and the submatrix of A 2 Rd⇥d .
We aim to conduct asymptotic inference for low dimensional components of the high dimensional
parameter ⇤ . Without loss of generality, we consider a single entry of ⇤ . In particular, we assume
⇥
⇤>
⇤
= ↵⇤ , ( ⇤ )> , where ↵⇤ 2 R is the entry of interest, while ⇤ 2 Rd 1 is treated as the
nuisance parameter. In the following, we construct a high dimensional score test named decorrelated
score test. It is worth noting that, our method and theory can be easily generalized to perform statistical
inference for an arbitrary low dimensional subvector of ⇤ .
Decorrelated Score Test: For score test, we are primarily interested in testing H0 : ↵⇤ = 0, since
this null hypothesis characterizes the uncertainty in variable selection. Our method easily generalizes
to H0 : ↵⇤ = ↵0 with ↵0 6= 0. For notational simplicity, we define the following key quantity
Let

= ↵,

> >

Tn ( ) = r21,1 Qn ( ; ) + r21,2 Qn ( ; ) 2 Rd⇥d .

(2.4)

. We define the decorrelated score function Sn (·, ·) 2 R as
⇥
⇤
⇥
⇤
Sn ( , ) = r1 Qn ( ; ) ↵ w( , )> · r1 Qn ( ; ) .

Here w( , ) 2 Rd

is obtained using the following Dantzig selector [8]
⇥
⇤
⇥
⇤
w( , ) = argmin kwk1 , subject to Tn ( ) ,↵
Tn ( ) , · w

(2.5)

1

w2Rd

1

1

 ,

(2.6)

>
where > 0 is a tuning parameter. Let b = ↵
b, b > , where b is the estimator attained by the high
dimensional EM algorithm (Algorithm 1). We define the decorrelated score statistic as
⇥
⇤
p
1/2
n · Sn b0 ,
Tn b0
,
(2.7)

where b0 = 0, b

> >

⇥

↵|

, and Tn b0

⇤

↵|

⇥
= 1, w b0 ,

>⇤

⇥
· Tn b0 · 1, w b0 ,

> ⇤>

.

Here we use b0 instead of b since we are interested in the null hypothesis H0 : ↵⇤ = 0. We can also
replace b0 with b and the theoretical results will remain the same. In §4 we will prove the proposed
decorrelated score statistic in (2.7) is asymptotically N (0, 1). Consequently, the decorrelated score
4

test with significance level 2 (0, 1) takes the form
⇥
⇤
⇥
p
1/2
n · Sn b0 ,
Tn b0 ↵|
2
/
S( ) = 1

1

(1

1

/2),

(1

/2)

⇤

,

where
(·) is the inverse function of the Gaussian cumulative distribution function. If S ( ) = 1,
we reject the null hypothesis H0 : ↵⇤ = 0. The intuition of this decorrelated score test is explained
in §D of the appendix. The key theoretical observation is Theorem 2.1, which connects r1 Qn (·; ·)
in (2.5) and Tn (·) in (2.7) with the score function and Fisher information in the presence of latent
structures. Let `⇥n ( ) be the⇤log-likelihood. Its score function is r`n ( ) and the Fisher information is
I( ⇤ ) = E ⇤ r2 `n ( ⇤ ) n, where E ⇤ (·) is the expectation under the model with parameter ⇤ .
1

Theorem 2.1. For the true parameter ⇤ and any
⇥
r1 Qn ( ; ) = r`n ( )/n, and E ⇤ Tn (

2 Rd , it holds that
⇤
⇤
) = I( ⇤ ) = E

⇤

Proof. See §I.1 of the appendix for a detailed proof.

⇥

r2 ` n (

⇤

⇤
) n.

(2.8)

Based on the decorrelated score test, it is easy to establish the decorrelated Wald test, which allows
us to construct confidence intervals. For compactness we defer it to the longer version of this paper.

3

Theory of Computation and Estimation

Before we present the main results, we introduce three technical conditions, which will significantly
ease our presentation. They will be verified for specific latent variable models in §E of the appendix.
The first two conditions, proposed by [2], characterize the properties of the population version lower
bound function Q(·; ·), i.e., the expectation of Qn (·; ·) defined in (2.1). We define the respective
population version M-step as follows. For the M-step in Algorithm 2, we define
M ( ) = argmax Q( 0 ; ).
(3.1)
0

For the M-step in Algorithm 3, we define
M ( ) = + ⌘ · r1 Q( ; ),
(3.2)
where ⌘ > 0 is the stepsize in Algorithm 3. We use B to denote the basin of attraction, i.e., the local
region where the high dimensional EM algorithm enjoys desired guarantees.
Condition 3.1. We define two versions of this condition.
• Lipschitz-Gradient-1( 1 , B). For the true parameter ⇤ and any 2 B, we have
⇥
⇤
⇥
⇤
⇤
r1 Q M ( ); ⇤
r1 Q M ( );
 1·k
k2 ,
(3.3)
2
where M (·) is the population version M-step (maximization implementation) defined in (3.1).
• Lipschitz-Gradient-2( 2 , B). For the true parameter ⇤ and any 2 B, we have
r1 Q( ;

⇤

r1 Q( ; )

)

2



2

·k

⇤

(3.4)

k2 .

Condition 3.1 defines a variant of Lipschitz continuity for r1 Q(·; ·). In the sequel, we will use (3.3)
and (3.4) in the analysis of the two implementations of the M-step respectively.
Condition 3.2 Concavity-Smoothness(µ, ⌫, B). For any 1 , 2 2 B, Q(·; ⇤ ) is µ-smooth, i.e.,
Q( 1 ; ⇤ ) Q( 2 ;
and ⌫-strongly concave, i.e.,
Q(

1;

⇤

)  Q(

2;

⇤

)+(

1

2)

>

· r1 Q(

2;

⇤

)

µ/2 · k

2

2
1 k2 ,

(3.5)

⇤

)+(

1

2)

>

· r1 Q(

2;

⇤

)

⌫/2 · k

2

2
1 k2 .

(3.6)

This condition indicates that, when the second variable of Q(·; ·) is fixed to be , the function is
‘sandwiched’ between two quadratic functions. The third condition characterizes the statistical error
between the sample version and population version M-steps, i.e., Mn (·) defined in Algorithms 2 and
3, and M (·) in (3.1) and (3.2). Recall k · k0 denotes the total number of nonzero entries in a vector.
Condition 3.3 Statistical-Error(✏, , s, n, B). For any fixed 2 B with k k0  s, we have that
⇤

M ( ) Mn ( ) 1  ✏
(3.7)
holds with probability at least 1
. Here ✏ > 0 possibly depends on , sparsity level s, sample size
n, dimension d, as well as the basin of attraction B.
In (3.7) the statistical error ✏ quantifies the `1 -norm of the difference between the population version
and sample version M-steps. Particularly, we constrain the input of M (·) and Mn (·) to be s-sparse.
Such a condition is different from the one used by [2]. In detail, they quantify the statistical error
5

with the `2 -norm and do not constrain the input of M (·) and Mn (·) to be sparse. Consequently, our
subsequent statistical analysis is different from theirs. The reason we use the `1 -normp
is that, it
characterizes the more refined entrywise statistical error, which converges at a fast rate of log d/n
(possibly with extra factors depending
on specific models). In comparison, the `2 -norm statistical
p
error converges at a slow rate of d/n, which does not decrease to zero as n increases with d
n.
Furthermore, the fine-grained entrywise statistical error is crucial to our key proof for quantifying the
effects of the truncation step (line 6 of Algorithm 1) on the iterative solution sequence.
3.1

Main Results

To simplify the technical analysis of the high dimensional EM algorithm, we focus on its resampling
version, which is illustrated in Algorithm 4 in §C of the appendix.
⇤
Theorem 3.4. We define B =
:k
k2  R , where R =  · k ⇤ k2 for some  2 (0, 1).
⇤
We assume Condition Concavity-Smoothness(µ, ⌫, B) holds and init
 R/2.
2
• For the maximization implementation of the M-step (Algorithm 2), we suppose that Condition
Lipschitz-Gradient-1( 1 , B) holds with ⇢1 := 1 /⌫ 2 (0, 1) and
⌃
⌥
sb = C · max 16/(1/⇢1 1)2 , 4 · (1 + )2 /(1 )2 · s⇤ ,
(3.8)
p
p
p
p
0
2
2
⇤
sb + C / 1  · s⇤ · ✏  min (1
⇢1 ) · R, (1 ) /[2 · (1 + )] · k k2 . (3.9)
Here C 1 and C 0 > 0 are constants. Under Condition Statistical-Error(✏, /T, sb, n/T, B) we
have that, for t = 1, . . . , T ,
p
p
p
p
t/2
(t)
⇤
 ⇢1 · R + sb + C 0 / 1  · s⇤ /(1
⇢1 ) · ✏
(3.10)
2
| {z }
|
{z
}
Optimization Error

Statistical Error

holds with probability at least 1
, where C is the same constant as in (3.9).
• For the gradient ascent implementation of the M-step (Algorithm 3), we suppose that Condition
Lipschitz-Gradient-2( 2 , B) holds with ⇢2 := 1 2 · (⌫
2 )/(⌫ + µ) 2 (0, 1) and the stepsize in
Algorithm 3 is set to ⌘ = 2/(⌫ + µ). Meanwhile, we assume (3.8) and (3.9) hold with ⇢1 replaced
by ⇢2 . Under Condition Statistical-Error(✏, /T, sb, n/T, B) we have that, for t = 1, . . . , T , (3.10)
holds with probability at least 1
, in which ⇢1 is replaced with ⇢2 .
0

Proof. See §G.1 of the appendix for a detailed proof.

The assumption in (3.8) states that the sparsity parameter sb is chosen to be sufficiently large and also
of the same order as the true sparsity level s⇤ . This assumption ensures that the error incurred by the
truncation step can be upper bounded. In addition, as is shown for specific latent variable models in
§E of the appendix, the error term ✏ in Condition Statistical-Error(✏,
/T,
p
psb, n/T, B) decreases as
p
0
sample size n increases. By the assumption in (3.8),
sb + C / 1  · s⇤ is of the same order
p
⇤
as
p s . Therefore, the assumption in (3.9) suggests the sample size n is sufficiently large such that
s⇤ · ✏ is sufficiently small. These assumptions guarantee that the entire iterative solution sequence
remains within the basin of attraction B in the presence of statistical error.
Theorem 3.4 illustrates that, the upper bound of the overall estimation error can be decomposed
into two terms. The first term is the upper bound of optimization error, which decreases to zero at a
geometric rate of convergence, because we have ⇢1 , ⇢2 < 1. Meanwhile,
p
p term is the upper
pthe second
0
bound of statistical error, which does not depend on t. Since
sb + C / 1  · s⇤ is of the same
p
p
⇤
⇤
order as s , this term is proportional to s · ✏, where ✏ is the entrywise statistical error between
M (·) and Mn (·). In p
§E of the appendix we prove that, for each specific latent variable model, ✏ is
roughly of the order log d/n. (There may be extra factors attached
pto ✏ depending on each specific
model.) Therefore, the statistical error term is roughly of the order s⇤ · log d/n. Consequently, for
a sufficiently large t = T such that the optimization and statistical
p error terms in (3.10) are of the
same order, the final estimator b = (T ) attains a (near-)optimal s⇤ · log d/n (possibly with extra
factors) statistical rate. For compactness, we give the following example and defer the details to §E.
Implications for Gaussian Mixture Model: We assume y1 , . . . , yn are the n i.i.d. realizations of
Y = Z · ⇤ + V . Here Z is a Rademacher random variable, i.e., P(Z = +1) = P(Z = 1) = 1/2,
and V ⇠ N (0, 2 · Id ) is independent of Z, where is the standard deviation. Suppose that we have
k ⇤ k2 /
r, where r > 0 is a sufficiently large constant that denotes the minimum signal-to-noise
ratio. In §E of the appendix we prove that there exists some constant C > 0 such that Conditions
6

Lipschitz-Gradient-1( 1 , B) and Concavity-Smoothness(µ, ⌫, B) hold with
1

= exp

C · r2 ,

B=

µ = ⌫ = 1,

⇤

:k

with R =  · k

k2  R

⇤

k2 ,  = 1/4.

For a sufficiently large n, we have that Condition Statistical-Error(✏, , s, n, B) holds with
q⇥
⇤
✏ = C · k ⇤ k1 +
·
log d + log(2/ ) n.
p
⇤
Then the first part of Theorem 3.4 implies b

C
·
s⇤ · log d · log n/n for a sufficiently
2
p
large T , which is near-optimal with respect to the minimax lower bound s⇤ log d/n.

4

Theory of Inference

To simplify the presentation of the unified framework, we lay out several technical conditions, which
will be verified for each model. Let ⇣ EM , ⇣ G , ⇣ T and ⇣ L be four quantities that scale with s⇤ , d and n.
These conditions will be verified for specific latent variable models in §F of the appendix.
⇤
Condition 4.1 Parameter-Estimation ⇣ EM . We have b
= O ⇣ EM .
P

1

Condition 4.2 Gradient-Statistical-Error ⇣ G . We have
r1 Q n (

⇤

;

⇤

Condition 4.3 Tn (·)-Concentration ⇣ T

⇤

⇤

= OP ⇣ G .
⇥
⇤
. We have Tn ( ⇤ ) E ⇤ Tn ( ⇤ )
r1 Q(

)

;

)

1

Condition 4.4 Tn (·)-Lipschitz ⇣ L . For any , we have
Tn ( )

Tn (

⇤

)

1,1

= OP ⇣ L · k

⇤

1,1

= OP ⇣ T .

k1 .

In the sequel, we lay out an assumption on several population quantities and the sample size n. Recall
⇤
d 1
that ⇤ = [↵⇤ , ( ⇤ )> ]> , where ↵⇤⇥ 2 R is⇤ the entry of interest, while
is the nuisance
⇥
⇤2 R
⇤
(d 1)⇥(d 1)
⇤
parameter. By the notations in §2.2, I( ) , 2 R
and I( ) ,↵ 2 R(d 1)⇥1 denote
⇤
the submatrices of the Fisher information matrix I( ⇤ ) 2 Rd⇥d . We define w⇤ , s⇤w and Sw
as
⇥
⇤
⇥
⇤
1
⇤
⇤
⇤
d 1
⇤
⇤
⇤
⇤
w = I( ) , · I( ) ,↵ 2 R , sw = kw k0 , and Sw = supp(w ).
(4.1)
⇥
⇤
⇥
⇤
⇤
⇤
⇤
We define 1 I( ) and d I( ) as the largest and smallest eigenvalues of I( ), and
⇥
⇤
⇥
⇤
⇥
⇤> ⇥
⇤ 1 ⇥
⇤
I( ⇤ ) ↵| = I( ⇤ ) ↵,↵
I( ⇤ ) ,↵ · I( ⇤ ) , · I( ⇤ ) ,↵ 2 R.
(4.2)

According to (4.1) and (4.2), we can easily verify that
⇥
⇤
⇥
⇤
⇥
⇤>
I( ⇤ ) ↵| = 1, (w⇤ )> · I( ⇤ ) · 1, (w⇤ )> .
(4.3)
⇥
⇤
⇥
⇤
The following assumption ensures that d I( ⇤ ) > 0. Hence, I( ⇤ ) , in (4.1) is invertible.
⇥
⇤
⇥
⇤
Also, according to (4.3) and the fact that d I( ⇤ ) > 0, we have I( ⇤ ) ↵| > 0.
Assumption 4.5 . We impose the following assumptions.
• For positive constants ⇢max and ⇢min , we assume
⇥
⇤
⇥
⇤
⇥
⇤
⇤
⇤
⇢max
)
)
⇢min ,
I( ⇤ ) ↵| = O(1),
1 I(
d I(
• The tuning parameter

of the Dantzig selector in (2.6) is set to

⇥

I(

⇤

)

⇤

1
↵|

= O(1). (4.4)

= C · ⇣ T + ⇣ L · ⇣ EM · 1 + kw⇤ k1 ,
(4.5)
where C 1 is a sufficiently large constant. The sample size n is sufficiently large such that
p
max kw⇤ k1 , 1 · s⇤w · = o(1), ⇣ EM = o(1), s⇤w · · ⇣ G = o(1/ n),
(4.6)
p
p
2
· ⇣ EM = o(1/ n), max 1, kw⇤ k1 · ⇣ L · ⇣ EM = o(1/ n).
⇥
⇤
The assumption on d I( ⇤ ) guarantees that the Fisher information matrix is positive definite. The
p
other assumptions in (4.4) guarantee the existence of the asymptotic variance of n · Sn b0 , in
the score statistic defined in (2.7). Similar assumptions are standard in existing asymptotic inference
results. For example, for mixture of regression model, [14] impose variants of these assumptions.
For specific models, we will show that ⇣ EM , ⇣ G , ⇣ T and all decrease with n, while ⇣ L increases
with n at a slow rate. Therefore, the assumptions in (4.6) ensure that the sample size n is sufficiently
large. We will make these assumptions more explicit after we specify ⇣ EM , ⇣ G , ⇣ T and ⇣ L for each

7

model. Note the assumptions in (4.6) imply that s⇤w = kw⇤ k0 needs to be small. For instance, for
specified in (4.5), max kw⇤ k1 , 1 · s⇤w · = o(1) in (4.6) implies s⇤w · ⇣ T = o(1). In the following,
p
p
we will prove that ⇣ T is of the order log d/n. Hence, we require that s⇤w = o n/ log d ⌧ d 1,
i.e., w⇤ 2 Rd 1 is sparse. Such a sparsity
can be
as follows. According to
⇥ assumption
⇤
⇥ understood
⇤
the definition of w⇤ in (4.1), we have I( ⇤ ) , · w⇤ = I( ⇤ ) ,↵ . Therefore, such a sparsity
⇥
⇤
⇥
⇤
assumption suggests I( ⇤ ) ,↵ lies within the span of a few columns of I( ⇤ ) , . Such a sparsity
assumption on w⇤ is necessary, because otherwise it is difficult to accurately estimate w⇤ in high
dimensional regimes. In the context of high dimensional generalized linear models, [26, 32] impose
similar sparsity assumptions.
4.1

Main Results

Decorrelated Score Test: The next theorem establishes the asymptotic normality of the decorrelated
score statistic defined in (2.7).
⇥
⇤>
Theorem 4.6. We consider ⇤ = ↵⇤ , ( ⇤ )> with ↵⇤ = 0. Under Assumption 4.5 and Conditions
4.1-4.4, we have that for n ! 1,
⇥
⇤
p
1/2 D
n · Sn b0 ,
Tn b0 ↵|
! N (0, 1),
(4.7)
⇥
⇤
where b0 and Tn b0 ↵| 2 R are defined in (2.7). The limiting variance of the decorrelated score
⇥
⇤
p
function n · Sn b0 , is I( ⇤ )
, which is defined in (4.2).
↵|

Proof. See §G.2 of the appendix for a detailed proof.
⇥
⇤
Optimality: [27] prove that for inferring ↵⇤ in the presence of nuisance parameter ⇤ , I( ⇤ ) ↵| is
the semiparametric efficient information, i.e., the minimum limiting variance of the (rescaled) score
function. Our proposed decorrelated score function achieves such a semiparametric information lower
bound and is therefore in this sense optimal.
In the following, we use Gaussian mixture model to illustrate the effectiveness of Theorem 4.6. We
defer the details and the implications for mixture of regression to §F of the appendix.
Implications for Gaussian Mixture Model: Under the same model considered in §3.1, if we assume
all quantities except s⇤w , s⇤ , d and n are constant, then we have that Conditions 4.1-4.4 hold with
p
p
p
3/2
⇣ EM = s⇤ log d · log n/n, ⇣ G = log d/n, ⇣ T = log d/n and ⇣ L = log d + log n
. Thus,
under Assumption 4.5, (4.7) holds when n ! 1. Also, we can verify that (4.6) in Assumption 4.5
⇥
⇤
2
holds if max s⇤w , s⇤ · (s⇤ )2 · (log d)5 = o n/(log n)2 .

5

Conclusion

We propose a novel high dimensional EM algorithm which naturally incorporates sparsity structure.
Our theory shows that, with a suitable initialization, the proposed algorithm converges at a geometric
rate and achieves an estimator with the (near-)optimal statistical rate of convergence. Beyond point
estimation, we further propose the decorrelated score and Wald statistics for testing hypotheses and
constructing confidence intervals for low dimensional components of high dimensional parameters.
We apply the proposed algorithmic framework to a broad family of high dimensional latent variable
models. For these models, our framework establishes the first computationally feasible approach for
optimal parameter estimation and asymptotic inference under high dimensional settings.

References
[1] A N A N D K U M A R , A ., G E , R ., H S U , D ., K A K A D E , S . M . and T E L G A R S K Y , M . (2014). Tensor
decompositions for learning latent variable models. Journal of Machine Learning Research 15 2773–2832.
[2] B A L A K R I S H N A N , S ., W A I N W R I G H T , M . J . and Y U , B . (2014). Statistical guarantees for the EM
algorithm: From population to sample-based analysis. arXiv preprint arXiv:1408.2156 .
[3] B A R T H O L O M E W , D . J ., K N O T T , M . and M O U S TA K I , I . (2011). Latent variable models and
factor analysis: A unified approach, vol. 899. Wiley.
[4] B E L L O N I , A ., C H E N , D ., C H E R N O Z H U K O V , V. and H A N S E N , C . (2012). Sparse models and
methods for optimal instruments with an application to eminent domain. Econometrica 80 2369–2429.
[5] B I C K E L , P. J ., R I T O V , Y. and T S Y B A K O V , A . B . (2009). Simultaneous analysis of Lasso and
Dantzig selector. Annals of Statistics 37 1705–1732.

8

[6] B O U C H E R O N , S ., L U G O S I , G . and M A S S A R T , P. (2013). Concentration inequalities: A nonasymptotic theory of independence. Oxford University Press.
[7] C A I , T., L I U , W. and L U O , X . (2011). A constrained `1 minimization approach to sparse precision
matrix estimation. Journal of the American Statistical Association 106 594–607.
[8] C A N D È S , E . and T A O , T. (2007). The Dantzig selector: Statistical estimation when p is much larger
than n. Annals of Statistics 35 2313–2351.
[9] C H A G A N T Y , A . T. and L I A N G , P. (2013). Spectral experts for estimating mixtures of linear regressions. arXiv preprint arXiv:1306.3729 .
[10] C H A U D H U R I , K ., D A S G U P TA , S . and V AT TA N I , A . (2009). Learning mixtures of Gaussians
using the k-means algorithm. arXiv preprint arXiv:0912.0086 .
[11] D A S G U P TA , S . and S C H U L M A N , L . (2007). A probabilistic analysis of EM for mixtures of separated,
spherical Gaussians. Journal of Machine Learning Research 8 203–226.
[12] D E M P S T E R , A . P., L A I R D , N . M . and R U B I N , D . B . (1977). Maximum likelihood from
incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Statistical
Methodology) 39 1–38.
[13] J AVA N M A R D , A . and M O N TA N A R I , A . (2014). Confidence intervals and hypothesis testing for
high-dimensional regression. Journal of Machine Learning Research 15 2869–2909.
[14] K H A L I L I , A . and C H E N , J . (2007). Variables selection in finite mixture of regression models. Journal
of the American Statistical Association 102 1025–1038.
[15] K N I G H T , K . and F U , W. (2000). Asymptotics for Lasso-type estimators. Annals of Statistics 28
1356–1378.
[16] L E E , J . D ., S U N , D . L ., S U N , Y. and T AY L O R , J . E . (2013). Exact inference after model selection
via the Lasso. arXiv preprint arXiv:1311.6238 .
[17] L O C K H A R T , R ., T AY L O R , J ., T I B S H I R A N I , R . J . and T I B S H I R A N I , R . (2014). A significance
test for the Lasso. Annals of Statistics 42 413–468.
[18] M C L A C H L A N , G . and K R I S H N A N , T. (2007). The EM algorithm and extensions, vol. 382. Wiley.
[19] M E I N S H A U S E N , N . and B Ü H L M A N N , P. (2010). Stability selection. Journal of the Royal Statistical
Society: Series B (Statistical Methodology) 72 417–473.
[20] M E I N S H A U S E N , N ., M E I E R , L . and B Ü H L M A N N , P. (2009). p-values for high-dimensional
regression. Journal of the American Statistical Association 104 1671–1681.
[21] N E S T E R O V , Y. (2004). Introductory lectures on convex optimization:A basic course, vol. 87. Springer.
[22] N I C K L , R . and
41 2852–2876.

VA N D E

G E E R , S . (2013). Confidence sets in sparse regression. Annals of Statistics

[23] S T Ä D L E R , N ., B Ü H L M A N N , P. and
regression models. TEST 19 209–256.

VA N D E

G E E R , S . (2010). `1 -penalization for mixture

[24] T AY L O R , J ., L O C K H A R T , R ., T I B S H I R A N I , R . J . and T I B S H I R A N I , R . (2014). Post-selection
adaptive inference for least angle regression and the Lasso. arXiv preprint arXiv:1401.3889 .
[25] T S E N G , P. (2004). An analysis of the EM algorithm and entropy-like proximal point methods. Mathematics of Operations Research 29 27–44.
[26]

VA N D E G E E R , S ., B Ü H L M A N N , P., R I T O V , Y. and D E Z E U R E , R . (2014). On asymptotically
optimal confidence regions and tests for high-dimensional models. Annals of Statistics 42 1166–1202.

[27]

VA N D E R

VA A R T , A . W. (2000). Asymptotic statistics, vol. 3. Cambridge University Press.

[28] V E R S H Y N I N , R . (2010). Introduction to the non-asymptotic analysis of random matrices. arXiv
preprint arXiv:1011.3027 .
[29] W A S S E R M A N , L . and R O E D E R , K . (2009). High-dimensional variable selection. Annals of Statistics
37 2178–2201.
[30] W U , C . F. J . (1983). On the convergence properties of the EM algorithm. Annals of Statistics 11
95–103.
[31] Y I , X ., C A R A M A N I S , C . and S A N G H AV I , S . (2013). Alternating minimization for mixed linear
regression. arXiv preprint arXiv:1310.3745 .
[32] Z H A N G , C . - H . and Z H A N G , S . S . (2014). Confidence intervals for low dimensional parameters in
high dimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology)
76 217–242.

9

