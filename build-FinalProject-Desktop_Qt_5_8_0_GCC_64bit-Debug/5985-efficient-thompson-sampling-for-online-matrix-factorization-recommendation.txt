Efficient Thompson Sampling for Online
Matrix-Factorization Recommendation
Jaya Kawale, Hung Bui, Branislav Kveton
Adobe Research
San Jose, CA
{kawale, hubui, kveton}@adobe.com

Long Tran Thanh
University of Southampton
Southampton, UK
ltt08r@ecs.soton.ac.uk

Sanjay Chawla
Qatar Computing Research Institute, Qatar
University of Sydney, Australia
sanjay.chawla@sydney.edu.au

Abstract
Matrix factorization (MF) collaborative filtering is an effective and widely used
method in recommendation systems. However, the problem of finding an optimal
trade-off between exploration and exploitation (otherwise known as the bandit
problem), a crucial problem in collaborative filtering from cold-start, has not been
previously addressed. In this paper, we present a novel algorithm for online MF
recommendation that automatically combines finding the most relevant items with
exploring new or less-recommended items. Our approach, called Particle Thompson sampling for MF (PTS), is based on the general Thompson sampling framework, but augmented with a novel efficient online Bayesian probabilistic matrix
factorization method based on the Rao-Blackwellized particle filter. Extensive experiments in collaborative filtering using several real-world datasets demonstrate
that PTS significantly outperforms the current state-of-the-arts.

1

Introduction

Matrix factorization (MF) techniques have emerged as a powerful tool to perform collaborative
filtering in large datasets [1]. These algorithms decompose a partially-observed matrix R ∈ RN ×M
into a product of two smaller matrices, U ∈ RN ×K and V ∈ RM ×K , such that R ≈ U V T .
A variety of MF-based methods have been proposed in the literature and have been successfully
applied to various domains. Despite their promise, one of the challenges faced by these methods
is recommending when a new user/item arrives in the system, also known as the problem of coldstart. Another challenge is recommending items in an online setting and quickly adapting to the
user feedback as required by many real world applications including online advertising, serving
personalized content, link prediction and product recommendations.
In this paper, we address these two challenges in the problem of online low-rank matrix completion
by combining matrix completion with bandit algorithms. This setting was introduced in the previous
work [2] but our work is the first satisfactory solution to this problem. In a bandit setting, we
can model the problem as a repeated game where the environment chooses row i of R and the
learning agent chooses column j. The Rij value is revealed and the goal (of the learning agent) is
to minimize the cumulative regret with respect to the optimal solution, the highest entry in each row
of R. The key design principle in a bandit setting is to balance between exploration and exploitation
which solves the problem of cold start naturally. For example, in online advertising, exploration
implies presenting new ads, about which little is known and observing subsequent feedback, while
exploitation entails serving ads which are known to attract high click through rate.
1

While many solutions have been proposed for bandit problems, in the last five years or so, there
has been a renewed interest in the use of Thompson sampling (TS) which was originally proposed
in 1933 [3, 4]. In addition to having competitive empirical performance, TS is attractive due to its
conceptual simplicity. An agent has to choose an action a (column) from a set of available actions so
as to maximize the reward r, but it does not know with certainty which action is optimal. Following
TS, the agent will select a with the probability that a is the best action. Let θ denotes the unknown
parameter governing reward structure, and O1:t the history of observations currently available to the
agent. The agent chooses a∗ = a with probability
Z h
i
0
I E [r|a, θ] = max
E
[r|a
,
θ]
P (θ|O1:t )dθ
0
a

which can be implemented by simply sampling θ from the posterior P (θ|O1:t ) and let a∗ =
arg maxa0 E [r|a0 , θ]. However for many realistic scenarios (including for matrix completion), sampling from P (θ|O1:t ) is not computationally efficient and thus recourse to approximate methods is
required to make TS practical.
We propose a computationally-efficient algorithm for solving our problem, which we call Particle
Thompson sampling for matrix factorization (PTS). PTS is a combination of particle filtering for
online Bayesian parameter estimation and TS in the non-conjugate case when the posterior does
not have a closed form. Particle filtering uses a set of weighted samples (particles) to estimate
the posterior density. In order to overcome the problem of the huge parameter space, we utilize
Rao-Blackwellization and design a suitable Monte Carlo kernel to come up with a computationally
and statistically efficient way to update the set of particles as new data arrives in an online fashion.
Unlike the prior work [2] which approximates the posterior of the latent item features by a single
point estimate, our approach can maintain a much better approximation of the posterior of the latent
features by a diverse set of particles. Our results on five different real datasets show a substantial
improvement in the cumulative regret vis-a-vis other online methods.

2

Probabilistic Matrix Factorization

We first review the probabilistic matrix factorization approach to
the low-rank matrix completion problem. In matrix completion, a
portion Ro of the N × M matrix R = (rij ) is observed, and the
goal is to infer the unobserved entries of R. In probabilistic matrix
factorization (PMF) [5], R is assumed to be a noisy perturbation of
a rank-K matrix R̄ = U V > where UN ×K and VM ×K are termed
the user and item latent features (K is typically small). The full
generative model of PMF is
Ui i.i.d. ∼
Vj i.i.d. ∼

N (0, σu2 IK )

↵,

v

Vj

Ui

M

Rij

u

N (0, σv2 IK )
N (Ui> Vj , σ 2 )

(1)

N ⇥M

Figure 1: Graphical model of
probabilistic matrix factoriza2
where the variances (σ 2 , σU
, σV2 ) are the parameters of the model. tion model
We also consider a full Bayesian treatment where the variances
2
σU
and σV2 are drawn from an inverse Gamma prior (while σ 2
−2
is held fixed), i.e., λU = σU
∼ Γ(α, β); λV = σV−2 ∼ Γ(α, β) (this is a special case of the
Bayesian PMF [6] where we only consider isotropic Gaussians)1 . Given this generative model,
from the observed ratings Ro , we would like to estimate the parameters U and V which will allow us to “complete” the matrix R. PMF is a MAP point-estimate which finds U, V to maximize
Pr(U, V |Ro , σ, σU , σV ) via (stochastic) gradient ascend (alternate least square can also be used [1]).
Bayesian PMF [6] attempts to approximate the full posterior Pr(U, V |Ro , σ, α, β). The joint posterior of U and V are intractable; however, the structure of the graphical model (Fig. 1) can be
exploited to derive an efficient Gibbs sampler.
rij |U, V i.i.d. ∼

We now provide the expressions for the conditional probabilities of interest. Supposed that V and
σU are known. Then the vectors Ui are independent for each user i. Let rts(i) = {j|rij ∈ Ro } be
o
the set of items rated by user i, observe that the ratings {Rij
|j ∈ rts(i)} are generated i.i.d. from Ui
1

[6] considers the full covariance structure, but they also noted that isotropic Gaussians are effective enough.

2

following a simple conditional linear Gaussian model. Thus, the posterior of Ui has the closed form
o
, σU , σ) = N (Ui |µui , (Λui )−1 )
Pr(Ui |V, Ro , σ, σU ) = Pr(Ui |Vrts(i) , Ri,rts(i)
X
1 X
1
1
o
rij
Vj .
where µui = 2 (Λui )−1 ζiu ; Λui = 2
Vj Vj> + 2 IK ; ζiu =
σ
σ
σu

(2)
(3)

j∈rts(i)

j∈rts(i)

The conditional posterior of V , Pr(V |U, Ro , σV , σ) is similarly factorized into
QM
v −1
v
) where the mean and precision are similarly defined. The posterior
j=1 N (Vj |µj , (Λj )
−2
of the precision λU = σU
given U (and simiarly for λV ) is obtained from the conjugacy of the
Gamma prior and the isotropic Gaussian
Pr(λU |U, α, β) = Γ(λU |

NK
1
2
+ α, kU kF + β).
2
2

(4)

Although not required for Bayesian PMF, we give the likelihood expression
Pr(Rij = r|V, Ro , σU , σ) = N (r|Vj> µui ,

1
+ Vj> ΛV,i Vj ).
σ2

(5)

The advantage of the Bayesian approach is that uncertainty of the estimate of U and V are available
which is crucial for exploration in a bandit setting. However, the bandit setting requires maitaining
online estimates of the posterior as the ratings arrive over time which makes it rather awkward
for MCMC. In this paper, we instead employ a sequential Monte-Carlo (SMC) method for online
Bayesian inference [7, 8]. Similar to the Gibbs sampler [6], we exploit the above closed form updates
to design an efficient Rao-Blackwellized particle filter [9] for maintaining the posterior over time.

3

Matrix-Factorization Recommendation Bandit

In a typical deployed recommendation system, users and observed ratings (also called rewards)
arrive over time, and the task of the system is to recommend item for each user so as to maximize
the accumulated expected rewards. The bandit setting arises from the fact that the system needs to
learn over time what items have the best ratings (for a given user) to recommend, and at the same
time sufficiently explore all the items.
We formulate the matrix factorization bandit as follows. We assume that ratings are generated
following Eq. (1) with a fixed but unknown latent features (U ∗ , V ∗ ). At time t, the environment
chooses user it and the system (learning agent) needs to recommend an item jt . The user then
rates the recommended item with rating rit ,jt ∼ N (Ui∗t > Vj∗t , σ 2 ) and the agent receives this rating
as a reward. We abbreviate this as rto = rit ,jt . The system recommends item jt using a policy
o
o
that takes into account the history of the observed ratings prior to time t, r1:t−1
, where r1:t
=
o t
∗> ∗
{(ik , jk , rk )}k=1 . The highest expected reward the system can earn at time t is maxj Ui Vj , and
this is achieved if the optimal item j ∗ (i) = arg maxj Ui∗ > Vj∗ is recommended. Since (U ∗ , V ∗ )
are unknown, the optimal item j ∗ (i) is also not known a priori. The quality of the recommendation
system is measured by its expected cumulative regret:
" n
#
" n
#
X
X
o
o
∗> ∗
CR = E
[rt − rit ,j ∗ (it ) ] = E
[rt − max Uit Vj ]
(6)
t=1

t=1

j

where the expectation is taken with respect to the choice of the user at time t and also the randomness
in the choice of the recommended items by the algorithm.
3.1

Particle Thompson Sampling for Matrix Factorization Bandit

While it is difficult to optimize the cumulative regret directly, TS has been shown to work well in
practice for contextual linear bandit [3]. To use TS for matrix factorization bandit, the main difficulty
is to incrementally update the posterior of the latent features (U, V ) which control the reward structure. In this subsection, we describe an efficient Rao-Blackwellized particle filter (RBPF) designed
to exploit the specific structure of the probabilistic matrix factorization model. Let θ = (σ, α, β) be
o
the control parameters and let posterior at time t be pt = Pr(U, V, σU , σV , |r1:t
, θ). The standard
3

Algorithm 1 Particle Thompson Sampling for Matrix Factorization (PTS)
Global control params: σ, σU , σV ; for Bayesian version (PTS-B): σ, α, β
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:

p̂0 ← InitializeParticles()
Ro = ∅
for t = 1, 2 . . . do
i ← current user
Sample d ∼ p̂t−1 .w
Ṽ ← p̂t−1 .V (d)
(d)
[If PTS-B] σ̃U ← p̂t−1 .σU
o
Sample Ũi ∼ Pr(Ui |Ṽ , σ̃U , σ, r1:t−1
)
. sample new Ui due to Rao-Blackwellization
>
ĵ ← arg maxj Ũi Ṽj
Recommend ĵ for user i and observe rating r.
rto ← (i, ĵ, r)
o
p̂t ← UpdatePosterior(p̂t−1 , r1:t
)
end for
o
procedure U PDATE P OSTERIOR(p̂, r1:t
)
(d)
(d)
. p̂ has the structure (w, particles) where particles[d] = (U (d) , V (d) , σU , σV ).
o
(i, j, r) ← rt
o
o
∀d, Λui (d) ← Λui (V (d) , r1:t−1
), ζiu (d) ← ζiu (V (d) , r1:t−1
)
. see Eq. (3)
P
(d)
o
(d)
wd = 1
. Reweighting; see Eq.(5)
∀d, wd ∝ Pr(Rij = r|V , σU , σ, r1:t−1 ), see Eq.(5),
1
∀d, i ∼ p̂.w; p̂0 .particles[d] ← p̂.particles[i]; ∀d, p̂0 .wd ← D
. Resampling
for all d do
. Move
Λui (d) ← Λui (d) + σ12 Vj Vj> ; ζiu (d) ← ζiu (d) + rVj
(d)
(d)
o
)
. see Eq. (2)
p̂0 .Ui ∼ Pr(Ui |p̂0 .V (d) , p̂0 .σU , σ, r1:t
0
(d)
[If PTS-B] Update the norm of p̂ .U
o
o
Λvj (d) ← Λvj (V (d) , r1:t
), ζjv (d) ← ζiu (V (d) , r1:t
)
(d)
0
0
(d)
0 (d)
o
p̂ .Vj ∼ Pr(Vj |p̂ .U , p̂ .σV , σ, r1:t )
(d)

26:
[If PTS-B] p̂0 .σU ∼ Pr(σU |p̂0 .U (d) , α, β)
27:
end for
28:
return p̂0
29: end procedure

. see Eq.(4)

particle filter would sample all of the parameters (U, V, σU , σV ). Unfortunately, in our experiments, degeneracy is highly problematic for such a vanilla particle filter (PF) even when σU , σV
are assumed known (see Fig. 4(b)). Our RBPF algorithm maintains the posterior distribution pt as
follows. Each of the particle conceptually represents a point-mass at V, σU (U and σV are integrated
PD
1
out analytically whenever possible)2 . Thus, pt (V, σU ) is approximated by p̂t = D
(d)
d=1 δ(V (d) ,σU
)
where D is the number of particles.
Crucially, since the particle filter needs to estimate a set of non-time-vayring parameters, having
0
an effective and efficient MCMC-kernel move Kt (V 0 , σU
; V, σU ) stationary w.r.t. pt is essential.
Our design of the move kernel Kt are based on two observations. First, we can make use of
U and σV as auxiliary variables, effectively sampling U, σV |V, σU ∼ pt (U, σV |V, σU ), and then
0
0
V 0 , σU
|U, σV ∼ pt (V 0 , σU
|U, σV ). However, this move would be highly inefficient due to the number of variables that need to be sampled at each update. Our second observation is the key to an
efficient implementation. Note that latent features for all users except the current user U−it are independent of the current observed rating rto : pt (U−it |V, σU ) = pt−1 (U−it |V, σU ), therefore at time
t we only have to resample Uit as there is no need to resample U−it . Furthermore, it suffices to
resample the latent feature of the current item Vjt . This leads to an efficient implementation of the
RBPF where each particle in fact stores3 U, V, σU , σV , where (U, σV ) are auxiliary variables, and
0
for the kernel move Kt , we sample Uit |V, σU then Vj0t |U, σV and σU
|U, α, β.
The PTS algorithm is given in Algo. 1. At each time t, the complexity is O(((N̂ + M̂ )K 2 + K 3 )D)
where N̂ and M̂ are the maximum number of users who have rated the same item and the maximum
2

When there are fewer users than items, a similar strategy can be derived to integrate out U and σV instead.
This is not inconsistent with our previous statement that conceptually a particle represents only a pointmass distribution δV,σU .
3

4

number of items rated by the same user, respectively. The dependency on K 3 arises from having
to invert the precision matrix, but this is not a concern since the rank K is typically small. Line
24 can be replaced by an incremental update with caching: after line 22, we can incrementally
update Λvj and ζjv for all item j previously rated by the current user i. This reduces the complexity
to O((M̂ K 2 + K 3 )D), a potentially significant improvement in a real recommendation systems
where each user tends to rate a small number of items.

4

Analysis

We believe that the regret of PTS can be bounded. However, the existing work on TS and bandits
does not provide sufficient tools for proper
√ analysis of our algorithm. In particular, while existing
techniques can provide O(log T ) (or O( T ) for gap-independent) regret bounds for our problem,
these bounds are typically linear in the number of entries of the observation matrix R (or at least
linear in the number of users), which is typically very large, compared to T . Thus, an ideal regret
bound in our setting is the one that has sub-linear dependency (or no dependency at all) on the
number of users. A key obstacle of achieving this is that, while the conditional posteriors of U and
V are Gaussians, neither their marginal and joint posteriors belong to well behaved classes (e.g.,
conjugate posteriors, or having closed forms). Thus, novel tools, that can handle generic posteriors,
are needed for efficient analysis. Moreover, in the general setting, the correlation between Ro and
the latent features U and V are non-linear (see, e.g., [10, 11, 12] for more details). As existing
techniques are typically designed for efficiently learning linear regressions, they are not suitable for
our problem. Nevertheless, we show how to bound the regret of TS in a very specific case of n × m
rank-1 matrices, and we leave the generalization of these results for future work.
In particular, we analyze the regret of PTS in the setting of Gopalan et al. [13]. We model our
N ×1
problem as follows. The parameter space is Θu × Θv , where Θu = {d, 2d, . . . , 1}
and Θv =
M ×1
{d, 2d, . . . , 1}
are discretizations of the parameter spaces of rank-1 factors u and v for some
integer 1/d. For the sake of theoretical analysis, we assume that PTS can sample from the full
posterior. We also assume that ri,j ∼ N (u∗i vj∗ , σ 2 ) for some u∗ ∈ Θu and v ∗ ∈ Θu . Note that
in this setting, the highest-rated item in expectation is the same for all users. We denote this item
by j ∗ = arg max 1≤j≤M vj∗ and assume that it is uniquely optimal, u∗j ∗ > u∗j for any j 6= j ∗ . We
leverage these properties in our analysis. The random variable Xt at time t is a pair of a random
N,M
rating matrix Rt = {ri,j }i=1,j=1 and a random row 1 ≤ it ≤ N . The action At at time t is a
column 1 ≤ jt ≤ M . The observation is Yt = (it , rit ,jt ). We bound the regret of PTS as follows.
Theorem 1. For any δ ∈ (0, 1) and  ∈ (0, 1), there exists T ∗ such that PTS on Θu × Θv recom1+ σ 2
mends items j 6= j ∗ in T ≥ T ∗ steps at most (2M 1−
d4 log T + B) times with probability of at
least 1 − δ, where B is a constant independent of T .
Proof. By Theorem 1 of Gopalan et al. [13], the number of recommendations j 6= j ∗ is bounded by
C(log T ) + B, where B is a constant independent of T . Now we bound C(log T ) by counting the
number of times that PTS selects models that cannot be distinguished from (u∗ , v ∗ ) after observing
Yt under the optimal action j ∗ . Let:

	
Θj = (u, v) ∈ Θu × Θv : ∀i : ui vj ∗ = u∗i vj∗∗ , vj ≥ maxk6=j vk
be the set of such models where action j is optimal. Suppose that our algorithm chooses model
(u, v) ∈ Θj . Then the KL divergence between the distributions of ratings ri,j under models (u, v)
and (u∗ , v ∗ ) is bounded from below as:
(ui vj − u∗i vj∗ )2
d4
≥
.
2
2σ
2σ 2
for any i. The last inequality follows from the fact that ui vj ≥ ui vj∗ = u∗i vj∗∗ > u∗i vj∗ , because j ∗ is uniquely optimal in (u∗ , v ∗ ). We know that ui vj − u∗i vj∗  ≥ d2 because the granularity of our discretization is d. Let i1 , . . . , in be any n row indices. Then the KL divergence
between the distributions of ratings in positions (i1 , j), . . . , (in , j) under models (u, v) and (u∗ , v ∗ )
Pn
d4
is t=1 DKL (uit vj k u∗it vj∗ ) ≥ n 2σ
2 . By Theorem 1 of Gopalan et al. [13], the models (u, v) ∈ Θj
Pn
are unlikely to be chosen by PTS in T steps when t=1 DKL (uit vj k u∗it vj∗ ) ≥ log T . This happens
DKL (ui vj k u∗i vj∗ ) =

2

1+ σ
after at most n ≥ 2 1−
d4 log T selections of (u, v) ∈ Θj . Now we apply the same argument to all
Θj , M − 1 in total, and sum up the corresponding regrets.

5

2

1+ σ
Remarks: Note that Theorem 1 implies at O(2M 1−
d4 log T ) regret bound that holds with high
2
probability. Here, d plays the role of a gap ∆, the smallest possible difference between the expected
ratings of item j 6= j ∗ in any row i. In this sense, our result is O((1/∆2 ) log T ) and is of a
similar magnitude as the results in Gopalan et al. [13]. While we restrict u∗ , v ∗ ∈ (0, 1]K×1 in
the proof, this does not affect the algorithm. In fact, the proof only focuses on high probability
events where the samples from the posterior are concentrated around the true parameters, and thus,
are within (0, 1]K×1 as well. Extending our proof to the general setting is not trivial. In particular,
moving from discretized parameters to continuous space introduces the abovementioned ill behaved
posteriors. While increasing the value of K will violate the fact that the best item will be the same
for all users, which allowed us to eliminate N from the regret bound.

5

Experiments and Results

The goal of our experimental evaluation is twofold: (i) evaluate the PTS algorithm for making online
recommendations with respect to various baseline algorithms on several real-world datasets and (ii)
understand the qualitative performance and intuition of PTS.
5.1

Dataset description

We use a synthetic dataset and five real world datasets to evaluate our approach. The synthetic
dataset is generated as follows - At first we generate the user and item latent features (U and V )
of rank K by drawing from a Gaussian distribution N (0, σu2 ) and N (0, σv2 ) respectively. The true
rating matrix is then R∗ = U V T . We generate the observed rating matrix R from R∗ by adding
Gaussian noise N (0, σ 2 ) to the true ratings. We use five real world datasets as follows: Movielens
100k, Movielens 1M, Yahoo Music4 , Book crossing5 and EachMovie as shown in Table 1.

# users
# items
# ratings
5.2

Movielens 100k Movielens 1M Yahoo Music Book crossing
943
6040
15400
6841
1682
3900
1000
5644
100k
1M
311,704
90k
Table 1: Characteristics of the datasets used in our study

EachMovie
36656
1621
2.58M

Baseline measures

There are no current approaches available that simultaneously learn both the user and item factors
by sampling from the posterior in a bandit setting. From the currently available algorithms, we
choose two kinds of baseline methods - one that sequentially updates the the posterior of the user
features only while fixing the item features to a point estimate (ICF) and another that updates the
MAP estimates of user and item features via stochastic gradient descent (SGD-Eps). A key challenge in online algorithms is unbiased offline evaluation. One problem in the offline setting is the
partial information available about user feedback, i.e., we only have information about the items
that the user rated. In our experiment, we restrict the recommendation space of all the algorithms
to recommend among the items that the user rated in the entire dataset which makes it possible to
empirically measure regret at every interaction. The baseline measures are as follows:
1) Random : At each iteration, we recommend a random movie to the user.
2) Most Popular : At each iteration, we recommend the most popular movie restricted to the movies
rated by the user on the dataset. Note that this is an unrealistically optimistic baseline for an online
algorithm as it is not possible to know the global popularity of the items beforehand.
3) ICF: The ICF algorithm [2] proceeds by first estimating the user and item latent factors (U and
V ) on a initial training period and then for every interaction thereafter only updates the user features
(U ) assuming the item features (V ) as fixed. We run two scenarios for the ICF algorithm one in
which we use 20% (icf-20) and 50% (icf-50) of the data as the training period respectively. During
this period of training, we randomly recommend a movie to the user to compute the regret. We use
the PMF implementation by [5] for estimating the U and V .
4) SGD-Eps: We learn the latent factors using an online variant of the PMF algorithm [5]. We use
the stochastic gradient descent to update the latent factors with a mini-batch size of 50. In order
to make a recommendation, we use the -greedy strategy and recommend the highest Ui V T with a
probability  and make a random recommendations otherwise. ( is set as 0.95 in our experiments.)
4
5

http://webscope.sandbox.yahoo.com/
http://www.bookcrossing.com

6

5.3

Results on Synthetic Dataset

30
20
10
0
0

20

40
60
Iterations

80

450

180

400

160

350

140
120
100
80
60

300
250
200
150

40

100

20

50

0
0

100

(a) N, M=10,K=1

200

100

200
300
Iterations

400

0
0

500

(b) N, M=20,K=1

200

400
600
Iterations

800

120

140

100

120

80

100

Cummulative Regret

40

Cummulative Regret

50
Cummulative Regret

Cummulative Regret

60

Cummulative Regret

We generated the synthetic dataset as mentioned earlier and run the PTS algorithm with 100 particles
for recommendations. We simulate the setting as mentioned in Section 3 and assume that at time t,
a random user it arrives and the system recommends an item jt . The user rates the recommended
item rit ,jt and we evaluate the performance of the model by computing the expected cumulative
regret defined in Eq(6). Fig. 2 shows the cumulative regret of the algorithm on the synthetic data
averaged over 100 runs using different size of the matrix and latent features K. The cumulative
regret increases sub-linearly with the number of interactions and this gives us confidence that our
approach works well on the synthetic dataset.

60
40
20

(c) N, M=30,K=1

60
40
20

0
0

1000

80

20

40
60
Iterations

80

0
0

100

20

40

60

80

100

Iterations

(d) N, M=10,K=2

(e) N, M=10,K=3

Figure 2: Cumulative regret on different sizes of the synthetic data and K averaged over 100 runs.
Results on Real Datasets
4

5

x 10

10

15
PTS
random
popular
icf−20
icf−50
sgd−eps
PTS−B

Cummulative Regret

Cummulative Regret

15

5

5

x 10

6
PTS
random
popular
icf−20
icf−50
sgd−eps
PTS−B

10

x 10

PTS
random
popular
icf−20
icf−50
sgd−eps
PTS−B

5
Cummulative Regret

5.4

5

4

3

2

1

0
0

2

4

6

8

Iterations

10

0
0

2

4

6
Iterations

4

x 10

(a) Movielens 100k

12

6
PTS
random
popular
icf−50
sgd−eps
PTS−B

0
0

0.5

1

1.5
2
Iterations

5

2.5

3

3.5
5

x 10

(c) Yahoo Music

x 10

5
Cummulative Regret

Cummulative Regret

14

12
x 10

6

x 10

16

10

(b) Movielens 1M

4

18

8

10
8
6

4

PTS
random
popular
icf−20
icf−50
sgd−eps
PTS−B

3

2

4
1
2
0
0

2

4

6

8

Iterations

0
0

10
4

x 10

(d) Book Crossing

0.5

1

1.5
Iterations

2

2.5

3
6

x 10

(e) EachMovie

Figure 3: Comparison with baseline methods on five datasets.
Next, we evaluate our algorithms on five real datasets and compare them to the various baseline
algorithms. We subtract the mean ratings from the data to centre it at zero. To simulate an extreme
cold-start scenario we start from an empty set of user and rating. We then iterate over the datasets
and assume that a random user it has arrived at time t and the system recommends an item jt
constrained to the items rated by this user in the dataset. We use K = 2 for all the algorithms and
use 30 particles for our approach. For PTS we set the value of σ 2 = 0.5 and σu2 = 1, σv2 = 1.
For PTS-B (Bayesian version, see Algo. 1 for more details), we set σ 2 = 0.5 and the initial shape
parameters of the Gamma distribution as α = 2 and β = 0.5. For both ICF-20 and ICF-50, we set
σ 2 = 0.5 and σu2 = 1. Fig. 3 shows the cumulative regret of all the algorithms on the five datasets6 .
Our approach performs significantly better as compared to the baseline algorithms on this diverse
set of datasets. PTS-B with no parameter tuning performs slightly better than PTS and achieves the
best regret. It is important to note that both PTS and PTS-B performs comparable to or even better
than the “most popular” baseline despite not knowing the global popularity in advance. Note that
ICF is very sensitive to the length of the initial training period; it is not clear how to set this apriori.
6

ICF-20 fails to run on the Bookcrossing dataset as the 20% data is too sparse for the PMF implementation.

7

4

4
RB
No RB

test error
pmf

3.5

3

0.6
ICF−20
PTS−20
PTS−100

0.4

3

MSE

MSE

2
2.5

0.2
0

1

2

−0.2
0
1.5

−0.4
−1

1

−0.6
0.5
−200

0

200

400
600
Iterations x 1000

(a) Movielens 1M

800

1000

−2
0

20

40

60

80

Iterations

(b) RB particle filter

100

−0.8
−0.4

−0.2

0

0.2

0.4

0.6

(c) Movie feature vector

Figure 4: a) shows MSE on movielens 1M dataset, the red line is the MSE using the PMF algorithm
b) shows performance of a RBPF (blue line) as compared to vanilla PF (red line) on a synthetic
dataset N,M=10 and c) shows movie feature vectors for a movie with 384 ratings, the red dot is the
feature vector from the ICF-20 algorithm (using 73 ratings). PTS-20 is the feature vector at 20% of
the data (green dots) and PTS-100 at 100% (blue dots).
We also evaluate the performance of our model in an offline setting as follows: We divide the
datasets into training and test set and iterate over the training data triplets (it , jt , rt ) by pretending
that jt is the movie recommended by our approach and update the latent factors according to RBPF.
We compute the recovered matrix R̂ as the average prediction U V T from the particles at each time
step and compute the mean squared error (MSE) on the test dataset at each iteration. Unlike the
batch method such as PMF which takes multiple passes over the data, our method was designed to
have bounded update complexity at each iteration. We ran the algorithm using 80% data for training
and the rest for testing and computed the MSE by averaging the results over 5 runs. Fig. 4(a) shows
the average MSE on the movielens 1M dataset. Our MSE (0.7925) is comparable to the PMF MSE
(0.7718) as shown by the red line. This demonstrates that the RBPF is performing reasonably well
for matrix factorization. In addition, Fig. 4(b) shows that on the synthetic dataset, the vanilla PF
suffers from degeneration as seen by the high variance. To understand the intuition why fixing the
latent item features V as done in the ICF does not work, we perform an experiment as follows: We
run the ICF algorithm on the movielens 100k dataset in which we use 20% of the data for training.
At this point the ICF algorithm fixes the item features V and only updates the user features U . Next,
we run our algorithm and obtain the latent features. We examined the features for one selected movie
from the particles at two time intervals - one when the ICF algorithm fixes them at 20% and another
one in the end as shown in the Fig. 4(c). It shows that movie features have evolved into a different
location and hence fixing them early is not a good idea.

6

Related Work

Probabilistic matrix completion in a bandit setting setting was introduced in the previous work by
Zhao et al. [2]. The ICF algorithm in [2] approximates the posterior of the latent item features by
a single point estimate. Several other bandit algorithms for recommendations have been proposed.
Valko et al. [14] proposed a bandit algorithm for content-based recommendations. In this approach,
the features of the items are extracted from a similarity graph over the items, which is known in
advance. The preferences of each user for the features are learned independently by regressing the
ratings of the items from their features. The key difference in our approach is that we also learn
the features of the items. In other words, we learn both the user and item factors, U and V , while
[14] learn only U . Kocak et al. [15] combine the spectral bandit algorithm in [14] with TS. Gentile
et al. [16] propose a bandit algorithm for recommendations that clusters users in an online fashion
based on the similarity of their preferences. The preferences are learned by regressing the ratings of
the items from their features. The features of the items are the input of the learning algorithm and
they only learn U . Maillard et al. [17] study a bandit problem where the arms are partitioned into
unknown clusters unlike our work which is more general.

7

Conclusion

We have proposed an efficient method for carrying out matrix factorization (M ≈ U V T ) in a bandit
setting. The key novelty of our approach is the combined use of Rao-Blackwellized particle filtering
and Thompson sampling (PTS) in matrix factorization recommendation. This allows us to simultaneously update the posterior probability of U and V in an online manner while minimizing the
cumulative regret. The state of the art, till now, was to either use point estimates of U and V or use
a point estimate of one of the factor (e.g., U ) and update the posterior probability of the other (V ).
PTS results in substantially better performance on a wide variety of real world data sets.

8

References
[1] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30–37, 2009.
[2] Xiaoxue Zhao, Weinan Zhang, and Jun Wang. Interactive collaborative filtering. In Proceedings of the 22nd ACM international conference on Conference on information & knowledge
management, pages 1411–1420. ACM, 2013.
[3] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In NIPS,
pages 2249–2257, 2011.
[4] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear
payoffs. In ICML (3), pages 127–135, 2013.
[5] Ruslan Salakhutdinov and Andriy Mnih. Probabilistic matrix factorization. In NIPS, volume 1,
pages 2–1, 2007.
[6] Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization using
markov chain monte carlo. In ICML, pages 880–887, 2008.
[7] Nicolas Chopin. A sequential particle filter method for static models. Biometrika, 89(3):539–
552, 2002.
[8] Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. Sequential monte carlo samplers. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 68(3):411–436, 2006.
[9] Arnaud Doucet, Nando De Freitas, Kevin Murphy, and Stuart Russell. Rao-blackwellised
particle filtering for dynamic bayesian networks. In Proceedings of the Sixteenth conference
on Uncertainty in artificial intelligence, pages 176–183. Morgan Kaufmann Publishers Inc.,
2000.
[10] A. Gelman and X. L Meng. A note on bivariate distributions that are conditionally normal.
Amer. Statist., 45:125–126, 1991.
[11] B. C. Arnold, E. Castillo, J. M. Sarabia, and L. Gonzalez-Vega. Multiple modes in densities
with normal conditionals. Statist. Probab. Lett., 49:355–363, 2000.
[12] B. C. Arnold, E. Castillo, and J. M. Sarabia. Conditionally specified distributions: An introduction. Statistical Science, 16(3):249–274, 2001.
[13] Aditya Gopalan, Shie Mannor, and Yishay Mansour. Thompson sampling for complex online
problems. In Proceedings of The 31st International Conference on Machine Learning, pages
100–108, 2014.
[14] Michal Valko, Rémi Munos, Branislav Kveton, and Tomáš Kocák. Spectral bandits for smooth
graph functions. In 31th International Conference on Machine Learning, 2014.
[15] Tomáš Kocák, Michal Valko, Rémi Munos, and Shipra Agrawal. Spectral thompson sampling.
In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.
[16] Claudio Gentile, Shuai Li, and Giovanni Zappella. Online clustering of bandits. arXiv preprint
arXiv:1401.8257, 2014.
[17] Odalric-Ambrym Maillard and Shie Mannor. Latent bandits. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014,
pages 136–144, 2014.

9

