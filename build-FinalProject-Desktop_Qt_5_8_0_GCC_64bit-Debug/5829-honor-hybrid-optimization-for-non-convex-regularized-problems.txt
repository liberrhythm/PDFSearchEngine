HONOR: Hybrid Optimization for NOn-convex
Regularized problems

Jieping Ye
Univeristy of Michigan, Ann Arbor, MI 48109
jpye@umich.edu

Pinghua Gong
Univeristy of Michigan, Ann Arbor, MI 48109
gongp@umich.edu

Abstract
Recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice. However, due
to the non-convexity and non-smoothness of the regularizer, how to efficiently
solve the non-convex optimization problem for large-scale data is still quite challenging. In this paper, we propose an efficient Hybrid Optimization algorithm
for NOn-convex Regularized problems (HONOR). Specifically, we develop a hybrid scheme which effectively integrates a Quasi-Newton (QN) step and a Gradient Descent (GD) step. Our contributions are as follows: (1) HONOR incorporates the second-order information to greatly speed up the convergence, while
it avoids solving a regularized quadratic programming and only involves matrixvector multiplications without explicitly forming the inverse Hessian matrix. (2)
We establish a rigorous convergence analysis for HONOR, which shows that convergence is guaranteed even for non-convex problems, while it is typically challenging to analyze the convergence for non-convex problems. (3) We conduct
empirical studies on large-scale data sets and results demonstrate that HONOR
converges significantly faster than state-of-the-art algorithms.

1 Introduction
Sparse learning with convex regularization has been successfully applied to a wide range of applications including marker genes identification [19], face recognition [22], image restoration [2],
text corpora understanding [9] and radar imaging [20]. However, it has been shown recently that
many convex sparse learning formulations are inferior to their non-convex counterparts in both theory and practice [27, 12, 23, 25, 16, 26, 24, 11]. Popular non-convex sparsity-inducing penalties
include Smoothly Clipped Absolute Deviation (SCAD) [10], Log-Sum Penalty (LSP) [6] and Minimax Concave Penalty (MCP) [23]. Although non-convex sparse learning reveals its advantage over
the convex one, it remains a challenge to develop an efficient algorithm to solve the non-convex
optimization problem especially for large-scale data.
DC programming [21] is a popular approach to solve non-convex problems whose objective functions can be expressed as the difference of two convex functions. However, a potentially non-trivial
convex subproblem is required to solve at each iteration, which is not practical for large-scale problems. SparseNet [16] can solve a least squares problem with a non-convex penalty. At each step,
SparseNet solves a univariate subproblem with a non-convex penalty which admits a closed-form
solution. However, to establish the convergence analysis, the parameter of the non-convex penalty
is required to be restricted to some interval such that the univariate subproblem (with a non-convex
penalty) is convex. Moreover, it is quite challenging to extend SparseNet to non-convex problems
with a non-least-squares loss, as the univariate subproblem generally does not admit a closed-form
solution. The GIST algorithm [14] can solve a class of non-convex regularized problems by iteratively solving a possibly non-convex proximal operator problem, which in turn admits a closed-form
solution. However, GIST does not well exploit the second-order information. The DC-PN algorithm
1

[18] can incorporate the second-order information to solve non-convex regularized problems but it
requires to solve a non-trivial regularized quadratic subproblem at each iteration.
In this paper, we propose an efficient Hybrid Optimization algorithm for NOn-convex Regularized
problems (HONOR), which incorporates the second-order information to speed up the convergence.
HONOR adopts a hybrid optimization scheme which chooses either a Quasi-Newton (QN) step or
a Gradient Descent (GD) step per iteration mainly depending on whether an iterate has very small
components. If an iterate does not have any small component, the QN-step is adopted, which uses
L-BFGS to exploit the second-order information. The key advantage of the QN-step is that it does
not need to solve a regularized quadratic programming and only involves matrix-vector multiplications without explicitly forming the inverse Hessian matrix. If an iterate has small components,
we switch to a GD-step. Our detailed theoretical analysis sheds light on the effect of such a hybrid scheme on the convergence of the algorithm. Specifically, we provide a rigorous convergence
analysis for HONOR, which shows that every limit point of the sequence generated by HONOR is
a Clarke critical point. It is worth noting that the convergence analysis for a non-convex problem
is typically much more challenging than the convex one, because many important properties for a
convex problem may not hold for non-convex problems. Empirical studies are also conducted on
large-scale data sets which include up to millions of samples and features; results demonstrate that
HONOR converges significantly faster than state-of-the-art algorithms.

2 Non-convex Sparse Learning
We focus on the following non-convex regularized optimization problem:
min {f (x) = l(x) + r(x)} ,

xâˆˆRn

(1)

where we make the following assumptions throughout the paper:
(A1) l(x) is coercive, continuously differentiable and âˆ‡l(x) is Lipschitz continuous with constant L. Moreover, l(x) > âˆ’âˆ for all x âˆˆ Rn .
Pn
(A2) r(x) = i=1 Ï(|xi |), where Ï(t) is non-decreasing, continuously differentiable and concave with respect to t in [0, âˆ); Ï(0) = 0 and Ïâ€² (0) 6= 0 with Ïâ€² (t) = âˆ‚Ï(t)/âˆ‚t denoting
the derivative of Ï(t) at the point t.
Remark 1 Assumption (A1) allows l(x) to be non-convex. Assumption (A2) implies that Ï(|xi |) is
generally non-convex with respect to xi and the only convex case is Ï(|xi |) = Î»|xi | with Î» > 0.
Moreover, Ï(|xi |) is continuously differentiable with respect to xi in (âˆ’âˆ, 0) âˆª (0, âˆ) and nondifferentiable at xi = 0. In particular, âˆ‚Ï(|xi |)/âˆ‚xi = Ïƒ(xi )Ïâ€² (|xi |) for any xi 6= 0, where
Ïƒ(xi ) = 1, if xi > 0; Ïƒ(xi ) = âˆ’1, if xi < 0 and Ïƒ(xi ) = 0, otherwise. In addition, Ïâ€² (0) > 0 must
hold (Otherwise Ïâ€² (0) < 0 implies Ï(t) â‰¤ Ï(0) + Ïâ€² (0)t < 0 for any t > 0, contradicting the fact
that Ï(t) is non-decreasing). It is also easy to show that, under the assumptions above, both l(x)
and r(x) are locally Lipschitz continuous. Thus, the Clarke subdifferential [7] is well-defined.
The commonly used least squares loss and the logistic regression loss satisfy the assumption (A1);
we can add a small term Î´kxk2 to make them coercive. The following popular non-convex regularizers satisfy the assumption (A2), where Î» > 0 and Î¸ > 0 except that Î¸ > 2 for SCAD.
â€¢ LSP: Ï(|xi |) = Î» log(1 + |xi |/Î¸).
ï£±
ï£² Î»|x2i |,
âˆ’xi +2Î¸Î»|xi |âˆ’Î»2
â€¢ SCAD: Ï(|xi |) =
,
2(Î¸âˆ’1)
ï£³
(Î¸ + 1)Î»2 /2,

Î»|xi | âˆ’ x2i /(2Î¸),
â€¢ MCP: Ï(|xi |) =
Î¸Î»2 /2,

if |xi | â‰¤ Î»,
if Î» < |xi | â‰¤ Î¸Î»,
if |xi | > Î¸Î».
if |xi | â‰¤ Î¸Î»,
if |xi | > Î¸Î».

Due to the non-convexity and non-differentiability of problem (1), the traditional subdifferential
concept for the convex optimization is not applicable here. Thus, we use the Clarke subdifferential
[7] to characterize the optimality of problem (1). We say xÌ„ is a Clarke critical point of problem (1),
if 0 âˆˆ âˆ‚ o f (xÌ„), where âˆ‚ o f (xÌ„) is the Clarke subdifferential of f (x) at x = xÌ„. To be self-contained,
2

we briefly review the Clarke subdifferential: for a locally Lipschitz continuous function f (x), the
Clarke generalized directional derivative of f (x) at x = xÌ„ along the direction d is defined as
f (x + Î±d) âˆ’ f (x)
.
Î±
xâ†’xÌ„,Î±â†“0

f o (xÌ„; d) = lim sup

Then, the Clarke subdifferential of f (x) at x = xÌ„ is defined as
âˆ‚ o f (xÌ„) = {Î´ âˆˆ Rn : f o (xÌ„; d) â‰¥ dT Î´, âˆ€d âˆˆ Rn }.
Interested readers may refer to Proposition 4 in the Supplement A for more properties about the
Clarke Subdifferential. We want to emphasize that some basic properties of the subdifferential of a
convex function may not hold for the Clarke Subdifferential of a non-convex function.

3 Proposed Optimization Algorithm: HONOR
Since each decomposable component function of the regularizer is only non-differentiable at the
origin, the objective function is differentiable, if the segment between any two consecutive iterates
do not cross any axis. This motivates us to design an algorithm which can keep the current iterate
in the same orthant of the previous iterate. Before we present the detailed HONOR algorithm, we
introduce two functions as follows:
Define a function Ï€ : Rn 7â†’ Rn with the i-th entry being:

xi , if Ïƒ(xi ) = Ïƒ(yi ),
Ï€i (xi ; yi ) =
0, otherwise,

where y âˆˆ Rn (yi is the i-th entry of y) is the parameter of the function Ï€; Ïƒ(Â·) is the sign function
defined as follows: Ïƒ(xi ) = 1, if xi > 0; Ïƒ(xi ) = âˆ’1, if xi < 0 and Ïƒ(xi ) = 0, otherwise.
Define the pseudo-gradient â‹„f (x) whose i-th entry is given by:
ï£±
âˆ‡i l(x) + Ïâ€² (|xi |), if xi > 0,
ï£´
ï£´
ï£´
ï£² âˆ‡i l(x) âˆ’ Ïâ€² (|xi |), if xi < 0,
âˆ‡i l(x) + Ïâ€² (0),
if xi = 0, âˆ‡i l(x) + Ïâ€² (0) < 0,
â‹„i f (x) =
ï£´
â€²
ï£´
âˆ‡
l(x)
âˆ’
Ï
(0),
if
xi = 0, âˆ‡i l(x) âˆ’ Ïâ€² (0) > 0,
ï£´
ï£³ i
0,
otherwise,

where Ïâ€² (t) is the derivative of Ï(t) at the point t.

Remark 2 If r(x) is convex, â‹„f (x) is the minimum-norm sub-gradient of f (x) at x. Thus, âˆ’ â‹„ f (x)
is a descent direction. However, â‹„f (x) is not even a sub-gradient of f (x) if r(x) is non-convex.
This indicates that some obvious concepts and properties for a convex problem may not hold in the
non-convex case. Thus, it is significantly more challenging to develop and analyze algorithms for a
non-convex problem.
Interestingly, we can still show that vk = âˆ’ â‹„ f (xk ) is a descent direction at the point xk (refer
to Supplement D and replace pk = Ï€(dk ; vk ) with vk ). To utilize the second-order information,
we may perform the optimization along the direction dk = H k vk , where H k is a positive definite
matrix containing the second-order information. However, dk is not necessarily a descent direction.
To address this issue, we use the following slightly modified direction pk :
pk = Ï€(dk ; vk ).
We can show that pk is a descent direction (proof is provided in Supplement D). Thus, we can
perform the optimization along the direction pk . Recall that we need to keep the current iterate in
the same orthant of the previous iterate. So the following iterative scheme is proposed:
xk (Î±) = Ï€(xk + Î±pk ; Î¾ k ),

(2)

where
Î¾ik

=



Ïƒ(xki ), if xki 6= 0,
Ïƒ(vik ), if xki = 0,
3

(3)

and Î± is a step size chosen by the following line search procedure: for constants Î±0 > 0, Î², Î³ âˆˆ
(0, 1) and m = 0, 1, Â· Â· Â· , find the smallest integer m with Î± = Î±0 Î² m such that the following
inequality holds:
f (xk (Î±)) â‰¤ f (xk ) âˆ’ Î³Î±(vk )T dk .

(4)

However, only using the above iterative scheme may not guarantee the convergence. The main challenge is: if there exists a subsequence K such that {xki }K converges to zero, it is possible that for
sufficiently large k âˆˆ K, |xki | is arbitrarily small but never equal to zero (refer to the proof of Theorem 1 for more details). To address this issue, we propose a hybrid optimization scheme. Specifically, for a small constant Ç« > 0, if I k = {i âˆˆ {1, Â· Â· Â· , n} : 0 < |xki | â‰¤ min(kvk k, Ç«), xki vik < 0}
is not empty, we switch the iteration to the following gradient descent step (GD-step):


1
xk (Î±) = arg min âˆ‡l(xk )T (x âˆ’ xk ) +
kx âˆ’ xk k2 + r(x) ,
2Î±
x
where Î± is a step size chosen by the following line search procedure: for constants Î±0 > 0, Î², Î³ âˆˆ
(0, 1) and m = 0, 1, Â· Â· Â· , find the smallest integer m with Î± = Î±0 Î² m such that the following
inequality holds:
f (xk (Î±)) â‰¤ f (xk ) âˆ’

Î³
kxk (Î±) âˆ’ xk k2 .
2Î±

(5)

The detailed steps of the algorithm are presented in Algorithm 1.
Remark 3 Algorithm 1 is similar to OWL-QN-type algorithms in [1, 3, 4, 17, 13]. However,
HONOR is significantly different from them: (1) The OWL-QN-type algorithms can only handle â„“1 regularized convex problems while HONOR is applicable to a class of non-convex problems beyond
â„“1 -regularized ones. (2) The convergence analyses of the OWL-QN-type algorithms heavily rely on
the convexity of the â„“1 -regularized problem. In contrast, the convergence analysis for HONOR is
applicable to non-convex cases beyond the convex ones, which is a non-trivial extension.
Algorithm 1: HONOR: Hybrid Optimization for NOn-convex Regularized problems
1
2
3

4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

Initialize x0 , H 0 and choose Î², Î³ âˆˆ (0, 1), Ç« > 0, Î±0 > 0;
for k = 0 to maxiter do
Compute vk â† âˆ’ â‹„ f (xk ) and I k = {i âˆˆ {1, Â· Â· Â· , n} : 0 < |xki | â‰¤ Ç«k , xki vik < 0}, where
Ç«k = min(kvk k, Ç«);
Initialize Î± â† Î±0 ;
if I k = âˆ… then
(QN-step)
Compute dk â† H k vk with a positive definite matrix H k using L-BFGS;
Alignment: pk â† Ï€(dk ; vk );
while Eq. (4) is not satisfied do
Î± â† Î±Î²; xk (Î±) â† Ï€(xk + Î±pk ; Î¾ k );
end
else
(GD-step)
while Eq. (5) is not satisfied do
Î± â† Î±Î²;

	
1
xk (Î±) â† arg minx âˆ‡l(xk )T (x âˆ’ xk ) + 2Î±
kx âˆ’ xk k2 + r(x) ;
end
end
xk+1 â† xk (Î±);
if some stopping criterion is satisfied then
stop and return xk+1 ;
end
end

4

4 Convergence Analysis
We first present a few basic propositions and then provide the convergence theorem based on the
propositions; all proofs of the presented propositions are carefully handled due to the lack of convexity. First of all, an optimality condition is presented (proof is provided in Supplement B), which
will be directly used in the proof of Theorem 1.
Proposition 1 Let xÌ„ = limkâˆˆK,kâ†’âˆ xk , vk = âˆ’ â‹„ f (xk ) and vÌ„ = âˆ’ â‹„ f (xÌ„), where K is a
subsequence of {1, 2, Â· Â· Â· , k, k + 1, Â· Â· Â· }. If lim inf kâˆˆK,kâ†’âˆ |vik | = 0 for all i âˆˆ {1, Â· Â· Â· , n}, then
vÌ„ = 0 and xÌ„ is a Clarke critical point of problem (1).
We subsequently show that we have a Lipschitz-continuous-like inequality in the following proposition (proof is provided in Supplement C), which is crucial to prove the final convergence theorem.
Proposition 2 Let vk = âˆ’â‹„f (xk ), xk (Î±) = Ï€(xk +Î±pk ; Î¾ k ) and qkÎ± =
with Î± > 0. Then under assumptions (A1) and (A2), we have

1
k
k
k
k
Î± (Ï€(x +Î±p ; Î¾ )âˆ’x )

(i) âˆ‡l(xk )T (xk (Î±) âˆ’ xk ) + r(xk (Î±)) âˆ’ r(xk ) â‰¤ âˆ’(vk )T (xk (Î±) âˆ’ xk ),
(ii) f (xk (Î±)) â‰¤ f (xk ) âˆ’ Î±(vk )T qkÎ± +

(6)

2

Î± L k 2
kqÎ± k .
2

(7)

We next show that both line search criteria in the QN-step [Eq. (4)] and the GD-step [Eq. (5)] at any
iteration k is satisfied in a finite number of trials (proof is provided in Supplement D).
Proposition 3 At any iteration k of the HONOR algorithm, if xk is not a Clarke critical point of
problem (1), then (a) for the QN-step, there exists an Î± âˆˆ [Î±Ì„k , Î±0 ] with 0 < Î±Ì„k â‰¤ Î±0 such that the
line search criterion in Eq. (4) is satisfied; (b) for the GD-step, the line search criterion in Eq. (5) is
satisfied whenever Î± â‰¥ Î² min(Î±0 , (1 âˆ’ Î³)/L). That is, both line search criteria at any iteration k
are satisfied in a finite number of trials.
We are now ready to provide the convergence proof for the HONOR algorithm:
Theorem 1 The sequence {xk } generated by the HONOR algorithm has at least a limit point and
every limit point of {xk } is a Clarke critical point of problem (1).
Proof It follows from Proposition 3 that both line search criteria in the QN-step [Eq. (4)] and the
GD-step [Eq. (5)] at each iteration can be satisfied in a finite number of trials. Let Î±k be the accepted
step size at iteration k. Then we have
f (xk ) âˆ’ f (xk+1 ) â‰¥ Î³Î±k (vk )T dk = Î³Î±k (vk )T H k vk (QN-step),
Î³
Î³
or f (xk ) âˆ’ f (xk+1 ) â‰¥
kxk+1 âˆ’ xk k2 â‰¥
kxk+1 âˆ’ xk k2 (GD-step).
k
2Î±
2Î±0

(8)
(9)

Recall that H k is positive definite and Î³ > 0, Î±k > 0, which together with Eqs.(8), (9) imply
that {f (xk )} is monotonically decreasing. Thus, {f (xk )} converges to a finite value fÂ¯, since f is
bounded from below (note that l(x) > âˆ’âˆ and r(x) â‰¥ 0 for all x âˆˆ Rn ). Due to the boundedness
of {xk } (see Proposition 7 in Supplement F), the sequence {xk } generated by the HONOR algorithm
has at least a limit point xÌ„. Since f is continuous, there exists a subsequence K of {1, 2 Â· Â· Â· , k, k +
1, Â· Â· Â· } such that
lim

kâˆˆK,kâ†’âˆ

xk = xÌ„,

lim f (xk ) =

kâ†’âˆ

(10)

lim

kâˆˆK,kâ†’âˆ

f (xk ) = fÂ¯ = f (xÌ„).

(11)

In the following, we prove the theorem by contradiction. Assume that xÌ„ is not a Clarke critical point
of problem (1). Then by Proposition 1, there exists at least one i âˆˆ {1, Â· Â· Â· , n} such that
lim inf |vik | > 0.

kâˆˆK,kâ†’âˆ

5

(12)

We next consider the following two cases:
(a) There exist a subsequence KÌƒ of K and an integer kÌƒ > 0 such that for all k âˆˆ KÌƒ, k â‰¥ kÌƒ, the
GD-step is adopted. Then for all k âˆˆ KÌƒ, k â‰¥ kÌƒ, we have


1
xk+1 = arg min âˆ‡l(xk )T (x âˆ’ xk ) + k kx âˆ’ xk k2 + r(x) .
2Î±
x
Thus, by the optimality condition of the above problem and properties of the Clarke subdifferential
(Proposition 4 in Supplement A), we have
0 âˆˆ âˆ‡l(xk ) +

1 k+1
(x
âˆ’ xk ) + âˆ‚ o r(xk+1 ).
Î±k

(13)

Taking limits with k âˆˆ KÌƒ for Eq. (9) and considering Eqs. (10), (11), we have
lim

kxk+1 âˆ’ xk k2 â‰¤ 0 â‡’

kâˆˆKÌƒ,kâ†’âˆ

xk =

lim
kâˆˆKÌƒ,kâ†’âˆ

lim

xk+1 = xÌ„.

(14)

kâˆˆKÌƒ,kâ†’âˆ

Taking limits with k âˆˆ KÌƒ for Eq. (13) and considering Eq. (14), Î±k â‰¥ Î² min(Î±0 , (1 âˆ’ Î³)/L)
[Proposition 3] and âˆ‚ o r(Â·) is upper-semicontinuous (upper-hemicontinuous) [8] (see Proposition 4
in the Supplement A), we have
0 âˆˆ âˆ‡l(xÌ„) + âˆ‚ o r(xÌ„) = âˆ‚ o f (xÌ„),
which contradicts the assumption that xÌ„ is not a Clarke critical point of problem (1).
(b) There exists an integer kÌ‚ > 0 such that for all k âˆˆ K, k â‰¥ kÌ‚, the QN-step is adopted. According
to Remark 7 (in Supplement F), we know that the smallest eigenvalue of H k is uniformly bounded
from below by a positive constant, which together with Eq. (12) implies
lim inf (vk )T H k vk > 0.

kâˆˆK,kâ†’âˆ

(15)

Taking limits with k âˆˆ K for Eq. (8), we have
lim

kâˆˆK,kâ†’âˆ

Î³Î±k (vk )T H k vk â‰¤ 0,

which together with Î³ âˆˆ (0, 1), Î±k âˆˆ (0, Î±0 ] and Eq. (15) implies that
lim

kâˆˆK,kâ†’âˆ

Î±k = 0.

(16)

Eq. (12) implies that there exist an integer kÌŒ > 0 and a constant Ç«Ì„ > 0 such that Ç«k =
min(kvk k, Ç«) â‰¥ Ç«Ì„ for all k âˆˆ K, k â‰¥ kÌŒ. Notice that for all k âˆˆ K, k â‰¥ kÌ‚, the QN-step is
adopted. Thus, we obtain that I k = {i âˆˆ {1, Â· Â· Â· , n} : 0 < |xki | â‰¤ Ç«k , xki vik < 0} = âˆ… for all
k âˆˆ K, k â‰¥ kÌ‚. We also notice that, if |xki | â‰¥ Ç«Ì„, then there exists a constant Î±Ì„i > 0 such that
xki (Î±) = Ï€i (xki + Î±pki ; Î¾ik ) = xki + Î±pki for all Î± âˆˆ (0, Î±Ì„i ], as {pki } is bounded (Proposition 8
in Supplement F). Therefore, we conclude that, for all k âˆˆ K, k â‰¥ kÌ„ = max(kÌŒ, kÌ‚) and for all
i âˆˆ {1, Â· Â· Â· , n}, at least one of the following three cases must happen:
xki = 0 â‡’ xki (Î±) = Ï€i (xki + Î±pki ; Î¾ik ) = xki + Î±pki , âˆ€Î± > 0,
or |xki | > Ç«k â‰¥ Ç«Ì„ â‡’ xki (Î±) = Ï€i (xki + Î±pki ; Î¾ik ) = xki + Î±pki , âˆ€Î± âˆˆ (0, Î±Ì„i ],
or xki vik â‰¥ 0 â‡’ xki pki â‰¥ 0 â‡’ xki (Î±) = Ï€i (xki + Î±pki ; Î¾ik ) = xki + Î±pki , âˆ€Î± > 0.
It follows that there exists a constant Î±Ì„ > 0 such that
1
qkÎ± = (xk (Î±) âˆ’ xk ) = pk , âˆ€k âˆˆ K, k â‰¥ kÌ„, Î± âˆˆ (0, Î±Ì„].
Î±

(17)

Thus, considering |pki | = |Ï€i (dki ; vik )| â‰¤ |dki | and vik pki â‰¥ vik dki for all i âˆˆ {1, Â· Â· Â· , n}, we have
kqkÎ± k2 = kpk k2 â‰¤ kdk k2 = (vk )T (H k )2 vk , âˆ€k âˆˆ K, k â‰¥ kÌ„, Î± âˆˆ (0, Î±Ì„],
k T

(v )

qkÎ±

k T

k

k T

k

k T

k k

= (v ) p â‰¥ (v ) d = (v ) H v , âˆ€k âˆˆ K, k â‰¥ kÌ„, Î± âˆˆ (0, Î±Ì„].
6

(18)
(19)

According to Proposition 8 (in Supplement F), we know that the largest eigenvalue of H k is uniformly bounded from above by some positive constant M . Thus, we have


2
2
(vk )T (H k )2 vk â‰¤
(vk )T H k vk âˆ’
âˆ’ M (vk )T H k vk , âˆ€k,
Î±L
Î±L
which together with Eqs. (18), (19) and dk = H k vk implies


2
2
k T k
k 2
(v ) qÎ± âˆ’
âˆ’ M (vk )T dk , âˆ€k âˆˆ K, k â‰¥ kÌ„, Î± âˆˆ (0, Î±Ì„].
kqÎ± k â‰¤
Î±L
Î±L

(20)

Considering Eqs. (7), (20), we have


Î±LM
(vk )T dk , âˆ€k âˆˆ K, k â‰¥ kÌ„, Î± âˆˆ (0, Î±Ì„],
f (xk (Î±)) â‰¤ f (xk ) âˆ’ Î± 1 âˆ’
2
which together with (vk )T dk = (vk )T H k vk â‰¥ 0 implies that the line search criterion in the
QN-step [Eq. (4)] is satisfied if
Î±LM
â‰¥ Î³ , 0 < Î± â‰¤ Î±0 and 0 < Î± â‰¤ Î±Ì„, âˆ€k âˆˆ K, k â‰¥ kÌ„.
2
Considering the backtracking form of the line search in QN-step [Eq. (4)], we conclude that the line
search criterion in the QN-step [Eq. (4)] is satisfied whenever
1âˆ’

Î±k â‰¥ Î² min(min(Î±Ì„, Î±0 ), 2(1 âˆ’ Î³)/(LM )) > 0, âˆ€k âˆˆ K, k â‰¥ kÌ„.
This leads to a contradiction with Eq. (16).
By (a) and (b), we conclude that xÌ„ = limkâˆˆK,kâ†’âˆ xk is a Clarke critical point of problem (1).



5 Experiments
In this section, we evaluate the efficiency of HONOR on solving the non-convex regularized loPN
gistic regression problem1 by setting l(x) = 1/N i=1 log(1 + exp(âˆ’yi aTi x)), where ai âˆˆ
Rn is the i-th sample associated with the label yi âˆˆ {1, âˆ’1}. Three non-convex regularizers (LSP, MCP and SCAD) are included in experiments, where the parameters are set as Î» =
1/N and Î¸ = 10âˆ’2 Î» (Î¸ is set as 2 + 10âˆ’2 Î» for SCAD as it requires Î¸ > 2). We compare HONOR with the non-convex solver2 GIST [14] on three large-scale, high-dimensional
and sparse data sets which are summarized in Table 1. All data sets can be downloaded from
http://www.csie.ntu.edu.tw/Ëœcjlin/libsvmtools/datasets/.
All algorithms are implemented in MatTable 1: Data set statistics.
lab 2015a under a Linux operating sysdatasets
kdd2010a
kdd2010b
url
tem and executed on an Intel Core
â™¯ samples N
510,302
748,401
2,396,130
i7-4790 CPU (@3.6GHz) with 32GB
dimensionality n 20,216,830 29,890,095 3,231,961
memory. We choose the starting points
0
x for the compared algorithms using the same random vector whose entries are i.i.d. sampled from
the standard Gaussian distribution. We terminate the compared algorithms if the relative change of
two consecutive objective function values is less than 10âˆ’5 or the number of iterations exceeds 1000
(HONOR) or 10000 (GIST). For HONOR, we set Î³ = 10âˆ’5 , Î² = 0.5, Î±0 = 1 and the number of
unrolling steps in L-BFGS as m = 10. For GIST, we use the non-monotone line search in experiments as it usually performs better than its monotone counterpart. To show how the convergence
behavior of HONOR varies over the parameter Ç«, we use three values: Ç« = 10âˆ’10 , 10âˆ’6 , 10âˆ’2 .
We report the objective function value (in log-scale) vs. CPU time (in seconds) plots in Figure 1.
We can observe from Figure 1 that: (1) If Ç« is set to a small value, the QN-step is adopted at almost
all steps in HONOR and HONOR converges significantly faster than GIST for all three non-convex
1

We do not include the term Î´kxk2 in the objective and find that the proposed algorithm still works well.
We do not involve SparseNet, DC programming and DC-PN in comparison, because (1) adapting
SparseNet to the logistic regression problem is challenging; (2) DC programming is shown to be much inferior to GIST; (3) The objective function value of DC-PN is larger than GIST in most cases [18].
2

7

400

600

800

1000

1000

800

1000

1500

2000

2500

10 -3

500

1000

1500

2000

2500

CPU time (seconds)

SCAD (kdd2010a)

SCAD (kdd2010b)
HONOR(Ç«=1e-10)
HONOR(Ç«=1e-6)
HONOR(Ç«=1e-2)
GIST

10 -1

-2

1000

1500

2000

CPU time (seconds)

2500

3000

3500

6000

8000

10000

12000

14000

10 -3
3000

CPU time (seconds)

HONOR(Ç«=1e-10)
HONOR(Ç«=1e-6)
HONOR(Ç«=1e-2)
GIST

10 0

10 -1

10 -2

10 -3

0

0.5

1

1.5

2

2.5

3
Ã—10 4

SCAD (url)

10 -2

2000

4000

CPU time (seconds)

10 -1

1000

Objective function value (logged scale)

4000

HONOR(Ç«=1e-10)
HONOR(Ç«=1e-6)
HONOR(Ç«=1e-2)
GIST

10 0

0

2000

CPU time (seconds)

10 -2

0

10 -1
0

10 -1

3000

10 0

1200

HONOR(Ç«=1e-10)
HONOR(Ç«=1e-6)
HONOR(Ç«=1e-2)
GIST

10 0

HONOR(Ç«=1e-10)
HONOR(Ç«=1e-6)
HONOR(Ç«=1e-2)
GIST

10 1

MCP (url)

CPU time (seconds)

10 0

500

600

MCP (kdd2010b)

10 -2

0

400

MCP (kdd2010a)

10 -1

10

200

CPU time (seconds)

HONOR(Ç«=1e-10)
HONOR(Ç«=1e-6)
HONOR(Ç«=1e-2)
GIST

500

0

CPU time (seconds)

10 0

0

10 0

1200

Objective function value (logged scale)

Objective function value (logged scale)

200

10 1

Objective function value (logged scale)

10 0

LSP (url)
HONOR(Ç«=1e-10)
HONOR(Ç«=1e-6)
HONOR(Ç«=1e-2)
GIST

4000

Objective function value (logged scale)

10 1

0

Objective function value (logged scale)

LSP (kdd2010b)
HONOR(Ç«=1e-10)
HONOR(Ç«=1e-6)
HONOR(Ç«=1e-2)
GIST

Objective function value (logged scale)

LSP (kdd2010a)

Objective function value (logged scale)

Objective function value (logged scale)

regularizers on all three data sets. This shows that using the second-order information greatly speeds
up the convergence. (2) When Ç« increases, the ratio of the GD-step adopted in HONOR increases.
Meanwhile, the convergence performance of HONOR generally degrades. In some cases, setting
a slightly larger Ç« and adopting a small number of GD steps even sligtly boosts the convergence
performance of HONOR (the green curves in the first row). But setting Ç« to a very small value is
always safe to guarantee the fast convergence of HONOR. (3) When Ç« is large enough, the GD steps
dominate all iterations of HONOR and HONOR converge much slower. In this case, HONOR converges even slower than GIST. The reason is that, at each iteration of HONOR, extra computational
cost is required in addition to the basic computation in the GD-step. Moreover, the non-monotone
line search is used in GIST while the monotone line search is adopted in the GD-step. (4) In some
cases (the first row), GIST is trapped in a local solution which has a much larger objective function
value than HONOR with a small Ç«. This implies that HONOR may have a potential of escaping
from high error plateau which often exists in high dimensional non-convex problems. These results
show the great potential of HONOR for solving large-scale non-convex sparse learning problems.

HONOR(Ç«=1e-10)
HONOR(Ç«=1e-6)
HONOR(Ç«=1e-2)
GIST

10 0

10 -1

10 -2

10 -3

0

0.5

1

1.5

2

CPU time (seconds)

2.5
Ã—10 4

Figure 1: Objective function value (in log-scale) vs. CPU time (in seconds) plots for different non-convex regularizers and different large-scale and high-dimensional data sets. The ratios of the GD-step adopted in HONOR are: LSP (kdd2010a): 0%, 1%, 34%; LSP (kdd2010b):
0%, 2%, 27%; LSP (url): 0.1%, 2%, 35%; MCP (kdd2010a): 0%, 88%, 100%; MCP (kdd2010b):
0%, 89%, 100%; MCP (url): 0%, 97%, 100%; SCAD (kdd2010a): 0%, 43%, 100%; SCAD (2010b):
0%, 32%, 99.5%; SCAD (url): 0%, 79%, 100%.

6 Conclusions
In this paper, we propose an efficient optimization algorithm called HONOR for solving non-convex
regularized sparse learning problems. HONOR incorporates the second-order information to speed
up the convergence in practice and uses a carefully designed hybrid optimization scheme to guarantee the convergence in theory. Experiments are conducted on large-scale data sets and results show
that HONOR converges significantly faster than state-of-the-art algorithms. In our future work, we
plan to develop parallel/distributed variants of HONOR to tackle much larger data sets.

Acknowledgements
This work is supported in part by research grants from NIH (R01 LM010730, U54 EB020403) and
NSF (IIS- 0953662, III-1539991, III-1539722).
8

References
[1] G. Andrew and J. Gao. Scalable training of â„“1 -regularized log-linear models. In ICML, pages 33â€“40,
2007.
[2] J. Bioucas-Dias and M. Figueiredo. A new TwIST: two-step iterative shrinkage/thresholding algorithms
for image restoration. IEEE Transactions on Image Processing, 16(12):2992â€“3004, 2007.
[3] R. H. Byrd, G. M. Chin, J. Nocedal, and F. Oztoprak. A family of second-order methods for convex
â„“1 -regularized optimization. Technical report, Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL, 2012.
[4] R. H. Byrd, G. M. Chin, J. Nocedal, and Y. Wu. Sample size selection in optimization methods for
machine learning. Mathematical Programming, 134(1):127â€“155, 2012.
[5] R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16(5):1190â€“1208, 1995.
[6] E. Candes, M. Wakin, and S. Boyd. Enhancing sparsity by reweighted â„“1 minimization. Journal of Fourier
Analysis and Applications, 14(5):877â€“905, 2008.
[7] F. Clarke. Optimization and Nonsmooth Analysis. John Wiley&Sons, New York, 1983.
[8] J. Dutta. Generalized derivatives and nonsmooth optimization, a finite dimensional tour. Top, 13(2):185â€“
279, 2005.
[9] L. El Ghaoui, G. Li, V. Duong, V. Pham, A. Srivastava, and K. Bhaduri. Sparse machine learning methods
for understanding large text corpora. In CIDU, pages 159â€“173, 2011.
[10] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal
of the American Statistical Association, 96(456):1348â€“1360, 2001.
[11] J. Fan, L. Xue, and H. Zou. Strong oracle optimality of folded concave penalized estimation. Annals of
Statistics, 42(3):819, 2014.
[12] G. Gasso, A. Rakotomamonjy, and S. Canu. Recovering sparse signals with a certain family of nonconvex
penalties and dc programming. IEEE Transactions on Signal Processing, 57(12):4686â€“4698, 2009.
[13] P. Gong and J. Ye. A modified orthant-wise limited memory quasi-newton method with convergence
analysis. In ICML, 2015.
[14] P. Gong, C. Zhang, Z. Lu, J. Huang, and J. Ye. A general iterative shrinkage and thresholding algorithm
for non-convex regularized optimization problems. In ICML, volume 28, pages 37â€“45, 2013.
[15] N. Jorge and J. Stephen. Numerical Optimization. Springer, 1999.
[16] R. Mazumder, J. Friedman, and T. Hastie. Sparsenet: Coordinate descent with nonconvex penalties.
Journal of the American Statistical Association, 106(495), 2011.
[17] P. Olsen, F. Oztoprak, J. Nocedal, and S. Rennie. Newton-like methods for sparse inverse covariance
estimation. In Advances in Neural Information Processing Systems (NIPS), pages 764â€“772, 2012.
[18] A. Rakotomamonjy, R. Flamary, and G. Gasso. Dc proximal newton for non-convex optimization problems. 2014.
[19] S. Shevade and S. Keerthi. A simple and efficient algorithm for gene selection using sparse logistic
regression. Bioinformatics, 19(17):2246, 2003.
[20] X. Tan, W. Roberts, J. Li, and P. Stoica. Sparse learning via iterative minimization with application to
mimo radar imaging. IEEE Transactions on Signal Processing, 59(3):1088â€“1101, 2011.
[21] P. Tao and L. An. The dc (difference of convex functions) programming and dca revisited with dc models
of real world nonconvex optimization problems. Annals of Operations Research, 133(1-4):23â€“46, 2005.
[22] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma. Robust face recognition via sparse representation.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(2):210â€“227, 2008.
[23] C. Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics,
38(2):894â€“942, 2010.
[24] C. Zhang and T. Zhang. A general theory of concave regularization for high-dimensional sparse estimation
problems. Statistical Science, 27(4):576â€“593, 2012.
[25] T. Zhang. Analysis of multi-stage convex relaxation for sparse regularization. JMLR, 11:1081â€“1107,
2010.
[26] T. Zhang. Multi-stage convex relaxation for feature selection. Bernoulli, 2012.
[27] H. Zou and R. Li. One-step sparse estimates in nonconcave penalized likelihood models. Annals of
Statistics, 36(4):1509, 2008.

9

