Structured Estimation with Atomic Norms:
General Bounds and Applications

Sheng Chen
Arindam Banerjee
Dept. of Computer Science & Engg., University of Minnesota, Twin Cities
{shengc,banerjee}@cs.umn.edu

Abstract
For structured estimation problems with atomic norms, recent advances in the literature express sample complexity and estimation error bounds in terms of certain
geometric measures, in particular Gaussian width of the unit norm ball, Gaussian
width of a spherical cap induced by a tangent cone, and a restricted norm compatibility constant. However, given an atomic norm, bounding these geometric
measures can be difficult. In this paper, we present general upper bounds for such
geometric measures, which only require simple information of the atomic norm under consideration, and we establish tightness of these bounds by providing
the corresponding lower bounds. We show applications of our analysis to certain
atomic norms, especially k-support norm, for which existing result is incomplete.

1

Introduction

Accurate recovery of structured sparse signal/parameter vectors from noisy linear measurements has
been extensively studied in the field of compressed sensing, statistics, etc. The goal is to recover
a high-dimensional signal (parameter) Î¸ âˆ— âˆˆ Rp which is sparse (only has a few nonzero entries),
possibly with additional structure such as group sparsity. Typically one assume linear models, y =
XÎ¸ âˆ— + Ï‰, in which X âˆˆ RnÃ—p is the design matrix consisting of n samples, y âˆˆ Rn is the observed
response vector, and Ï‰ âˆˆ Rn is an unknown noise vector. By leveraging the sparsity of Î¸ âˆ— , previous
work has shown that certain L1 -norm based estimators [22, 7, 8] can find a good approximation of
Î¸ âˆ— using sample size n  p. Recent work has extended the notion of unstructured sparsity to other
structures in Î¸ âˆ— which can be captured or approximated by some norm R(Â·) [10, 18, 3, 11, 6, 19]
other than L1 , e.g., (non)overlapping group sparsity with L1 /L2 norm [24, 15], etc. In general, two
broad classes of estimators are considered in recovery analysis: (i) Lasso-type estimators [22, 18, 3],
which solve the regularized optimization problem
Î¸Ì‚Î»n = argmin
Î¸âˆˆRp

1
kXÎ¸ âˆ’ yk22 + Î»n R(Î¸) ,
2n

(1)

and (ii) Dantzig-type estimators [7, 11, 6], which solve the constrained problem
Î¸Ì‚Î»n = argmin R(Î¸) s.t. Râˆ— (XT (XÎ¸ âˆ’ y)) â‰¤ Î»n ,

(2)

Î¸âˆˆRp

where Râˆ— (Â·) is the dual norm of R(Â·). Variants of these estimators exist [10, 19, 23], but the recovery
analysis proceeds along similar lines as these two classes of estimators.
To establish recovery guarantees, [18] focused on Lasso-type estimators and R(Â·) from the class of
decomposable norm, e.g., L1 , non-overlapping L1 /L2 norm. The upper bound for the estimation
error kÎ¸Ì‚Î»n âˆ’Î¸ âˆ— k2 for any decomposable norm is characterized in terms of three geometric measures:
(i) a dual norm bound, as an upper bound for Râˆ— (XT Ï‰), (ii) sample complexity, the minimal sample
size needed for a certain restricted eigenvalue (RE) condition to be true [4, 18], and (iii) a restricted
1

norm compatibility constant between R(Â·) and L2 norms [18, 3]. The non-asymptotic estimation
âˆš
error bound typically has the form kÎ¸Ì‚Î»n âˆ’ Î¸ âˆ— ||2 â‰¤ c/ n, where c depends on a product of dual
norm bound and restricted norm compatibility, whereas the sample complexity characterizes the
minimum number of samples after which the error bound starts to be valid. In recent work, [3]
extended the analysis of Lasso-type estimator for decomposable norm to any norm, and gave a more
succinct characterization of the dual norm bound for Râˆ— (XT Ï‰) and the sample complexity for the
RE condition in terms of Gaussian widths [14, 10, 20, 1] of suitable sets where, for any set A âˆˆ Rp ,
the Gaussian width is defined as
w(A) = E sup hu, gi ,
(3)
uâˆˆA

where g is a standard Gaussian random vector. For Dantzig-type estimators, [11, 6] obtained similar
extensions. To be specific, assume entries in X and Ï‰ are i.i.d. normal, and define the tangent cone,
TR (Î¸ âˆ— ) = cone {u âˆˆ Rp | R(Î¸ âˆ— + u) â‰¤ R(Î¸ âˆ— )} .
(4)
âˆš
Then one can get (high-probability) upper bound for Râˆ— (XT Ï‰) as O( nw(â„¦R )) where â„¦R = {u âˆˆ
Rp |R(u) â‰¤ 1} is the unit norm ball, and the RE condition is satisfied with O(w2 (TR (Î¸ âˆ— ) âˆ© Spâˆ’1 ))
samples, in which Spâˆ’1 is the unit sphere. For convenience, we denote by CR (Î¸ âˆ— ) the spherical
cap TR (Î¸ âˆ— ) âˆ© Spâˆ’1 throughout the paper. Further, the restricted norm compatibility is given by
Î¨R (Î¸ âˆ— ) = supuâˆˆTR (Î¸âˆ— ) R(u)
kuk2 (see Section 2 for details).
Thus, for any given norm, it suffices to get a characterization of (i) w(â„¦R ), the width of the unit norm ball, (ii) w(CR (Î¸ âˆ— )), the width of the spherical cap induced by the tangent cone TR (Î¸ âˆ— ),
and (iii) Î¨R (Î¸ âˆ— ), the restricted norm compatibility in the tangent cone. For the special case of L1
norm, accurate characterization of all three measures exist [10, 18]. However, for more general
norms, the literature is rather limited. For w(â„¦R ), the characterization is often reduced to comparison with either w(CR (Î¸ âˆ— )) [3] or known results on other norm balls [13]. While w(CR (Î¸ âˆ— ))
has been investigated for certain decomposable norms [10, 9, 1], little is known about general nondecomposable norms. One general approach for upper bounding w(CR (Î¸ âˆ— )) is via the statistical
dimension [10, 19, 1], which computes the expected squared distance between a Gaussian random
vector and the polar cone of TR (Î¸ âˆ— ). To specify the polar, one need full information of the subdifferential âˆ‚R(Î¸ âˆ— ), which could be difficult to obtain for non-decomposable norms. A notable
bound for (overlapping) L1 /L2 norms is presented in [21], which yields tight bounds for mildly
non-overlapping cases, but is loose for highly overlapping ones. For Î¨R (Î¸ âˆ— ), the restricted norm
compatibility, results are only available for decomposable norms [18, 3].
In this paper, we present a general set of bounds for the width w(â„¦R ) of the norm ball, the width
w(CR (Î¸ âˆ— )) of the spherical cap, and the restricted norm compatibility Î¨R (Î¸ âˆ— ). For the analysis,
we consider the class of atomic norms that are invariant under sign-changes, i.e., the norm of a
vector stays unchanged if any entry changes only by flipping its sign. The class is quite general, and
covers most of the popular norms used in practical applications, e.g., L1 norm, ordered weighted
L1 (OWL) norm [5] and k-support norm [2]. Specifically we show that sharp bounds on w(â„¦R )
can be obtained using simple calculation based on a decomposition inequality from [16]. To upper
bound w(CR (Î¸ âˆ— )) and Î¨R (Î¸ âˆ— ), instead of a full specification of TR (Î¸ âˆ— ), we only require some
information regarding the subgradient of R(Î¸ âˆ— ), which is often readily accessible. The key insight
is that bounding statistical dimension often ends up computing the expected distance from Gaussian
vector to a single point rather than to the whole polar cone, thus the full information on âˆ‚R(Î¸ âˆ— ) is
unnecessary. In addition, we derive the corresponding lower bounds to show the tightness of our
results. As examples, we illustrate the bounds for L1 and OWL norms [5]. Finally, we give sharp
bounds for the recently proposed k-support norm [2], for which existing analysis is incomplete.
The rest of the paper is organized as follows: we first review the relevant background for Dantzigtype estimator and atomic norm in Section 2. In Section 3, we introduce the general bounds for the
geometric measures. In Section 4, we discuss the tightness of our bounds. Section 5 is dedicated to
the example of k-support norm, and we conclude in Section 6.

2

Background

In this section, we briefly review the recovery guarantee for the generalized Dantzig selector in (2)
and the basics on atomic norms. The following lemma, originally [11, Theorem 1], provides an error
bound for kÎ¸Ì‚Î»n âˆ’ Î¸ âˆ— k2 . Related results have appeared for other estimators [18, 10, 19, 3, 23].
2

Lemma 1 Assume that y = XÎ¸ âˆ— + Ï‰,
âˆš where entries of X and Ï‰ are i.i.d. copies of standard
Gaussian random variable. If Î»n â‰¥ c1 nw(â„¦R ) and n > c2 w2 (TR (Î¸ âˆ— ) âˆ© Spâˆ’1 ) = w2 (CR (Î¸ âˆ— ))
for some constant c1 , c2 > 1, with high probability, the estimate Î¸Ì‚Î»n given by (2) satisfies


âˆ—
âˆ— w(â„¦R )
kÎ¸Ì‚Î»n âˆ’ Î¸ k2 â‰¤ O Î¨R (Î¸ ) âˆš
.
(5)
n
In this Lemma, there are three geometric measuresâ€”w(â„¦R ), w(CR (Î¸ âˆ— )) and Î¨R (Î¸ âˆ— )â€”which need
to be determined for specific R(Â·) and Î¸ âˆ— . In this work, we focus on general atomic norms R(Â·).
Given a set of atomic vectors A âŠ‚ Rp , the corresponding atomic norm of any Î¸ âˆˆ Rp is given by
(
)
X
X
kÎ¸kA = inf
ca : Î¸ =
ca a, ca â‰¥ 0 âˆ€ a âˆˆ A
(6)
aâˆˆA

aâˆˆA

In order for k Â· kA to be a valid norm, atomic vectors in A has to span Rp , and a âˆˆ A iff âˆ’a âˆˆ A.
The unit ball of atomic norm k Â· kA is given by â„¦A = conv(A). In addition, we assume that the
atomic set A contains v  a for any v âˆˆ {Â±1}p if a belongs to A, where  denotes the elementwise
(Hadamard) product for vectors. This assumption guarantees that both k Â· kA and its dual norm
are invariant under sign-changes, which is satisfied by many widely used norms, such as L1 norm,
OWL norm [5] and k-support norm [2]. For the rest of the paper, we will use â„¦A , TA (Î¸ âˆ— ), CA (Î¸ âˆ— )
and Î¨A (Î¸ âˆ— ) with A replaced by appropriate subscript for specific norms. For any vector u and
coordinate set S, we define uS by zeroing out all the coordinates outside S.

3

General Analysis for Atomic Norms

In this section, we present detailed analysis of the general bounds for the geometric measures,
w(â„¦A ), w(CA (Î¸ âˆ— )) and Î¨A (Î¸ âˆ— ). In general, knowing the atomic set A is sufficient for bounding w(â„¦A ). For w(CA (Î¸ âˆ— )) and Î¨A (Î¸ âˆ— ), we only need a single subgradient of kÎ¸ âˆ— kA and some
simple additional calculations.
3.1

Gaussian width of unit norm ball

Although the atomic set A may contain uncountably many vectors, we assume that A can be decomposed as a union of M â€œsimpleâ€ sets, A = A1 âˆª A2 âˆª . . . âˆª AM . By â€œsimple,â€ we mean the
Gaussian width of each Ai is easy to compute/bound. Such a decomposition assumption is often
satisfied by commonly used atomic norms, e.g., L1 , L1 /L2 , OWL, k-support norm. The Gaussian
width of the unit norm ball of k Â· kA can be easily obtained using the following lemma, which is
essentially the Lemma 2 in [16]. Related results appear in [16].
Lemma 2 Let M > 4, A1 , Â· Â· Â· , AM âŠ‚ Rp , and A = âˆªm Am . The Gaussian width of unit norm
ball of k Â· kA satisfies
p
w(â„¦A ) = w(conv(A)) = w(A) â‰¤ max w(Am ) + 2 sup kzk2 log M
(7)
1â‰¤mâ‰¤M

zâˆˆA

Next we illustrate application of this result to bounding the width of the unit norm ball of L1 and
OWL norm.
Example 1.1 (L1 norm): Recall that the L1 norm can be viewed as the atomic norm induced by the
set AL1 = {Â±ei : 1 â‰¤ i â‰¤ p}, where {ei }pi=1 is the canonical basis of Rp . Since the Gaussian
width of a singleton is 0, if we treat A as the union of individual {+ei } and {âˆ’ei }, we have
p
p
(8)
w(â„¦L1 ) â‰¤ 0 + 2 log 2p = O( log p) .
Example 1.2 (OWL norm): A recent variant of L1 norm is the so-called ordered weighted L1
Pp
(OWL) norm [13, 25, 5] defined as kÎ¸kowl = i=1 wi |Î¸|â†“i , where w1 â‰¥ w2 â‰¥ . . . â‰¥ wp â‰¥ 0 are
pre-specified ordered weights, and |Î¸|â†“ is the permutation of |Î¸| with entries sorted in decreasing
order. In [25], the OWL norm is proved to be an atomic norm with atomic set
(
)
[
[
[
vS
p
p
Aowl =
Ai =
u âˆˆ R : uS c = 0, uS = Pi
, v âˆˆ {Â±1}
. (9)
j=1 wj
1â‰¤iâ‰¤p
1â‰¤iâ‰¤p | supp(S)|=i
3


We first apply Lemma 2 to each set Ai , and note that each Ai contains 2i pi atomic vectors.
s
s
r
r
 
p
p
i
p
2i
2
i
w(Ai ) â‰¤ 0 + 2
log 2
â‰¤ Pi
2 + log
â‰¤
2 + log
,
Pi
i
i
wÌ„
i
( j=1 wj )2
j=1 wj
where wÌ„ is the average of w1 , . . . , wp . Then we apply the lemma again to Aowl and obtain
âˆš

2p
2p
log p
w(â„¦owl ) = w(Aowl ) â‰¤
2 + log p +
log p = O
,
wÌ„
wÌ„
wÌ„

(10)

which matches the result in [13].
3.2

Gaussian width of the intersection of tangent cone and unit sphere

In this subsection, we consider the computation of general w(CA (Î¸ âˆ— )). Using the definition of dual
norm, we can write kÎ¸ âˆ— kA as kÎ¸ âˆ— kA = supkukâˆ—A â‰¤1 hu, Î¸ âˆ— i, where k Â· kâˆ—A denotes the dual norm of
k Â· kA . The uâˆ— for which huâˆ— , Î¸ âˆ— i = kÎ¸ âˆ— kA , is a subgradient of kÎ¸ âˆ— kA . One can obtain uâˆ— by
simply solving the so-called polar operator [26] for the dual norm k Â· kâˆ—A ,
uâˆ— âˆˆ argmax hu, Î¸ âˆ— i .

(11)

kukâˆ—
A â‰¤1

Based on polar operator, we start with the Lemma 3, which plays a key role in our analysis.
Lemma 3 Let uâˆ— bePa solution to the polar operator (11), and define the weighted L1 semi-norm
p
k Â· kuâˆ— as kvkuâˆ— = i=1 |uâˆ—i | Â· |vi |. Then the following relation holds
TA (Î¸ âˆ— ) âŠ† Tuâˆ— (Î¸ âˆ— ) ,
where Tuâˆ— (Î¸ âˆ— ) = cone{v âˆˆ Rp | kÎ¸ âˆ— + vkuâˆ— â‰¤ kÎ¸ âˆ— kuâˆ— }.
The proof of this lemma is in supplementary material. Note that the solution to (11) may not be
unique. A good criterion for choosing uâˆ— is to avoid zeros in uâˆ— , as any uâˆ—i = 0 will lead to the
unboundedness of unit ball of k Â· kuâˆ— , which could potentially increase the size of Tuâˆ— (Î¸ âˆ— ). Next we
present the upper bound for w(CA (Î¸ âˆ— )).
Theorem 4 Suppose that uâˆ— is one of the solutions to (11), and define the following sets,
Q = {i | uâˆ—i = 0},

S = {i | uâˆ—i 6= 0, Î¸iâˆ— 6= 0},

R = {i | uâˆ—i 6= 0, Î¸iâˆ— = 0} .

The Gaussian width w(CA (Î¸ âˆ— )) is upper bounded by
ï£± âˆš
ï£´
ï£² p , if R is empty
w(CA (Î¸ âˆ— )) â‰¤

ï£´
ï£³

q

m + 23 s +

2Îº2max
s log
Îº2min

,
pâˆ’m
s



(12)

, if R is nonempty

where m = |Q|, s = |S|, Îºmin = miniâˆˆR |uâˆ—i | and Îºmax = maxiâˆˆS |uâˆ—i |.
Proof: By Lemma 3, we have w(CA (Î¸ âˆ— )) â‰¤ w(Tuâˆ— (Î¸ âˆ— ) âˆ© Spâˆ’1 ) , w(Cuâˆ— (Î¸ âˆ— )). Hence we can
focus on bounding w(Cuâˆ— (Î¸ âˆ— )). We first analyze the structure of v that satisfies kÎ¸ âˆ— + vkuâˆ— â‰¤
kÎ¸ âˆ— kuâˆ— . For the coordinates Q = {i | uâˆ—i = 0}, the corresponding entries vi â€™s can be arbitrary since
it does not affect the value of kÎ¸ âˆ— + vkuâˆ— . Thus all possible vQ form a m-dimensional subspace,
âˆ—
where m = |Q|. For S âˆª R = {i | uâˆ—i 6= 0}, we define Î¸Ìƒ = Î¸SâˆªR
and vÌƒ = vSâˆªR , and vÌƒ needs to
satisfy
kvÌƒ + Î¸Ìƒkuâˆ— â‰¤ kÎ¸Ìƒkuâˆ— ,
which is similar to the L1 -norm tangent cone except that coordinates are weighted by |uâˆ— |. Therefore
we use the techniques for proving the Proposition 3.10 in [10]. Based on the structure of v, The
normal cone at Î¸ âˆ— for Tuâˆ— (Î¸ âˆ— ) is given by
N (Î¸ âˆ— ) = {z : hz, vi â‰¤ 0 âˆ€v s.t. kv + Î¸ âˆ— kuâˆ— â‰¤ kÎ¸ âˆ— kuâˆ— }
= {z : zi = 0 for i âˆˆ Q, zi = |uâˆ—i |sign(Î¸Ìƒi )t for i âˆˆ S, |zi | â‰¤ |uâˆ—i |t for i âˆˆ R, for any t â‰¥ 0} .
4

Given a standard Gaussian random vector g, using the relation between Gaussian width and statistical dimension (Proposition 2.4 and 10.2 in [1]), we have
X
X
X
w2 (Cuâˆ— (Î¸ âˆ— )) â‰¤ E[ inf âˆ— kz âˆ’ gk22 ] = E[ inf âˆ—
gi2 +
(zj âˆ’ gj )2 +
(zk âˆ’ gk )2 ]
zâˆˆN (Î¸ )

= |Q| + E[
2

â‰¤ |Q| + t

zâˆˆN (Î¸ )

X

inf

zSâˆªR âˆˆN (Î¸ âˆ— )

X

|uâˆ—j |2

iâˆˆQ

jâˆˆS

(|uâˆ—j |sign(Î¸Ìƒj )t âˆ’ gj )2 +

jâˆˆS

(zk âˆ’ gk )2 ]

kâˆˆR

+ |S| + E[

jâˆˆS

kâˆˆR

X

X
kâˆˆR

inf

(zk âˆ’ gk )2 ]

|zk |â‰¤|uâˆ—
k |t

+âˆž

âˆ’g 2
(gk âˆ’ |uâˆ—k |t)2 exp( k )dgk
2
|uâˆ—
k |t


X
X 2
1
|uâˆ—k |2 t2
âˆš
â‰¤ |Q| + t2
|uâˆ—j |2 + |S| +
exp
âˆ’
(âˆ—) .
2
2Ï€ |uâˆ—k |t

2
âˆš
â‰¤ |Q| + t2
|uâˆ—j |2 + |S| +
2Ï€
jâˆˆS
kâˆˆR
X

X

jâˆˆS

kâˆˆR

Z

!

The details for the derivation above can be found in Appendix C of [10]. If R is empty, by taking
t = 0, we have
X
(âˆ—) â‰¤ |Q| + t2
|uâˆ—j |2 + |S| = |Q| + |S| = p .
jâˆˆS

If Rris nonempty, we denote Îºmin = miniâˆˆR |uâˆ—i | and Îºmax = maxiâˆˆS |uâˆ—i |. Taking t =


|SâˆªR|
1
2
log
, we obtain
Îºmin
|S|
 2 2
Îº
t

 2


2|R| exp âˆ’ min
2
|S âˆª R|
2Îºmax
2
2
âˆš
log
(âˆ—) â‰¤ |Q| + |S|(Îºmax t + 1) +
= |Q| + |S|
+1
Îº2min
|S|
2Ï€Îºmin t


|R||S|
2Îº2max
|S âˆª R|
3
r
+
â‰¤
|Q|
+
|S|
log
+ |S| .


2
Îºmin
|S|
2
|S âˆª R| Ï€ log |SâˆªR|
|S|
Substituting |Q| = m, |S| = s and |S âˆª R| = p âˆ’ m into the last inequality completes the proof.
Suppose that Î¸ âˆ— is a s-sparse vector. We illustrate the above bound on the Gaussian width of the
spherical cap using L1 norm and OWL norm as examples.
Example 2.1 (L1 norm): The dual norm of L1 is Lâˆž norm, and its easy to verify that uâˆ— =
[1, 1, . . . , 1]T âˆˆ Rp is a solution to (11). Applying Theorem 4 to uâˆ— , we have
r
r
p
 p 
3
âˆ—
w(CL1 (Î¸ )) â‰¤
s + 2s log
=O
s + s log
.
2
s
s
Example 2.2 (OWL norm): For OWL, its dual norm is given by kukâˆ—owl = maxbâˆˆAowl hb, ui.
W.l.o.g. we assume Î¸ âˆ— = |Î¸ âˆ— |â†“ , and a solution to (11) is given by uâˆ— = [w1 , . . . , ws , wÌƒ, wÌƒ, . . . , wÌƒ]T ,
in which wÌƒ is the average of ws+1 , . . . , wp . If all wi â€™s are nonzero, the Gaussian width satisfies
r
p
3
2w2
âˆ—
w(Cowl (Î¸ )) â‰¤
s + 21 s log
.
2
wÌƒ
s
3.3

Restricted norm compatibility

The next theorem gives general upper bounds for the restricted norm compatibility Î¨A (Î¸ âˆ— ).
Theorem 5 Assume that kukA â‰¤ max{Î²1 kuk1 , Î²2 kuk2 } for all u âˆˆ Rp . Under the setting of
Theorem 4, the restricted norm compatibility Î¨A (Î¸ âˆ— ) is upper bounded by
(
Î¦ , if R isnempty 
âˆš o
âˆ—
Î¨A (Î¸ ) â‰¤
,
(13)
Î¦Q + max Î²2 , Î²1 1 + ÎºÎºmax
s , if R is nonempty
min
where Î¦ = supuâˆˆRp

kukA
kuk2

and Î¦Q = supsupp(u)âŠ†Q
5

kukA
kuk2 .

Proof: As analyzed in the proof of Theorem 4, vQ for v âˆˆ Tuâˆ— (Î¸ âˆ— ) can be arbitrary, and the
vSâˆªR = vQc satisfies
X
X
X
âˆ—
âˆ—
kvQc + Î¸Q
=â‡’
|Î¸iâˆ— + vi ||uâˆ—i | +
|vj ||uâˆ—j | â‰¤
|Î¸iâˆ— ||uâˆ—i |
c kuâˆ— â‰¤ kÎ¸Qc kuâˆ—
iâˆˆS

=â‡’

X

(|Î¸iâˆ— | âˆ’ |vi |) |uâˆ—i | +

iâˆˆS

X

|vj ||uâˆ—j | â‰¤

jâˆˆR

jâˆˆR

X

|Î¸iâˆ— ||uâˆ—i |

=â‡’

iâˆˆS

Îºmin kvR k1 â‰¤ Îºmax kvS k1

iâˆˆS

If R is empty, by Lemma 3, we obtain
Î¨A (Î¸ âˆ— ) â‰¤ Î¨uâˆ— (Î¸ âˆ— ) ,

kvkA
kvkA
â‰¤ sup
=Î¦.
vâˆˆRp kvk2
vâˆˆTuâˆ— (Î¸ âˆ— ) kvk2
sup

If R is nonempty, we have
Î¨A (Î¸ âˆ— ) â‰¤ Î¨uâˆ— (Î¸ âˆ— ) â‰¤

kvQ kA + kvQc kA
kvkA + kv0 kA
â‰¤
sup
kvk2
kv + v0 k2
vâˆˆTuâˆ— (Î¸ âˆ— )
supp(v)âŠ†Q, supp(v0 )âŠ†Qc
sup

0
0
Îºmin kvR
k1 â‰¤Îºmax kvS
k1

â‰¤

kvkA
+
supp(v)âŠ†Q kvk2
sup

â‰¤ Î¦Q +

max{Î²1 kv0 k1 , Î²2 kv0 k2 }
kv0 k2

sup

supp(v0 )âŠ†Qc
0
0
Îºmin kvR
k1 â‰¤Îºmax kvS
k1
Îºmax
Î²(1 + Îºmin )kv0 k1
max{Î²2 ,
sup
}
kv0 k2
supp(v0 )âŠ†S


â‰¤ Î¦Q + max{Î²2 , Î²1

Îºmax
1+
Îºmin



âˆš

s} ,

in which the last inequality in the first line uses the property of Tuâˆ— (Î¸ âˆ— ).
Remark: We call Î¦ the unrestricted norm compatibility, and Î¦Q the subspace norm compatibility,
both of which are often easier to compute than Î¨A (Î¸ âˆ— ). The Î²1 and Î²2 in the assumption of k Â· kA
can have multiple choices, and one has the flexibility to choose the one that yields the tightest bound.
Example 3.1 (L1 norm): To apply the Theorem 5 to L1 norm, we can choose Î²1 = 1 and Î²2 = 0.
We recall the uâˆ— for L1 norm, whose Q is empty while R is nonempty. So we have for s-sparse Î¸ âˆ—
 
 
âˆš
1 âˆš
s =2 s.
Î¨L1 (Î¸ âˆ— ) â‰¤ 0 + max 0, 1 +
1
Example 3.2 (OWL norm): For OWL, note that k Â· kowl â‰¤ w1 k Â· k1 . Hence we choose Î²1 = w1
and Î²2 = 0. As a result, we similarly have for s-sparse Î¸ âˆ—
n

w1  âˆš o 2w12 âˆš
Î¨owl (Î¸ âˆ— ) â‰¤ 0 + max 0, w1 1 +
s â‰¤
s.
wÌƒ
wÌƒ

4

Tightness of the General Bounds

So far we have shown that the geometric measures can be upper bounded for general atomic norms.
One might wonder how tight the bounds in Section 3 are for these measures. For w(â„¦A ), as the
result from [16] depends on the decomposition of A for the ease of computation, it might be tricky
to discuss its tightness in general. Hence we will focus on the other two, w(CA (Î¸ âˆ— )) and Î¨A (Î¸ âˆ— ).
To characterize the tightness, we need to compare the lower bounds of w(CA (Î¸ âˆ— )) and Î¨A (Î¸ âˆ— ),
with their upper bounds determined by uâˆ— . While there can be multiple uâˆ— , it is easy to see that any
convex combination of them is also a solution to (11). Therefore we can always find a uâˆ— that has
the largest support, i.e., supp(u0 ) âŠ† supp(uâˆ— ) for any other solution u0 . We will use such uâˆ— to
generate the lower bounds. First we need the following lemma for the cone TA (Î¸ âˆ— ).
Lemma 6 Consider a solution uâˆ— to (11), which satisfies supp(u0 ) âŠ† supp(uâˆ— ) for any other
solution u0 . Under the setting of notations in Theorem 4, we define an additional set of coordinates
P = {i | uâˆ—i = 0, Î¸iâˆ— = 0}. Then the tangent cone TA (Î¸ âˆ— ) satisfies
T1 âŠ• T2 âŠ† cl(TA (Î¸ âˆ— )) ,
(14)
where âŠ• denotes the direct (Minkowski) sum operation, cl(Â·) denotes the closure, T1 = {v âˆˆ
Rp | vi = 0 for i âˆˆ
/ P} is a |P|-dimensional subspace, and T2 = {v âˆˆ Rp | sign(vi ) =
âˆ—
âˆ’sign(Î¸i ) for i âˆˆ supp(Î¸ âˆ— ), vi = 0 for i âˆˆ
/ supp(Î¸ âˆ— )} is a | supp(Î¸ âˆ— )|-dimensional orthant.
6

The proof of Lemma 6 is given in supplementary material. The following theorem gives us the lower
bound for w(CA (Î¸ âˆ— )) and Î¨A (Î¸ âˆ— ).
Theorem 7 Under the setting of Theorem 4 and Lemma 6, the following lower bounds hold,
âˆš
w(CA (Î¸ âˆ— )) â‰¥ O( m + s) ,
Î¨A (Î¸ âˆ— ) â‰¥ Î¦QâˆªS .

(15)
(16)

Proof: To lower bound w(CA (Î¸ âˆ— )), we use Lemma 6 and the relation between Gaussian width and
statistical dimension (Proposition 10.2 in [1]),
r
w(TA (Î¸ âˆ— )) â‰¥ w(T1 âŠ• T2 âˆ© Spâˆ’1 ) â‰¥ E[
inf âˆ— kz âˆ’ gk22 ] âˆ’ 1 (âˆ—) ,
zâˆˆNT1 âŠ•T2 (Î¸ )

âˆ—

where the normal cone NT1 âŠ•T2 (Î¸ ) of T1 âŠ• T2 is given by NT1 âŠ•T2 (Î¸ âˆ— ) = {z : zi = 0 for i âˆˆ
P, sign(zi ) = sign(Î¸iâˆ— ) for i âˆˆ supp(Î¸ âˆ— )}. Hence we have
r
s X
X
âˆš
| supp(Î¸ âˆ— )|
2
2
gj I{gj Î¸jâˆ— <0} ] âˆ’ 1 = |P| +
gi +
âˆ’ 1 = O( m + s) ,
(âˆ—) = E[
2
âˆ—
iâˆˆP

jâˆˆsupp(Î¸ )

where the last equality follows the fact that P âˆª supp(Î¸ âˆ— ) = Q âˆª S. This completes proof of (15).
To prove (16), we again use Lemma 6 and the fact P âˆª supp(Î¸ âˆ— ) = Q âˆª S. Noting that k Â· kA is
invariant under sign-changes, we get
kvkA
kvkA
kvkA
Î¨A (Î¸ âˆ— ) = sup
â‰¥ sup
=
sup
= Î¦QâˆªS .
kvk
kvk
âˆ—
âˆ—
2
2
vâˆˆT
âŠ•T
vâˆˆTA (Î¸ )
supp(v)âŠ†Pâˆªsupp(Î¸ ) kvk2
1
2
Remark: We compare the lower bounds (15) (16) with the upper bounds (12) (13). If R is empty,
m + s = p, and the lower bounds actually match the upper bounds up to a constant factor for both
w(CA (Î¸ âˆ— )) and Î¨A (Î¸ âˆ— ). If R is nonempty, the lower and upper bounds of w(CA (Î¸ âˆ— )) differ by a
2Îº2
âˆ—
multiplicative factor Îº2max log( pâˆ’m
s ), which can be small in practice. For Î¨A (Î¸ ), as Î¦QâˆªS â‰¥ Î¦Q ,
min
âˆš
we usually have at most an additive O( s) term in upper bound, since the assumption on k Â· kA
often holds with a constant Î²1 and Î²2 = 0 for most norms.

5

Application to the k-Support Norm

In this section, we apply our general results on geometric measures to a non-trivial example, ksupport norm [2], which has been proved effective for sparse recovery [11, 17, 12]. The k-support
norm can be viewed as an atomic norm, for which A = {a âˆˆ Rp | kak0 â‰¤ k, kak2 â‰¤ 1}. The
k-support norm can be explicitly expressed as an infimum convolution given by

nX
o

kÎ¸ksp
kui k2  kui k0 â‰¤ k ,
(17)
k = P inf
i

ui =Î¸

i

and its dual norm is the so-called 2-k symmetric gauge norm defined as
âˆ—

kÎ¸ksp
= kÎ¸k(k) = k|Î¸|â†“1:k k2 ,
k

(18)

It is straightforward to see that the dual norm is simply the L2 norm of the largest k entries in |Î¸|.
Suppose that all the sets of coordinates with cardinality k can be listed as S1 , S2 , . . . , S(p) . Then A
k

can be written as A = A1 âˆª . . . âˆª A(p) , where each Ai = {a âˆˆ Rp | supp(a) âŠ† Si , kak2 â‰¤ 1}.
k
p
âˆš


It is not difficult to see that w(Ai ) = E supaâˆˆAi ha, gi = EkgSi k2 â‰¤ EkgSi k22 â‰¤ k. Using
Lemma 2, we know the Gaussian width of the unit ball of k-support norm
s  
r
r

p
p
âˆš
âˆš
p
sp
w(â„¦k ) â‰¤ k + 2 log
â‰¤ k + 2 k log
+k =O
k log
+k ,
(19)
k
k
k
âˆ—
which matches that in [11]. Now we turn to the calculation of w(Cksp (Î¸ âˆ— )) and Î¨sp
k (Î¸ ). As we have
âˆ—
seen in the general analysis, the solution u to the polar operator (11) is important in characterizing
the two quantities. We first present a simple procedure in Algorithm 1 for solving the polar operator
âˆ—
for k Â· ksp
procedure can be utilized to compute
k . The time complexity is only O(p log p + k). This
âˆ—
the k-support norm, or be applied to estimation with k Â· ksp
using
generalized conditional gradient
k
method [26], which requires solving the polar operator in each iteration.

7

âˆ—

Algorithm 1 Solving polar operator for k Â· ksp
k
Input: Î¸ âˆ— âˆˆ Rp , positive integer k
Output: solution uâˆ— to the polar operator (11)
1: z = |Î¸ âˆ— |â†“ , t = 0
2: for i = 1 to k do
3:
Î³1 = kz1:iâˆ’1 k2 , Î³2 = kzi:p k1 , d = k âˆ’ i + 1, Î² = âˆš
4:

if

Î³12
2Î±

Î³2
,
Î³22 d+Î³12 d2

Î± = âˆš Î³1
2

1âˆ’Î² 2 d

,w=

z1:iâˆ’1
2Î±

+ Î²Î³2 > t and Î² < wiâˆ’1 then
Î³2

5:
t = 2Î±1 + Î²Î³2 , uâˆ— = [w, Î²1]T (1 is (p âˆ’ i + 1)-dimensional vector with all ones)
6:
end if
7: end for
8: change the sign and order of uâˆ— to conform with Î¸ âˆ—
9: return uâˆ—
âˆ—

Theorem 8 For a given Î¸ âˆ— , Algorithm 1 returns a solution to polar operator (11) for k Â· ksp
k .
The proof of this theorem is provided in supplementary material. Now we consider w(Cksp (Î¸ âˆ— ))
âˆ—
âˆ—
âˆ—
âˆ—
and Î¨sp
k (Î¸ ) for s-sparse Î¸ (here s-sparse Î¸ means | supp(Î¸ )| = s) in three scenarios: (i) overspecified k, where s < k, (ii) exactly specified k, where s = k, and (iii) under-specified k, where
s > k. The bounds are given in Theorem 9, and the proof is also in supplementary material.
Theorem 9 For given s-sparse Î¸ âˆ— âˆˆ Rp , the Gaussian width w(Cksp (Î¸ âˆ— )) and the restricted norm
âˆ—
compatibility Î¨sp
k (Î¸ ) for a specified k are given by
ï£± q
ï£± âˆš
p , if s < k
2p
ï£´
ï£´
ï£´
ï£´
k , if s < k
ï£´
ï£´
ï£´
ï£´
r
ï£´
ï£´
ï£´
ï£´
ï£² âˆš
ï£²

âˆ—2
2Î¸max
p
3
Î¸âˆ—
sp âˆ—
2(1 + Î¸max
) , if s = k
,
w(Cksp (Î¸ âˆ— )) â‰¤
2 s + Î¸ âˆ—2 s log s , if s = k , Î¨k (Î¸ ) â‰¤
âˆ—
min
min
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´ q
q
ï£´
ï£´

ï£´
ï£´
2Îº2
ï£³ (1 + Îºmax ) 2s , if s > k
ï£³
3
s + max s log p , if s > k
2

Îº2min

Îºmin

s

âˆ—
âˆ—
= maxiâˆˆsupp(Î¸âˆ— ) |Î¸iâˆ— | and Î¸min
= miniâˆˆsupp(Î¸âˆ— ) |Î¸iâˆ— |.
where Î¸max

k

(20)

sp âˆ—
âˆ—
Remark: Previously Î¨sp
k (Î¸ ) is unknown and the bound on w(Ck (Î¸ )) given in [11] is loose, as
it used the result in [21]. Based on Theorem 9, we note that the choice of k can affect the recovery
guarantees. Over-specified k leads to a direct dependence on the dimensionality p for w(Cksp (Î¸ âˆ— ))
âˆ—
and Î¨sp
k (Î¸ ), resulting in a weak error bound. The bounds are sharp for exactly specified or underspecified k. Thus, it is better to under-specify k in practice. where the estimation error satifies
!
r
s + s log (p/k)
âˆ—
(21)
kÎ¸Ì‚Î»n âˆ’ Î¸ k2 â‰¤ O
n

6

Conclusions

In this work, we study the problem of structured estimation with general atomic norms that are
invariant under sign-changes. Based on Dantzig-type estimators, we provide the general bounds
for the geometric measures. In terms of w(â„¦A ), instead of comparison with other results or direct
calculation, we demonstrate a third way to compute it based on decomposition of atomic set A.
For w(CA (Î¸ âˆ— )) and Î¨A (Î¸ âˆ— ), we derive general upper bounds, which only require the knowledge
of a single subgradient of kÎ¸ âˆ— kA . We also show that these upper bounds are close to the lower
bounds, which makes them practical in general. To illustrate our results, we discuss the application
to k-support norm in details and shed light on the choice of k in practice.

Acknowledgements
The research was supported by NSF grants IIS-1447566, IIS-1422557, CCF-1451986, CNS1314560, IIS-0953274, IIS-1029711, and by NASA grant NNX12AQ39A.
8

References
[1] D. Amelunxen, M. Lotz, M. B. McCoy, and J. A. Tropp. Living on the edge: Phase transitions in convex
programs with random data. Inform. Inference, 3(3):224â€“294, 2014.
[2] A. Argyriou, R. Foygel, and N. Srebro. Sparse prediction with the k-support norm. In Advances in Neural
Information Processing Systems (NIPS), 2012.
[3] A. Banerjee, S. Chen, F. Fazayeli, and V. Sivakumar. Estimation with norm regularization. In Advances
in Neural Information Processing Systems (NIPS), 2014.
[4] P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. The
Annals of Statistics, 37(4):1705â€“1732, 2009.
[5] M. Bogdan, E. van den Berg, W. Su, and E. Candes. Statistical estimation and testing via the sorted L1
norm. arXiv:1310.1969, 2013.
[6] T. T. Cai, T. Liang, and A. Rakhlin. Geometrizing Local Rates of Convergence for High-Dimensional
Linear Inverse Problems. arXiv:1404.4408, 2014.
[7] E. Candes and T Tao. The Dantzig selector: statistical estimation when p is much larger than n. The
Annals of Statistics, 35(6):2313â€“2351, 2007.
[8] E. J. CandeÌ€s, J. K. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate measurements. Communications on Pure and Applied Mathematics, 59(8):1207â€“1223, 2006.
[9] E. J. Cands and B. Recht. Simple bounds for recovering low-complexity models. Math. Program., 141(12):577â€“589, 2013.
[10] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse
problems. Foundations of Computational Mathematics, 12(6):805â€“849, 2012.
[11] S. Chatterjee, S. Chen, and A. Banerjee. Generalized dantzig selector: Application to the k-support norm.
In Advances in Neural Information Processing Systems (NIPS), 2014.
[12] S. Chen and A. Banerjee. One-bit compressed sensing with the k-support norm. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2015.
[13] M. A. T. Figueiredo and R. D. Nowak. Sparse estimation with strongly correlated variables using ordered
weighted l1 regularization. arXiv:1409.4005, 2014.
[14] Y. Gordon. Some inequalities for gaussian processes and applications. Israel Journal of Mathematics,
50(4):265â€“289, 1985.
[15] L. Jacob, G. Obozinski, and J.-P. Vert. Group lasso with overlap and graph lasso. In International
Conference on Machine Learning (ICML), 2009.
[16] A. Maurer, M. Pontil, and B. Romera-Paredes. An Inequality with Applications to Structured Sparsity
and Multitask Dictionary Learning. In Conference on Learning Theory (COLT), 2014.
[17] A. M. McDonald, M. Pontil, and D. Stamos. Spectral k-support norm regularization. In Advances in
Neural Information Processing Systems (NIPS), 2014.
[18] S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for the analysis of
regularized M -estimators. Statistical Science, 27(4):538â€“557, 2012.
[19] S. Oymak, C. Thrampoulidis, and B. Hassibi. The Squared-Error of Generalized Lasso: A Precise Analysis. arXiv:1311.0830, 2013.
[20] Y. Plan and R. Vershynin. Robust 1-bit compressed sensing and sparse logistic regression: A convex
programming approach. IEEE Transactions on Information Theory, 59(1):482â€“494, 2013.
[21] N. Rao, B. Recht, and R. Nowak. Universal Measurement Bounds for Structured Sparse Signal Recovery.
In International Conference on Artificial Intelligence and Statistics (AISTATS), 2012.
[22] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society,
Series B, 58(1):267â€“288, 1996.
[23] J. A. Tropp. Convex recovery of a structured signal from independent random linear measurements. In
Sampling Theory, a Renaissance. 2015.
[24] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the
Royal Statistical Society, Series B, 68:49â€“67, 2006.
[25] X. Zeng and M. A. T. Figueiredo. The Ordered Weighted `1 Norm: Atomic Formulation, Projections, and
Algorithms. arXiv:1409.4271, 2014.
[26] X. Zhang, Y. Yu, and D. Schuurmans. Polar operators for structured sparse estimation. In Advances in
Neural Information Processing Systems (NIPS), 2013.

9

