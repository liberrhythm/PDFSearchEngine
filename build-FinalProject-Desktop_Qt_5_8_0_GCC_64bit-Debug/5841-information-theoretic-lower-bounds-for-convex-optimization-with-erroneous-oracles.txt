Information-theoretic lower bounds for convex
optimization with erroneous oracles

Jan VondraÃÅk
IBM Almaden Research Center
San Jose, CA 95120
jvondrak@us.ibm.com

Yaron Singer
Harvard University
Cambridge, MA 02138
yaron@seas.harvard.edu

Abstract
We consider the problem of optimizing convex and concave functions with access
to an erroneous zeroth-order oracle. In particular, for a given function x ‚Üí f (x)
we consider optimization when one is given access to absolute error oracles that
return values in [f (x) ‚àí , f (x) + ] or relative error oracles that return value in
[(1 ‚àí )f (x), (1 + )f (x)], for some  > 0. We show stark information theoretic
impossibility results for minimizing convex functions and maximizing concave
functions over polytopes in this model.

1

Introduction

Consider the problem of minimizing a convex function over some convex domain. It is well known
that this problem is solvable in the sense that there are algorithms which make polynomially-many
calls to an oracle that evaluates the function at every given point, and return a point which is arbitrarily close to the true minimum of the function. But suppose that instead of the true value of
the function, the oracle has some small error. Would it still be possible to optimize the function
efficiently? To formalize the notion of error, we can consider two types of erroneous oracles:
‚Ä¢ For a given function f : [0, 1]n ‚Üí [0, 1] we say that fe : [0, 1]n ‚Üí [0, 1] is an absolute
-erroneous oracle if ‚àÄx ‚àà [0, 1]n we have that: fe(x) = f (x) + Œæx where Œæx ‚àà [‚àí, ].
‚Ä¢ For a given function f : [0, 1]n ‚Üí R we say that fe : [0, 1]n ‚Üí R is a relative -erroneous
oracle if ‚àÄx ‚àà [0, 1]n we have that: fe(x) = Œæx f (x) where Œæx ‚àà [1 ‚àí , 1 + ].
Note that we intentionally do not make distributional assumptions about the errors. This is in contrast to noise, where the errors are assumed to be random and independently generated from some
distribution. In such cases, under reasonable conditions on the distribution, one can obtain arbitrarily good approximations of the true function value by averaging polynomially many points in some
-ball around the point of interest. Stated in terms of noise, in this paper we consider oracles that
have some small adversarial noise, and wish to understand whether desirable optimization guarantees are obtainable. To avoid ambiguity, we refrain from using the term noise altogether, and refer
to such as inaccuracies in evaluation as error.
While distributional i.i.d. assumptions are often reasonable models, evaluating our dependency on
these assumptions seems necessary. From a practical perspective, there are cases in which noise
can be correlated, or where the data we use to estimate the function is corrupted in some arbitrary
way. Furthermore, since we often optimize over functions that we learn from data, the process of
fitting a model to a function may also introduce some bias that does not necessarily vanish, or may
vanish. But more generally, it seems like we should morally know the consequences that modest
inaccuracies may have.
1

f(x)

x
Figure 1: An illustration of an erroneous oracle to a convex function that fools a gradient descent algorithm.
Benign cases. In the special case of a linear function f (x) = c| x, for some c ‚àà Rn , a relative -error has little effect on the optimization. By querying f (ei ), for every i ‚àà [n] we can extract
e
ci ‚àà [(1‚àí)ci , (1+)ci ] and then optimize over f 0 (x) = e
c| x. This results in a (1¬±)-multiplicative
approximation. Alternatively, if the erroneous oracle fe happens to be a convex function, optimizing fÀú(x) directly retains desirable optimization guarantees, up to either additive and multiplicative
errors. We are therefore interested in scenarios where the error does not necessarily have nice properties.
Gradient descent fails with error. For a simple example, consider the function illustrated in
Figure 1. The figure illustrates a convex function (depicted in blue) and an erroneous version of
it (dotted red), s.t. on every point, the oracle is at most some additive  > 0 away from the true
function value (the  margins of the function are depicted in grey). If we assume that a gradient
descent algorithm is given access to the erroneous version (dotted red) instead of the true function
(blue), the algorithm will be trapped in a local minimum that can be arbitrarily far from the true
minimum. But the fact that a naive gradient descent algorithm fails does not necessarily mean that
there isn‚Äôt an algorithm that can overcome small errors. This narrates the main question in this paper.
Is convex optimization robust to error?
Main Results. Our results are largely spoilers. We present stark information-theoretic lower
bounds for both relative and absolute -erroneous oracles, for any constant and even sub-constant
 > 0. In particular, we show that:
‚Ä¢ For minimizing a convex function (or maximizing a concave function) f : [0, 1]n ‚Üí [0, 1]
over [0, 1]n : we show that for any fixed Œ¥ > 0, no algorithm can achieve an additive
approximation within 1/2 ‚àí Œ¥ of the optimum, using a subexponential number of calls to
an absolute n‚àí1/2+Œ¥ -erroneous oracle.
‚Ä¢ For minimizing a convex function f : [0, 1]n ‚Üí [0, 1] over a polytope P ‚äÇ [0, 1]n : for any
fixed  > 0, no algorithm can achieve a finite multiplicative factor using a subexponential
number of calls to a relative -erroneous oracle.
‚Ä¢ For maximizing a concave function f : [0, 1]n ‚Üí [0, 1] over a polytope P ‚äÇ [0, 1]n : for
any fixed  > 0, no algorithm can achieve a multiplicative factor better than Œò(n‚àí1/2+ )
using a subexponential number of calls to a relative -erroneous oracle;
‚Ä¢ For maximizing a concave function f : [0, 1]n ‚Üí [0, 1] over [0, 1]n : for any fixed  > 0,
no algorithm can obtain a multiplicative factor better than 1/2 +  using a subexponential
number of calls to a relative -erroneous oracle. (And there is a trivial 1/2-approximation
without asking any queries.)
Somewhat surprisingly, many of the impossibility results listed above are shown for a class of extremely simple convex and concave functions, namely, affine functions: f (x) = c| x + b. This is
2

in sharp contrast to the case of linear functions (without the constant term b) with relative erroneous
oracles as discussed above. In addition, we note that our results extend to strongly convex functions.
1.1

Related work

The oracle models we study here fall in the category of zeroth-order or derivative free. Derivativefree methods have a rich history in convex optimization and were among the earliest to numerically
solve unconstrained optimization problems. Recently these approaches have enjoyed increasing
interest, as they are useful in scenarios where black-box access is given to the function or cases in
which gradient information is difficult to compute or does not exist [9, 8, 11, 15, 14, 6]
There has been a rich line of work for noisy oracles, where the oracles return some erroneous version
of the function value which is random. In a stochastic framework, these settings correspond to
repeatedly choosing points in some convex domain, obtaining noisy realizations of some underlying
convex function‚Äôs value. Most frequently, the assumption is that one is given a first-order noisy
oracle with some assumptions about the distribution that generates the error [13, 12]. In the learning
theory community, optimization with stochastic noisy oracles is often motivated by multi-armed
bandits settings [4, 1], and regret minimization with zeroth-order feedback [2]. All these models
consider the case in which the error is drawn from a distribution.
The model of adversarial noise in zeroth order oracles has been mentioned in [10] which considers
a related model of erroneous oracles and informally argues that exponentially many queries are
required to approximately minimize a convex function in this model (under an `2 -ball constraint).
In recent work, Belloni et al. [3] study convex optimization with erroneous oracles. Interestingly,
Belloni et al. show positive results. In their work they develop a novel algorithm that is based on
sampling from an approximately log-concave distribution using the Hit-and-Run method and show
that their method has polynomial query complexity. In contrast to the negative results we show in
this work, the work of Belloni et al. assumes the (absolute) erroneous oracle returns f (x) + Œæx
with Œæx ‚àà [‚àí n , n ]. That is, the error is not a constant term, but rather is inversely proportional
to the dimension. Our lower bounds for additive approximation hold when the oracle error is not
1
1
necessarily a constant but Œæx ‚àà [ n1/2‚àíŒ¥
, n1/2‚àíŒ¥
] for a constant 0 < Œ¥ < 1/2.

2

Preliminaries

Optimization and convexity. For a minimization problem, given a nonnegative objective function
f and a polytope P we will say that an algorithm provides a (multiplicative) Œ±-approximation (Œ± >
1) if it finds a point xÃÑ ‚àà P s.t. f (xÃÑ) ‚â§ Œ± minx‚ààP f (x). For a maximization problem, an algorithm
provides an Œ±-approximation (Œ± < 1) if it finds a point xÃÑ s.t. f (xÃÑ) ‚â• Œ± maxx‚ààP f (x).
For absolute erroneous oracles, given an objective function f and a polytope P we will aim to find
a point xÃÑ ‚àà P which is within an additive error of Œ¥ from the optimum, with Œ¥ as small as possible.
That is, for a Œ¥ > 0 we aim to find a point xÃÑ s.t. |f (xÃÑ)‚àíminx f (x)| < Œ¥ in the case of minimization.
A function f : P ‚Üí R is convex on P if f (tx + (1 ‚àí t)y) ‚â§ tf (x) + (1 ‚àí t)f (y) (or concave if
f (tx + (1 ‚àí t)y) ‚â• tf (x) + (1 ‚àí t)f (y)) for every x, y ‚àà P and t ‚àà [0, 1].
Chernoff bounds. Throughout the paper we appeal to the Chernoff bounds. We note that while
typically stated for independent random variables X1 , . . . , Xm , Chernoff bounds also hold for negatively associated random variables.
Definition 2.1 ([5], Definition 1). Random variables X1 , . . . , Xn are negatively associated, if for
¬Ø
every I ‚äÜ [n] and every non-decreasing f : RI ‚Üí R, g : RI ‚Üí R,
¬Ø ‚â§ E[f (Xi , i ‚àà I)]E[g(Xj , j ‚àà I)].
¬Ø
E[f (Xi , i ‚àà I)g(Xj , j ‚àà I)]
Claim 2.2 ([5], Theorem 14).
Pn Let X1 , . . . , Xn be negatively associated random variables that take
values in [0, 1] and ¬µ = E[ i=1 Xi ]. Then, for any Œ¥ ‚àà [0, 1] we have that:
n
X
2
Pr[
Xi > (1 + Œ¥)¬µ] ‚â§ e‚àíŒ¥ ¬µ/3 ,
i=1

3

n
X
2
Pr[
Xi < (1 ‚àí Œ¥)¬µ] ‚â§ e‚àíŒ¥ ¬µ/2 .
i=1

We apply this to random variables that are formed by selecting a random subset of a fixed size. In
particular, we use the following.
Claim 2.3. Let x1 , . . . , xn ‚â• 0 be fixed. For 1 ‚â§ k ‚â§ n, let R be a uniformly random subset of k
elements out of [n]. Let Xi = xi if i ‚àà R and Xi = 0 otherwise. Then X1 , . . . , Xn are negatively
associated.
Proof. For x1 = x2 = . . . = xn = 1, the statement holds by Corollary 11 of [5] (which refers
to this distribution as the Fermi-Dirac model). The generalization to arbitrary xi ‚â• 0 follows from
Proposition 4 of [5] with Ij = {j} and hj (t) = xj t.

3

Optimization over the unit cube

We start with optimization over [0, 1]n , arguably the simplest possible polytope. We show that
already in this setting, the presence of adversarial noise prevents us from achieving much more than
trivial results.
3.1

Convex minimization

First let us consider convex minimization over [0, 1]n . In this setting, we show that errors as small
as n‚àí(1‚àíŒ¥)/2 prevent us from optimizing within a constant additive error.
Theorem 3.1. Let Œ¥ > 0 be a constant. There are instances of a convex function f : [0, 1]n ‚Üí
[0, 1] accessible through an absolute n‚àí(1‚àíŒ¥)/2 -erroneous oracle, such that a (possibly randomized)
Œ¥
algorithm that makes eO(n ) queries cannot find a solution of value better than within additive
Œ¥
1/2 ‚àí o(1) of the optimum with probability more than e‚àí‚Ñ¶(n ) .
We remark that the proof of this theorem is inspired by the proof of hardness of ( 21 + )approximation for unconstrained submodular maximization [7]; in particular it can be viewed as
a simple application of the ‚Äúsymmetry gap‚Äù argument (see [16] for a more general exposition).
Proof. Let  = n‚àí(1‚àíŒ¥)/2 ; we can assume that  < 12 , otherwise n is constant and the statement
is trivial. We will construct an -erroneous oracle (both in the relative and absolute sense) for a
convex function f : [0, 1]n ‚Üí [0, 1]. Consider a partition of [n] into two subsets A, B of size
|A| = |B| = n/2 (which will be eventually chosen randomly). We define the following function:
‚Ä¢ f (x) =

1
2

P
P
+ n1 ( i‚ààA xi ‚àí j‚ààB xj ).

This is a convex (in fact linear) function. Next, we define the following modification of f , which
could be the function returned by an -erroneous oracle.
‚Ä¢ If |

P

xi ‚àí

P

xj | > 12 n, then fÀú(x) = f (x) =

‚Ä¢ If |

P

xi ‚àí

P

xj | ‚â§ 12 n, then fÀú(x) = 12 .

i‚ààA
i‚ààA

j‚ààB
j‚ààB

1
2

P
P
+ n1 ( i‚ààA xi ‚àí j‚ààB xj ).

P
P
Note that f (x) and fÀú(x) differ only in the region where | i‚ààA xi ‚àí j‚ààB xj | ‚â§ 12 n. In particular,
1+
1
Àú
the value of f (x) in this region is within [ 1‚àí
2 , 2 ], while f (x) = 2 , so an -erroneous oracle for
f (x) (both in the relative and absolute sense) could very well return fÀú(x) instead.
Now assume that (A, B) is a random partition, unknown to the algorithm. We argue
Pthat with
high probability, a fixed query x issued by the algorithm will have the property that | i‚ààA xi ‚àí
P
1
j‚ààB xj | ‚â§ 2 n. More precisely, since (A, B) is chosen at random subject to |A| = |B| = n/2,
4

P
we have that i‚ààA xi is a sum of negatively associated random variables in [0, 1] (by Claim 2.3).
P
Pn
The expectation of this quantity is ¬µ = E[ i‚ààA xi ] = 12 i=1 xi ‚â§ 21 n. By Claim 2.2, we have
X
X
2
2
1
n
Pr[
xi > ¬µ + n] = Pr[
xi > (1 +
)¬µ] < e‚àí(n/(4¬µ)) ¬µ/3 ‚â§ e‚àí n/24 .
4
4¬µ
i‚ààA
i‚ààA
P
P
P
n
Since 12 i‚ààA xi + 12 i‚ààB xi = 12 i=1 xi = ¬µ, we get
X
X
X
2
1
1
Pr[
xi ‚àí
xi > n] = Pr[
xi ‚àí ¬µ > n] < e‚àí n/24 .
2
4
i‚ààA

i‚ààB

i‚ààA

By symmetry,
Pr[|

X
i‚ààB

xi ‚àí

X

xj | >

j‚ààA

2
1
n] < 2e‚àí n/24 .
2

We emphasize that this holds for a fixed query x.
Recall that we assumed the algorithm to be deterministic. Hence, as long as its queries satisfy the
property above, the answers will be fÀú(x) = 1/2, and the algorithm will follow the same path of
computation, no matter what the choice of (A, B) is. (Effectively we will not learn anything about
A and B.) Considering the sequence of queries on this computation path, if the number of queries is
2
t then with probability at least 1‚àí2te‚àí n/24 the queries will indeed fall in the region where fÀú(x) =
2
1/2 and the algorithm will follow this path. If t ‚â§ e n/48 , this happens with probability at least
2
1 ‚àí 2e‚àí n/48 . In this case, all the points queried by the algorithm as well as the returned solution
xout (by the same argument) satisfies fÀú(xout ) = 1/2, and hence f (xout ) ‚â• 1‚àí
2 . In contrast, the
‚àí(1‚àíŒ¥)/2
actual optimum is f (1B ) = 0. Recall that  = n
; hence, f (xout ) ‚â• 12 (1 ‚àí n‚àí(1‚àíŒ¥)/2 )
and the bounds on the number of queries and probability of success are as in the statement of the
theorem.
Finally, consider a randomized algorithm. Denote by (R1 , R2 , . . . , ...) the random variables used
by the algorithm in its decisions. We can condition on a fixed choice of (R1 = r1 , R2 = r2 , . . .)
which makes the algorithm deterministic. By our proof, the algorithm conditioned on this choice
Œ¥
cannot succeed with probability more than e‚àí‚Ñ¶(n ) . Since this is true for each particular choice of
(r1 , r2 , . . .), by averaging it is also true for a random choice of (R1 , R2 , . . .). Hence, we obtain the
same result for randomized algorithms as well.
3.2

Concave maximization

Here we consider the problem of maximizing a concave function f : [0, 1]n ‚Üí [0, 1]. One can
obtain a result for concave maximization analogous to Theorem 3.1, which we do not state; in
terms of additive errors, there is really no difference between convex minimization and concave
maximization. However, in the case of concave maximization we can also formulate the following
hardness result for multiplicative approximation.
Theorem 3.2. If a concave function f : [0, 1]n ‚Üí [0, 1] is accessible through a relative-Œ¥-erroneous
2
oracle, then for any  ‚àà [0, Œ¥], an algorithm that makes less than e n/48 queries cannot find a
‚àí2 n/48
solution of value greater than 1+
.
2 OP T with probability more than 2e
Proof. This result follows from the same construction as Theorem 3.1. Recall that f (x) is a linear
function, hence also concave. As we mentioned in the proof of Theorem 3.1, fÀú(x) could be the
values returned by a relative -erroneous oracle. Now we consider an arbitrary  > 0; note that for
Œ¥ ‚â•  it still holds that fÀú(x) is a relative Œ¥-erroneous oracle.
n

By the same proof, an algorithm querying less than e n/48 points cannot find a solution of value
‚àí2 n/48
. In contrast, the optimum of the maximization
better than 1+
2 with probability more than 2e
problem is supx‚àà[0,1]n f (x) = 1. Therefore, the algorithm cannot achieve multiplicative approximation better than 1+
2 .
We note that this hardness result is optimal due to the following easy observation.
5

Theorem 3.3. For any concave function f : [0, 1]n ‚Üí R+ , let OP T = supx‚àà[0,1]n f (x). Then


1 1
1
1
f
, ,...,
‚â• OP T.
2 2
2
2
Proof. By compactness, the optimum is attained at a point: let OP T = f (x‚àó ). Let also x0 =
(1, 1, . . . , 1) ‚àí x‚àó . We have x0 ‚àà [0, 1]n and hence f (x0 ) ‚â• 0. By concavity, we obtain


 ‚àó

1
x + x0
f (x‚àó ) + f (x0 )
1
1
1 1
f
, ,...,
=f
‚â•
‚â• f (x‚àó ) = OP T.
2 2
2
2
2
2
2

In other words, a multiplicative 12 -approximation for this problem is trivial to obtain ‚Äî even without
asking any queries about f . We just return the point ( 12 , 12 , . . . , 12 ). Thus we can conclude that for
concave maximization, a relative -erroneous oracle is not useful at all.

4

Optimization over polytopes

In this section we consider optimization of convex and concave functions over a polytope
P = {x ‚â• 0 : Ax = b}. We will show inappoximability results for the relative error model. Note
that for the absolute error case, the lower bound on convex minimization from the previous section
holds, and can be applied to show a lower bound for concave maximization with absolute errors.
Theorem 4.1. Let , Œ¥ ‚àà (0, 1/2) be some constants. There are convex functions for which no
Œ¥
algorithm can obtain a finite approximation ratio to minx‚ààP f (x) using ‚Ñ¶(en ) queries to a relative
-erroneous oracle of the function.
P
1/2+Œ¥
Proof. We will prove our theorem for the case in which P = {x ‚â• 0 :
}. Let
i xi ‚â§ n
H be a subset of indices chosen uniformly at random from all subsets of size exactly n1/2+Œ¥ . We
construct two functions:
X
f (x) = n1+Œ¥ ‚àí n1/2
xi
i‚ààH

g(x) = n

1+Œ¥

‚àín

Œ¥

X

xi

i

Observe that both these functions are convex and non-negative. Also, observe that
P the minimizer
1/2+Œ¥
of f is x‚àó = 1H and f (x‚àó ) = 0, while the minimizer of g is any vector x0 :
i xi = n
0
1+Œ¥
1/2+2Œ¥
1+Œ¥
and g(x ) = n
‚àín
= Œò(n ). Therefore, the ratio between these two functions is
unbounded. We will now construct the erroneous oracle in the following manner:

g(x), if (1 ‚àí )f (x) ‚â§ g(x) ‚â§ (1 + )f (x)
e
f (x) =
f (x) otherwise
By definition, fe is an -erroneous oracle to f . The claim will follow from the fact that given access
to fe one cannot distinguish between f and g using a subexponential number of queries. This implies
the inapproximability result since an approximation algorithm which guarantees a finite approximation ratio using a subexponential number of queries could be used to distinguish between the two
functions: if the algorithm returns an answer strictly greater than 0 then we know the underlying
function is g and otherwise it is f.
Given a query x ‚àà [0, 1]n to the oracle, we will consider two cases.
‚Ä¢ In case the query x is such that

P

i

xi ‚â§ n1/2 then we have that:

n1+Œ¥ ‚àí n ‚â§ f (x) ‚â§ n1+Œ¥
n1+Œ¥ ‚àí nŒ¥+1/2 ‚â§ g(x) ‚â§ n1+Œ¥
6

Since for any , Œ¥ > P
0 there is a large enough n s.t. nŒ¥ > (1 + )/, this implies that for
any query for which i xi ‚â§ n1/2 then we have that g(x) ‚àà [(1 ‚àí )f (x), (1 + )f (x)]
and thus the oracle returns g(x).
P
1/2
‚Ä¢ In
then we can interpret the value of
i xi > n
P case the query is such that
x
which
determines
value
of
f
as
a
sum
of
negatively
associated random variables
i
i‚ààH
X1 , . . . , Xn where Xi realizes with probability n‚àí1/2+Œ¥ and takes value xi if realized
(see Claim 2.3). WePcan then apply the Chernoff bound (Claim 2.2), using the fact that
E[f (x)] = n1/2‚àíŒ¥ i xi , and get that for any constant 0 < Œ≤ < 1 we have that with
Œ¥
probability 1 ‚àí e‚àí‚Ñ¶(n ) :

P x

P x
X
i i
i i
1 ‚àí Œ≤ 1/2‚àíŒ¥
‚â§
xi ‚â§ 1 + Œ≤ 1/2‚àíŒ¥
n
n
i‚ààH
Œ¥

By using Œ≤ ‚â§ /(1 + ), this implies that with probability at least 1 ‚àí e‚àí‚Ñ¶(n ) we get that:
(1 ‚àí )f (x) ‚â§ g(x) ‚â§ (1 + )f (x)
Since the likelihood of distinguishing between f and g on a single query is exponentially
small in nŒ¥ , the same arguments used throughout the paper imply that it takes an exponential number of queries to distinguish between f and g.
Œ¥

To conclude, for any query x ‚àà [0, 1]n it takes ‚Ñ¶(en ) queries to distinguish between f and g.
As discussed above, due to the fact that the ratio between the optima of these two functions is
unbounded, this concludes the proof.
Theorem 4.2. ‚àÄ constants , Œ¥ ‚àà (0, 1/2) there is a concave function f : [0, 1]n ‚Üí R+ for which
no algorithm can obtain an approximation strictly better than O(n‚àí1/2+Œ¥ ) to maxx‚ààP f (x) using
Œ¥
‚Ñ¶(en ) queries to a relative -erroneous oracle of the function.
Proof. We follow a similar methodology as in the proof of Theorem 4.1. We again we select a
P
1/2+Œ¥
set H of size n1/2+Œ¥ u.a.r. and construct two functions: f (x) = n1/2 i‚ààH xi + n 
and
P
n1/2+Œ¥
Œ¥
e
. As in the proof of Theorem 4.1 the noisy oracle f (x) = g(x) when
g(x) = n
i xi +

(1 ‚àí )f (x) ‚â§ g(x) ‚â§ (1 + )f (x) and otherwise fe(x) = f (x). Note that both functions are concave
and non-negative, and by its definition the oracle is -erroneous for the function f . For b = n1/2+Œ¥
it is easy to see that the optimal value when the objective is f is n1+Œ¥ while the optimal value is
O(n1/2+Œ¥ ) when the objective is g, which implies that one cannot obtain an approximation better
‚àí1/2+Œ¥
than ‚Ñ¶(n
) with a subexponential number of queries. In case the query to the oracle is a point
P
1/2
x s.t.
x
‚â§
n
, then by Chernoff bound arguments, similar to the ones we used above, with
i i
Œ¥
probability
1 ‚àí e‚àí‚Ñ¶(n ) we get (1 ‚àí )f (x) ‚â§ g(x) ‚â§ (1 + )f (x). Thus, for any query in
P at least1/2
which i xi ‚â§ n , the likelihood of the oracle returning f is exponentially small in nŒ¥ .
P
In case the query is a point x s.t. i xi > n1/2 standard concentration bound arguments as before,
Œ¥
imply that with probability at least 1 ‚àí e‚àí‚Ñ¶(n ) we get (1 ‚àí )f (x) ‚â§ g(x) ‚â§ (1 + )f (x). Since
the likelihood of distinguishing between f and g on a single query is exponentially small in nŒ¥ , we
can conclude that it takes an exponential number of queries to distinguish between f and g.

5

Optimization over assignments

In this section, we consider the concave maximization problem over a more specific polytope,
Ô£±
Ô£º
k
Ô£≤
Ô£Ω
X
Pn,k = x ‚àà Rn√ók
:
xij = 1 ‚àÄi ‚àà [n] .
+
Ô£≥
Ô£æ
j=1

This can be viewed as the matroid polytope for a partition matroid on n blocks of k elements, or
alternatively the convex hull of assignments of n items to k agents. In this case, there is a trivial
1
1
k -approximation, similar to the 2 -approximation in the case of a unit cube.
7

Theorem 5.1. For any k ‚â• 2 and a concave function f : Pn,k ‚Üí R+ , let OP T = supx‚ààPn,k f (x).
Then


1 1
1
1
f
, ,...,
‚â• OP T.
k k
k
k
(`)

Proof. By compactness, the optimum is attained at a point: let OP T = f (x‚àó ). Let xij =
x‚àói,(j+` mod k) ; i.e., x(`) is a cyclic shift of the coordinates of x‚àó by ` in each block. We have
Pk‚àí1 (`)
Pk
x(`) ‚àà Pn,k and k1 `=0 xij = k1 j=1 x‚àóij = k1 . By concavity and nonnegativity of f , we obtain
!


k‚àí1
1
1 1
1 X (`)
1
1
, ,...,
=f
x
f
‚â• f (x(0) ) = OP T.
k k
k
k
k
k
`=0

We show that this approximation is best possible, if we have access only to a Œ¥-erroneous oracle.
Theorem 5.2. If k ‚â• 2 and a concave function f : Pn,k ‚Üí [0, 1] is accessible through a relative-Œ¥2
erroneous oracle, then for any  ‚àà [0, Œ¥], an algorithm that makes less than e n/6k queries cannot
‚àí2 n/6k
find a solution of value greater than 1+
.
k OP T with probability more than 2e
Note that this result is nontrivial only for n  k. In other words, the hardness factor of k is never
worse than a square root of the dimension of the problem. Therefore, this result can be viewed as
interpolating between the hardness of 1+
2 -approximation over the unit cube (Theorem 3.2), and the
hardness of nŒ¥‚àí1/2 -approximation over a general polytope (Theorem 4.2).
Proof. Given œÄ : [n] ‚Üí [k], we construct a function f œÄ : Pn,k ‚Üí [0, 1] (where œÄ describes the
intended optimal solution):
‚Ä¢ f œÄ (x) =

1
n

Pn

i=1

xi,œÄ(i) .

Next we define a modified function fÀúœÄ as follows:
‚Ä¢ If |f œÄ (x) ‚àí k1 | >


k

then fÀúœÄ (x) = f œÄ (x)

‚Ä¢ If |f œÄ (x) ‚àí k1 | ‚â§


k

then fÀúœÄ (x) = k1 .

1+
By definition, f œÄ (x) and fÀúœÄ (x) differ only if |f œÄ (x) ‚àí k1 | ‚â§ k , and then f œÄ (x) ‚àà [ 1‚àí
k , k ] while
fÀúœÄ (x) = k1 . Therefore, fÀúœÄ (x) is a valid relative -erroneous oracle for f œÄ .

Similarly to the proofs above, we argue that if œÄ is chosen uniformly at random, then with high
probability fÀúœÄ (x) = k1 for any fixed query x ‚àà Pn,k . This holds again by a Chernoff bound: For a
Pk
Pn
fixed xij such that j=1 xij = 1, we have that f œÄ (x) = n1 i=1 xi,œÄ(i) = n1 Z where Z is a sum of
P
independent random variables with expectation k1 i,j xij = nk . The random variables attain values
2

in [0, 1]. By the Chernoff bound, Pr[|Z ‚àí nk | >  nk ] < 2e‚àí n/3k . This gives


2
1

Pr |f œÄ (x) ‚àí | >
< 2e‚àí n/3k .
k
k
2

By the same arguments as before, if the algorithm asks less than e n/6k queries, then it will not
2
detect a point such that |f œÄ (x) ‚àí k1 | > k with probability more than 2e‚àí n/6k . Then the query answers will all be fÀúœÄ (x) = k1 and the value of the returned solution will be at most 1+
k . Meanwhile,
the optimum solution is x‚àói,œÄ(i) = 1 for all i, which gives f (x‚àó ) = 1.
Acknowledgements. YS was supported by NSF grant CCF-1301976, CAREER CCF-1452961 and a Google
Faculty Research Award.

8

References
[1] Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization with
multi-point bandit feedback. In COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel,
June 27-29, 2010, pages 28‚Äì40, 2010.
[2] Alekh Agarwal, Dean P. Foster, Daniel Hsu, Sham M. Kakade, and Alexander Rakhlin. Stochastic convex
optimization with bandit feedback. SIAM Journal on Optimization, 23(1):213‚Äì240, 2013.
[3] Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, and Alexander Rakhlin. Escaping the local
minima via simulated annealing: Optimization of approximately convex functions. COLT 2015.
[4] SeÃÅbastien Bubeck and NicoloÃÄ Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed
bandit problems. Foundations and Trends in Machine Learning, 5(1):1‚Äì122, 2012.
[5] Devdatt Dubhashi, Volker Priebe, and Desh Ranjan. Negative dependence through the FKG inequality.
In Research report MPI-I-96-1-020, Max-Planck Institut fuÃàr Informatik, SaarbruÃàcken, 1996.
[6] John C. Duchi, Michael I. Jordan, Martin J. Wainwright, and Andre Wibisono. Optimal rates for zeroorder convex optimization: The power of two function evaluations. IEEE Transactions on Information
Theory, 61(5):2788‚Äì2806, 2015.
[7] Uriel Feige, Vahab S. Mirrokni, and Jan VondraÃÅk. Maximizing non-monotone submodular functions.
SIAM J. Comput., 40(4):1133‚Äì1153, 2011.
[8] Abraham Flaxman, Adam Tauman Kalai, and H. Brendan McMahan. Online convex optimization in the
bandit setting: gradient descent without a gradient. In Proceedings of the Sixteenth Annual ACM-SIAM
Symposium on Discrete Algorithms, SODA 2005, Vancouver, British Columbia, Canada, January 23-25,
2005, pages 385‚Äì394, 2005.
[9] Kevin G. Jamieson, Robert D. Nowak, and Benjamin Recht. Query complexity of derivative-free optimization. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural
Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe,
Nevada, United States., pages 2681‚Äì2689, 2012.
[10] A.S. Nemirovsky and D.B. Yudin. Problem Complexity and Method Efficiency in Optimization. J. Wiley
& Sons, New York, 1983.
[11] Yurii Nesterov. Random gradient-free minimization of convex functions. CORE Discussion Papers
2011001, UniversiteÃÅ catholique de Louvain, Center for Operations Research and Econometrics (CORE),
2011.
[12] Aaditya Ramdas, BarnabaÃÅs PoÃÅczos, Aarti Singh, and Larry A. Wasserman. An analysis of active learning
with uniform feature noise. In Proceedings of the Seventeenth International Conference on Artificial
Intelligence and Statistics, AISTATS 2014, Reykjavik, Iceland, April 22-25, 2014, pages 805‚Äì813, 2014.
[13] Aaditya Ramdas and Aarti Singh. Optimal rates for stochastic convex optimization under tsybakov noise
condition. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pages 365‚Äì373, 2013.
[14] Ohad Shamir. On the complexity of bandit and derivative-free stochastic convex optimization. In COLT
2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton University, NJ,
USA, pages 3‚Äì24, 2013.
[15] Sebastian U. Stich, Christian L. MuÃàller, and Bernd GaÃàrtner. Optimization of convex functions with random
pursuit. CoRR, abs/1111.0194, 2011.
[16] Jan VondraÃÅk. Symmetry and approximability of submodular maximization problems. SIAM J. Comput.,
42(1):265‚Äì304, 2013.

9

