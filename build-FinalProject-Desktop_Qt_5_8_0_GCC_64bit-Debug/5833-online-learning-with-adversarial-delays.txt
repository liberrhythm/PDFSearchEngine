Online Learning with Adversarial Delays
Kent Quanrudâˆ— and Daniel Khashabiâ€ 
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{quanrud2,khashab2}@illinois.edu

Abstract
We study the performance of standard online learning algorithms when the feedback is delayed by an adversary. We show that online-gradient-descent
âˆš
[1] and follow-the-perturbed-leader [2] achieve regret O( D) in the
delayed setting, where D is the sum
âˆš of delays of each roundâ€™s feedback. This
bound collapses to an optimal O( T ) bound in the usual setting of no delays
(where D = T ). Our main contribution is to show that standard algorithms for
online learning already have simple regret bounds in the most general setting of
delayed feedback, making adjustments to the analysis and not to the algorithms
themselves. Our results help affirm and clarify the success of recent algorithms in
optimization and machine learning that operate in a delayed feedback model.

1

Introduction

Consider the following simple game. Let K be a bounded set, such as the unit `1 ball or a collection
of n experts. Each round t, we pick a point xt âˆˆ K. An adversary then gives us a cost function ft ,
PT
and we incur the loss `t = ft (xt ). After T rounds, our total loss is the sum LT = t=1 `t , which
we want to minimize.
We cannot hope to beat the adversary, so to speak, when the adversary picks the cost function after
we select our point. There is margin for optimism, however, if rather than evaluate our total loss in
absolute terms, we compare our strategy to the best fixed point in hindsight. The regret of a strategy
PT
PT
x1 , . . . , xT âˆˆ K is the additive difference R(T ) = t=1 ft (xt ) âˆ’ arg minxâˆˆK t=1 ft (x).
Surprisingly, one can obtain positive results in terms of regret. Kalai and Vempala
âˆš showed that a
simple and randomized follow-the-leader type algorithm achieves R(T ) = O( T ) in expectation
for linear cost functions [2] (here, the big-O notation assumes that the diameter of K and the ft â€™s
are bounded by constants). If K is convex, then even if the cost vectors are more generally convex
cost functions (where we incur losses of the form `t = ft (xt ), âˆš
with ft a convex function), Zinkevich showed that gradient descent achieves regret R(T ) = O( T ) [1]. There is a large body of
theoretical literature about this setting, called online learning (see for example the surveys by Blum
[3], Shalev-Shwartz [4], and Hazan [5]).
Online learning is general enough to be applied to a diverse family of problems. For example, Kalai
and Vempalaâ€™s algorithm can be applied to online combinatorial problems such as shortest paths
[6], decision trees [7], and data structures [8, 2]. In addition to basic machine learning problems
with convex loss functions, Zinkevich considers applications to industrial optimization, where the
âˆ—
http://illinois.edu/~quanrud2/. Supported in part by NSF grants CCF-1217462, CCF1319376, CCF-1421231, CCF-1526799.
â€ 
http://illinois.edu/~khashab2/. Supported in part by a grant from Google.

1

value of goods is not known until after the goods are produced. Other examples of applications of
online learning include universal portfolios in finance [9] and online topic-ranking for multi-labeled
documents [10].
The standard setting assumes that the cost vector ft (or more generally, the feedback) is given to and
processed by the player before making the next decision in round t + 1. Philosophically, this is not
how decisions are made in real life: we rush through many different things at the same time with no
pause for careful consideration, and we may not realize our mistakes for a while. Unsurprisingly, the
assumption of immediate feedback is too restrictive for many real applications. In online advertising,
online learning algorithms try to predict and serve ads that optimize for clicks [11]. The algorithm
learns by observing whether or not an ad is clicked, but in production systems, a massive number of
ads are served between the moment an ad is displayed to a user and the moment the user has decided
to either click or ignore that ad. In military applications, online learning algorithms are used by radio
jammers to identify efficient jamming strategies [12]. After a jammer attempts to disrupt a packet
between a transmitter and a receiver, it does not know if the jamming attempt succeeded until an
acknowledgement packet is sent by the receiver. In cloud computing, online learning helps devise
efficient resource allocation strategies, such as finding the right mix of cheaper (and inconsistent)
spot instances and more reliable (and expensive) on-demand instances when renting computers for
batch jobs [13]. The learning algorithm does not know how well an allocation strategy worked for
a batch job until the batch job has ended, by which time many more batch jobs have already been
launched. In finance, online learning algorithms managing portfolios are subject to information and
transaction delays from the market, and financial firms invest heavily to minimize these delays.
One strategy to handle delayed feedback is to pool independent copies of a fixed learning algorithm,
each of which acts as an undelayed learner over a subsequence of the rounds. Each round is delegated to a single instance from the pool of learners, and the learner is required to wait for and process
its feedback before rejoining the pool. If there are no learners available, a new copy is instantiated
and added to the pool. The size of the pool is proportional to the maximum number of outstanding
delays at any point of decision, and the overall regret is bounded by the sum of regrets of the individual learners. This approach is analyzed for constant delays by Weinberger and Ordentlich [14], and
a more sophisticated analysis is given by Joulani et al. [15]. If Î± is the expected maximum
number
âˆš
of outstanding feedbacks, then Joulani et al. obtain a regret bound on the order of O( Î±T ) (in expectation) for the setting considered here. The blackbox nature of this approach begets simultaneous
bounds for other settings such as partial information and stochastic rewards. Although maintaining
copies of learners in proportion to the delay may be prohibitively resource intensive, Joulani et al.
provide a more efficient variant for the stochastic bandit problem, a setting not considered here.
Another line of research is dedicated to scaling gradient descent type algorithms to distributed settings, where asynchronous processors naturally introduce delays in the learning framework. A classic reference in this area is the book of Bertsekas and Tsitskilis [16]. If the data is very sparse, so that
input instances and their gradients are somewhat orthogonal, then intuitively we can apply gradients
out of order without significant interference across rounds. This idea is explored by Recht et al. [17],
who analyze and test parallel algorithm on a restricted class of strongly convex loss functions, and
by Duchi et al. [18] and McMahan and Streeter [19], who design and analyze distributed variants
of adaptive gradient descent [20]. Perhaps the most closely related work in this area is by Langford
et al., who study the online-gradient-descent algorithm of Zinkevich when the delays are
bounded by a constant number of rounds [21]. Research in this area has largely moved on from the
simplistic models considered here; see [22, 23, 24] for more recent developments.
The impact of delayed feedback in learning algorithms is also explored by Riabko [25] under the
framework of â€œweak teachersâ€.
For the sake of concreteness, we establish the following notation for the delayed setting. For each
round t, let dt âˆˆ Z+ be a non-negative integer delay. The feedback from round t is delivered at the
end of round t + dt âˆ’ 1, and can be used in round t + dt . In the standard setting with no delays,
dt = 1 for all t. For each round t, let Ft = {u âˆˆ [T ] : u + du âˆ’ 1 = t} be the set of rounds whose
PT
feedback appears at the end of round t. We let D = t=1 dt denote the sum of all delays; in the
standard setting with no delays, we have D = T .
In this paper, we investigate the implications of delayed feedback when the delays are adversarial
(i.e., arbitrary), with no assumptions or restrictions made on the adversary. Rather than design new

2

algorithms that may generate a more involved analysis, we study the performance of the classical
algorithms online-gradient-descent and follow-the-perturbed-leader, essentially unmodified, when the feedback is delayed.
In the delayed setting, we prove that both algoâˆš
rithms
have
a
simple
regret
bound
of
O(
D).
These
bounds collapse to match the well-known
âˆš
O( T ) regret bounds if there are no delays (i.e., where D = T ).
Paper organization In Section 2, we analyze the online-gradient-descent algorithm in
the delayed setting, giving upper bounds on the regret as a function of the sum of delays D. In
Section 3, we analyze the follow-the-perturbed-leader in the delayed setting and derive
a regret bound in terms of D. Due to space constraints, extensions to online-mirror-descent
and follow-the-lazy-leader are deferred to the appendix. We conclude and propose future
directions in Section 4.

2

Delayed gradient descent

Convex optimization In online convex optimization, the input domain K is convex, and each
cost function ft is convex. For this setting, Zinkevich proposed a simple online algorithm, called
online-gradient-descent, designed as follows [1]. The first point, x1 , is picked in K
arbitrarily. After picking the tth point xt , online-gradient-descent computes the gradient
âˆ‡ft |xt of the loss function at xt , and chooses xt+1 = Ï€K (xt âˆ’ Î·âˆ‡ft |xt ) in the subsequent round,
for some parameter Î· âˆˆ R>0 . Here, Ï€K is the projection that maps a point x0 to its nearest point
in K (discussed further below). Zinkevich showed that, assuming the Euclidean diameter of K
and the Euclidean lengths of all gradients âˆ‡ftâˆš
|x are bounded by constants, online-gradientdescent has an optimal regret bound of O( T ).
Delayed gradient descent In the delayed setting, the loss function ft is not necessarily given by
the adversary before we pick the next point xt+1 (or even at all). The natural generalization of
online-gradient-descent to this setting is to process the convex loss functions and apply
their gradients the moment they are delivered. That is, we update
X
x0t+1 = xt âˆ’ Î·
âˆ‡fs |xs ,
sâˆˆFt

for some fixed parameter Î·, and then project xt+1 = Ï€K (x0t+1 ) back into K to choose our (t + 1)th
point. In the setting of Zinkevich, we have Ft = {t} for each t, and this algorithm is exactly
online-gradient-descent. Note that a gradient âˆ‡fs |xs does not need to be timestamped by
the round s from which it originates, which is required by the pooling strategies of Weinberger and
Ordentlich [14] and Joulani et al. [15] in order to return the feedback to the appropriate learner.
Theorem 2.1. Let K be a convex set with diameter 1, let f1 , . . . , fT be convex functions over K
with kâˆ‡ft |x k2 â‰¤ L for all x âˆˆ K and t âˆˆ [T ], and let Î· âˆˆ R be a fixed parameter. In the presence
of adversarial delays, online-gradient-descent selects points x1 , . . . , xT âˆˆ K such that
for all y âˆˆ K,


T
T
X
X
1
2
ft (xt ) âˆ’
ft (y) = O
+ Î·L (T + D) ,
Î·
t=1
t=1
where D denotes the sum of delays over all rounds t âˆˆ [T ].
âˆš
âˆš
âˆš
For Î· = 1/L T + D, Theorem 2.1 implies a regret bound of O(L D + T ) = O(L D). This
choice of Î· requires prior knowledge of the final sum D. When this sum is not known, one can
calculate D on the fly: if there are Î´ outstanding (undelivered) cost functions at a round t, then D
increases by exactly Î´. Obviously, Î´ â‰¤ T and T â‰¤ D, so D at most doubles. We can therefore
employ the â€œdoubling trickâ€ of Auer et al. [26] to dynamically adjust Î· as D grows.
In the undelayed setting analyzed by Zinkevich, we have D = T , and the regret bound of Theorem
2.1 matches that obtained by Zinkevich.
âˆš If each delay dt is bounded by some fixed value Ï„ , Theorem
2.1 implies a regret bound of O(L Ï„ T ) that matches that of Langford et al. [21]. In both of these
special cases, the regret bound is known to be tight.
3

Before proving Theorem 2.1, we review basic definitions and facts on convexity. A function f :
K â†’ R is convex if
f ((1 âˆ’ Î±)x + Î±y) â‰¤ (1 âˆ’ Î±)f (x) + Î±f (y)
âˆ€x, y âˆˆ K, Î± âˆˆ [0, 1].
If f is differentiable, then f is convex iff
f (x) + âˆ‡f |x Â· (y âˆ’ x) â‰¤ f (y)
âˆ€x, y âˆˆ K.
(1)
For f convex but not necessarily differentiable, a subgradient of f at x is any vector that can replace
âˆ‡f |x in equation (1). The (possible empty) set of gradients of f at x is denoted by âˆ‚f (x).
The gradient descent may occasionally update along a gradient that takes us out of the constrained
domain K. If K is convex, then we can simply project the point back into K.
Lemma 2.2. Let K be a closed convex set in a normed linear space X and x âˆˆ X a point, and let
x0 âˆˆ K be the closest point in K to x. Then, for any point y âˆˆ K,
kx âˆ’ yk2 â‰¤ kx0 âˆ’ yk2 .
We let Ï€K denote the map taking a point x to its closest point in the convex set K.
Proof of Theorem 2.1. Let y = arg minxâˆˆK (f1 (x) + Â· Â· Â· + fT (x)) be the best point in hindsight at
the end of all T rounds. For t âˆˆ [T ], by convexity of ft , we have,
ft (y) â‰¥ ft (xt ) + âˆ‡ft |xt Â· (y âˆ’ xt ).
Fix t âˆˆ [T ], and
t+1 and y. By Lemma 2.2, we know that
 consider
 the distance between x
P
kxt+1 âˆ’ yk2 â‰¤ x0t+1 âˆ’ y 2 , where x0t+1 = xt âˆ’ Î· sâˆˆFt âˆ‡fs |xs .
We split the sum of gradients applied in a single round P
and consider them one by one. For each
s âˆˆ Ft , let Ft,s = {r âˆˆ Ft : r < s}, and let xt,s = xt âˆ’Î· râˆˆFt,s âˆ‡fr |xr . Suppose Ft is nonempty,
and fix s0 = max Ft to be the last index in Ft . By Lemma 2.2, we have,

2 
2
2
kxt+1 âˆ’ yk2 â‰¤ x0t+1 âˆ’ y 2 = xt,s0 âˆ’ Î·âˆ‡fs0 |xs0 âˆ’ y 2

2

2
= kxt,s0 âˆ’ yk2 âˆ’ 2Î· âˆ‡fs0 |xs0 Â· (xt,s0 âˆ’ y) + Î· 2 âˆ‡fs0 |xs0 2 .
Repeatedly unrolling the first term in this fashion gives
X
X
2
2
2
kxt+1 âˆ’ yk2 â‰¤ kxt âˆ’ yk2 âˆ’ 2Î·
âˆ‡fs |xs Â· (xt,s âˆ’ y) + Î· 2
kâˆ‡fs |xs k2 .
sâˆˆFt

sâˆˆFt

For each s âˆˆ Ft , by convexity of f , we have,
âˆ’âˆ‡fs |xs Â· (xt,s âˆ’ y) = âˆ‡fs |xs Â· (y âˆ’ xt,s ) = âˆ‡fs |xs Â· (y âˆ’ xs ) + âˆ‡fs |xs Â· (xs âˆ’ xt,s )
â‰¤ fs (y) âˆ’ fs (xs ) + âˆ‡fs |xs Â· (xs âˆ’ xt,s ).
By assumption, we also have kâˆ‡fs |xs k2 â‰¤ L for each s âˆˆ Ft . With respect to the distance between
xt+1 and y, this gives,
X
2
2
kxt+1 âˆ’ yk2 â‰¤ kxt âˆ’ yk2 + 2Î·
(fs (y) âˆ’ fs (xs ) + âˆ‡fs |xs Â· (xs âˆ’ xt,s )) + Î· 2 Â· |Ft | Â· L2 .
sâˆˆFt

Solving this inequality for the regret terms
over all rounds t âˆˆ [T ], we have,
T
X

(ft (xt ) âˆ’ ft (y)) =

t=1

T X
X

P

sâˆˆFt

fs (xs ) âˆ’ fs (y) and taking the sum of inequalities

fs (xs ) âˆ’ fs (y)

t=1 sâˆˆFt

T
X
1 X
2
2
â‰¤
Â·
kxt âˆ’ yk2 âˆ’ kxt+1 âˆ’ yk2 + 2Î·
âˆ‡fs |xs Â· (xs âˆ’ xt,s ) + Î· 2 Â· |Ft | Â· L2
2Î· t=1
sâˆˆFt
!
T
T X
X
1 X
Î·
2
2
=
kxt âˆ’ yk2 âˆ’ kxt+1 âˆ’ yk2 + T L2 +
âˆ‡fs |xs Â· (xs âˆ’ xt,s )
2Î· t=1
2
t=1

!

sâˆˆFt

â‰¤

T
X

X
1
Î·
+ T L2 +
âˆ‡fs |xs Â· (xs âˆ’ xt,s ).
2Î·
2
t=1
sâˆˆFt

4

(2)

The first two terms are familiar from the standard analysis of online-gradient-descent. It
remains to analyze the last sum, which we call the delay term.
Each summand âˆ‡fs |xs Â· (xs âˆ’ xt,s ) in the delay term contributes loss proportional to the distance
between the point xs when the gradient âˆ‡fs |xs is generated and the point xt,s when the gradient is
applied. This distance is created by the other gradients that are applied in between, and the number
of such in-between gradients are intimately tied to the total delay, as follows. By Cauchy-Schwartz,
the delay term is bounded above by
T X
X

âˆ‡fs |xs Â· (xs âˆ’ xt,s ) â‰¤

t=1 sâˆˆFt

T X
X

kâˆ‡fs |xs k2 kxs âˆ’ xt,s k2 â‰¤ L

t=1 sâˆˆFt

T X
X

kxs âˆ’ xt,s k2 . (3)

t=1 sâˆˆFt

Consider a single term kxs âˆ’ xt,s k2 for fixed t âˆˆ [T ] and s âˆˆ Ft . Intuitively, the difference xt,s âˆ’xs
is roughly the sum of gradients received between round s and when we apply the gradient from round
s in round t. More precisely, by applying the triangle inequality and Lemma 2.2, we have,
kxt,s âˆ’ xs k2 â‰¤ kxt,s âˆ’ xt k2 + kxt âˆ’ xs k2 â‰¤ kxt,s âˆ’ xt k2 + kx0t âˆ’ xs k2 .


For the same reason, we have kx0t âˆ’ xs k2 â‰¤ kx0t âˆ’ xtâˆ’1 k2 + x0tâˆ’1 âˆ’ xs 2 , and unrolling in this
fashion, we have,
kxt,s âˆ’ xs k2 â‰¤ kxt,s âˆ’ xt k2 +

tâˆ’1
tâˆ’1 X
X
X 
X
 0




xr+1 âˆ’ xr  â‰¤ Î·
âˆ‡fp |x  + Î·
âˆ‡fq |x 
p 2
q 2
2
r=s

â‰¤Î·Â·LÂ·

|Ft,s | +

r=s qâˆˆFr

pâˆˆFt,s

!

tâˆ’1
X

|Fr | .

(4)

r=s

PT P
After substituting equation (4) into equation (3), it remains to bound the sum t=1 sâˆˆFt (|Ft,s | +
Ptâˆ’1
Ptâˆ’1
r=s |Fr |). Consider a single term |Ft,s | +
r=s |Fr | in the sum. This quantity counts, for a
gradient âˆ‡fs |xs from round s delivered just before round t â‰¥ s, the number of other gradients that
are applied while âˆ‡fs |xs is withheld. Fix two rounds s and t, and consider an intermediate round
r âˆˆ {s, . . . , t}. If r < t then fix q âˆˆ Fr , and if r = t then fix q âˆˆ Ft,s . The feedback from round q
is applied in a round r between round s and round t. We divide our analysis into two scenarios. In
one case, q â‰¤ s, and the gradient from round q appears only after s, as in the following diagram.
âˆ‡fq |xq

q

/ Â·Â·Â·

âˆ‡fs |xs

/s

/ Â·Â·Â·

/% r

/ Â·Â·Â·

"/

/ Â·Â·Â·

$/

t

In the other case, q > s, as in the following diagram.
âˆ‡fs |xs

s

/ Â·Â·Â·

âˆ‡fq |xq

/q

/ Â·Â·Â·

r

/) t

For each round u, let du denote the number of rounds the gradient feedback is delayed (so u âˆˆ
Fu+du ). There are at most ds instances of the latter case, since q must lie in s+1, . . . , t. The first case
can be charged to dq . To bound the first case, observe that for fixed q, the number of indices s such
that q < s â‰¤ dq + q â‰¤ ds + s is at most dq . That is, all instances of the second case for a fixed q can
PT P
Ptâˆ’1
PT
be charged to dq . Between the two cases, we have t=1 sâˆˆFt (|Ft,s | + r=s |Fr |) â‰¤ 2 t=1 dt ,
and the delay term is bounded by
T X
X

âˆ‡fs |xs Â· (xs âˆ’ xt,s ) â‰¤ 2Î· Â· L2

t=1 sâˆˆFt

T
X

dt .

t=1

With respect to the overall regret, this gives,
T
X

T
X
1
T
(f (xt ) âˆ’ f (y)) â‰¤
+ Î· Â· L2
+2
dt
2Î·
2
t=1
t=1

!


=O


1
+ Î·L2 D ,
Î·


as desired.
5

PT P
Remark 2.3. The delay term t=1 sâˆˆFt âˆ‡fs |xs Â· (xs âˆ’ xt,s ) is a natural point of entry for a
sharper analysis based on strong sparseness assumptions. The distance xs âˆ’ xt,s is measured by its
projection against the gradient âˆ‡fs |xs , and the preceding proof assumes the worst case and bounds
the dot product with the Cauchy-Schwartz inequality. If, for example, we assume that gradients
are pairwise orthogonal and analyze online-gradient-descent in the unconstrained setting,
then the dot product âˆ‡fs |xs Â· (xs âˆ’ xt,s ) is 0 and the delay term vanishes altogether.

3

Delaying the Perturbed Leader

Discrete online linear optimization In discrete online linear optimization, the input domain K âŠ‚
Rn is a (possibly discrete) set with bounded diameter, and each cost function ft is of the form
ft (x) = ct Â· x for a bounded-length cost vector ct . The previous algorithm online-gradientdescent does not apply here because K is not convex.
A natural algorithm for this problem is follow-the-leader. Each round t, let yt =
arg minxâˆˆK xÂ·(c1 +Â· Â· Â·+ct ) be the optimum choice over the first t cost vectors. The algorithm picking yt in round t is called be-the-leader, and can be shown to have zero regret. Of course, bethe-leader is infeasible since the cost vector ct is revealed after picking yt . follow-theleader tries the next best thing, picking ytâˆ’1 in round t. Unfortunately, this strategy can have
linear regret, largely because it is a deterministic algorithm that can be manipulated by an adversary.
Kalai and Vempala [2] gave a simple and elegant correction called follow-the-perturbedleader. Let  > 0 be a parameter to be fixed later, and let Q = [0, 1/]n be the cube of length
1/. Each round t, follow-the-perturbed-leader randomly picks a vector c0 âˆˆ Q by the
uniform distribution, and then selects xt = arg minxâˆˆK x Â· (c0 + Â· Â· Â· + ctâˆ’1 ) to optimize over the
previous costs plus the random perturbation c0 . With the diameter of K and the lengths kct k of each
cost vector held
âˆš constant, Kalai and Vempala showed that follow-the-perturbed-leader
has regret O( T ) in expectation.
Following the delayed and perturbed leader More generally, follow-the-perturbedleader optimizes over all information available to the algorithm, plus some additional noise to
smoothen the worst-case analysis. If the cost vectors are delayed, we naturally interpret followthe-perturbed-leader to optimize over all cost vectors ct delivered in time for round t when
picking its point xt . That is, the tth leader becomes the best choice with respect to all cost vectors
delivered in the first t rounds:
ytd = arg min
xâˆˆK

t X
X

cr Â· x

s=1 râˆˆFs

(we use the superscript d to emphasize the delayed setting). The tth perturbed leader optimizes over
all cost vectors delivered through the first t rounds in addition to the random perturbation c0 âˆˆ Q :
!
t X
X
yÌƒtd = arg min c0 Â· x +
cr Â· x .
xâˆˆK

s=1 râˆˆFs

d
In the delayed setting, follow-the-perturbed-leader chooses xt = yÌƒtâˆ’1
in round t. We
claim that follow-the-perturbed-leader has a direct âˆš
and simple regret bound in terms of
the sum of delays D, that collapses to Kalai and Vempalaâ€™s O( T ) regret bound in the undelayed
setting.
Theorem 3.1. Let K âŠ† Rn be a set with L1 -diameter â‰¤ 1, c1 , . . . , cT âˆˆ Rn with kct k1 â‰¤ 1 for all
t, and Î· > 0. In the presence of adversarial delays, follow-the-perturbed-leader picks
points x1 , . . . , xT âˆˆ K such that for all y âˆˆ K,
T
X
t=1

E[ct Â· xt ] â‰¤

T
X


ct Â· y + O âˆ’1 + D .

t=1

âˆš
âˆš
For  = 1/ D, Theorem 3.1 implies a regret bound of O( D). When D is not known a priori, the
doubling trick can be used to adjust  dynamically (see the discussion following Theorem 2.1).
6

To analyze follow-the-perturbed-leader in the presence of delays, we introduce the notion of a prophet, who is a sort of omniscient leader who sees the feedback immediately. Formally,
the tth prophet is the best point with respect to all the cost vectors over the first t rounds:
zt = arg min(c1 + Â· Â· Â· + ct ) Â· x.
xâˆˆK

The tth perturbed prophet is the best point with respect to all the cost vectors over the first t rounds,
in addition to a perturbation c0 âˆˆ Q :
zÌƒt = arg min(c0 + c1 + Â· Â· Â· + ct ) Â· x.

(5)

xâˆˆK

The prophets and perturbed prophets behave exactly as the leaders and perturbed leaders in the
setting of Kalai and Vempala with no delays. In particular, we can apply the regret bound of Kalai
and Vempala to the (infeasible) strategy of following the perturbed prophet.
Lemma 3.2 ([2]). Let K âŠ† Rn be a set with L1 -diameter â‰¤ 1, let c1 , . . . , cT âˆˆ Rn be cost vectors
bounded by kct k1 â‰¤ 1 for all t, and let  > 0. If zÌƒ1 , . . . , zÌƒT âˆ’1 âˆˆ K are chosen per equation (5),

PT
PT
then t=1 E[ct Â· zÌƒiâˆ’1 ] â‰¤ t=1 ct Â· y + O âˆ’1 + T . for all y âˆˆ K.
The analysis by Kalai and Vempala observes that when there are no delays, two consecutive perturbed leaders yÌƒt and yÌƒt+1 are distributed similarly over the random noise [2, Lemma 3.2]. Instead,
we will show that yÌƒtd and zÌƒt are distributed in proportion to delays. We first require a technical
lemma that is implicit in [2].
Lemma 3.3. Let K be a set with L1 -diameter â‰¤ 1, and let u, v âˆˆ Rn be vectors. Let y, z âˆˆ Rn
be random vectors defined by y = arg minyâˆˆK (q + u) Â· y and z = arg minzâˆˆK (q + v) Â· z, where
Qn
q is chosen uniformly at random from Q = i=1 [0, r], for some fixed length r > 0. Then, for any
vector c,
E[c Â· z] âˆ’ E[c Â· y] â‰¤

kv âˆ’ uk1 kckâˆ
.
r

Proof. Let Q0 = v+Q and Q00 = u+Q, and write y = arg minyâˆˆK q 00 Â·y and z = arg minzâˆˆK q 0 Â·z,
where q 0 âˆˆ Q0 and q 00 âˆˆ Q00 are chosen uniformly at random. Then
E[c Â· z] âˆ’ E[c Â· y] = Eq00 âˆˆQ00 [c Â· z] âˆ’ Eq0 âˆˆQ0 [c Â· y].
0

0

Subtracting P[q âˆˆ Q âˆ© Q00 ]Eq0 âˆˆQ0 âˆ©Q00 [c Â· z] from both terms on the right, we have
Eq00 âˆˆQ00 [c Â· z] âˆ’ Eq0 âˆˆQ0 [c Â· y]
= P[q 00 âˆˆ Q00 \ Q0 ] Â· Eq00 âˆˆQ00 \Q0 [c Â· z] âˆ’ P[q 0 âˆˆ Q0 \ Q00 ] Â· Eq0 âˆˆQ0 \Q00 [c Â· y]
By symmetry, P[q 00 âˆˆ Q00 \ Q0 ] = P[q 0 âˆˆ Q0 \ Q00 ], and we have,
E[c Â· z] âˆ’ E[c Â· y] â‰¤ (P[q 00 âˆˆ Q00 \ Q0 ])Eq00 âˆˆQ00 \Q0 ,q0 âˆˆQ0 \Q00 [c Â· (z âˆ’ y)].
By assumption, K has L1 -diameter â‰¤ 1, so ky âˆ’ zk1 â‰¤ 1, and by HÃ¶lderâ€™s inequality, we have,
E[c Â· z] âˆ’ E[c Â· y] â‰¤ P[q 00 âˆˆ Q00 \ Q0 ]kckâˆ .
It remains to bound P[q 00 âˆˆ Q00 \ Q0 ] = P[q 0 âˆˆ Q0 \ Q00 ]. If kv âˆ’ uk1 â‰¤ r, we have,



n
n 
Y
Y
kv âˆ’ uk1
|(vi âˆ’ ui )|
0
0
00
0
â‰¥ vol(Q ) 1 âˆ’
.
vol(Q âˆ© Q ) =
(r âˆ’ |vi âˆ’ ui |) = vol(Q )
1âˆ’
r
r
i=1
i=1
Otherwise, if ku âˆ’ vk1 > r, then vol(Q0 âˆ© Q00 ) = 0 â‰¥ vol(Q0 )(1 âˆ’ kv âˆ’ uk1 /r). In either case,
we have,
P[q 0 âˆˆ Q0 \ Q00 ] =

kv âˆ’ uk1
vol(Q0 âˆ© Q00 )
vol(Q0 âˆ© Q00 )
â‰¤1âˆ’
â‰¤
,
0
0
vol(Q )
vol(Q )
r


and the claim follows.
Lemma 3.3 could also have been proven geometrically in similar fashion to Kalai and Vempala.
7

Lemma 3.4.
vectors.

PT

t=1



d
â‰¤ D, where D is the sum of delays of all cost
E[ct Â· zÌƒtâˆ’1 ] âˆ’ E ct Â· yÌƒtâˆ’1

Pt
P
Proof. Let ut = s=1 ct be the sum of all costs through the first t rounds, and vt = s:s+ds â‰¤t ct
be the sum of cost vectors actually delivered through the first t rounds. Then the perturbed prophet
d
zÌƒtâˆ’1 optimizes over c0 + utâˆ’1 and yÌƒtâˆ’1
optimizes over c0 + vtâˆ’1 . By Lemma 3.3, for each t, we
have


d
â‰¤  Â· kutâˆ’1 âˆ’ vtâˆ’1 k1 kct kâˆ â‰¤  Â· |{s < t : s + ds â‰¥ t}|
Ec0 âˆ¼Q [ct Â· zÌƒtâˆ’1 ] âˆ’ Ec0 âˆ¼Q ct Â· yÌƒtâˆ’1
Summed over all T rounds, we have,
T
X



Ec0 [ct Â· zÌƒt ] âˆ’ Ec0 ct Â·

yÌƒtd



T
X
â‰¤
|{s < t : s + ds â‰¥ t}|.

t=1

t=1

PT

The sum t=1 |{s < t : s + ds â‰¥ t}| charges each cost vector cs once for every round it is delayed,


PT
and therefore equals D. Thus, t=1 Ec0 [ct Â· zÌƒt ] âˆ’ Ec0 ct Â· yÌƒtd â‰¤ D, as desired.

Now we complete the proof of Theorem 3.1.
Proof of Theorem 3.1. By Lemma 3.4 and Lemma 3.2, we have,
T
T
T
X
X

 X
d
E[ct Â· x] + O(âˆ’1 + D),
E ct Â· yÌƒtâˆ’1
E[ct Â· zÌƒtâˆ’1 ] + D â‰¤ arg min
â‰¤
t=1

xâˆˆK

t=1

4

t=1



as desired.

Conclusion

âˆš
We prove O( D) regret bounds for online-gradient-descent
âˆš and follow-theperturbed-leader in the delayed setting, directly extending the O( T ) regret bounds known
in the undelayed setting. More importantly, by deriving a simple bound as a function of the delays, without any restriction on the delays, we establish a simple and intuitive model for measuring
delayed learning. This work suggests natural relationships between the regret bounds of online
learning algorithms and delays in the feedback.
Beyond analyzing existing algorithms, we hope that optimizing over the regret as a function of D
may inspire different (and hopefully simple) algorithms that readily model real world applications
and scale nicely to distributed environments.
Acknowledgements We thank Avrim Blum for introducing us to the area of online learning and
helping us with several valuable discussions. We thank the reviewers for their careful and insightful
reviews: finding errors, referencing relevant works, and suggesting a connection to mirror descent.

References
[1] M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proc. 20th
Int. Conf. Mach. Learning (ICML), pages 928â€“936, 2003.
[2] A. Kalai and S. Vempala. Efficient algorithms for online decision problems. J. Comput. Sys. Sci., 71:291â€“
307, 2005. Extended abstract in Proc. 16th Ann. Conf. Comp. Learning Theory (COLT), 2003.
[3] A. Blum. On-line algorithms in machine learning. In A. Fiat and G. Woeginger, editors, Online algorithms, volume 1442 of LNCS, chapter 14, pages 306â€“325. Springer Berlin Heidelberg, 1998.
[4] S. Shalev-Shwartz. Online learning and online convex optimization. Found. Trends Mach. Learn.,
4(2):107â€“194, 2011.
[5] E. Hazan. Introduction to online convex optimization. Internet draft available at http://ocobook.
cs.princeton.edu, 2015.
[6] E. Takimoto and M. Warmuth. Path kernels and multiplicative updates. J. Mach. Learn. Research, 4:773â€“
818, 2003.

8

[7] D. Helmbold and R. Schapire. Predicting nearly as well as the best pruning of a decision tree. Mach.
Learn. J., 27(1):61â€“68, 1997.
[8] A. Blum, S. Chawla, and A. Kalai. Static optimality and dynamic search optimality in lists and trees.
Algorithmica, 36(3):249â€“260, 2003.
[9] T. M. Cover. Universal portfolios. Math. Finance, 1(1):1â€“29, 1991.
[10] K. Crammer and Y. Singer. A family of additive online algorithms for category ranking. J. Mach. Learn.
Research, 3:1025â€“1058, 2003.
[11] X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers, and J. QuiÃ±onero
Candela. Practical lessons from predicting clicks on ads at facebook. In Proc. 20th ACM Conf. Knowl.
Disc. and Data Mining (KDD), pages 1â€“9. ACM, 2014.
[12] S. Amuru and R. M. Buehrer. Optimal jamming using delayed learning. In 2014 IEEE Military Comm.
Conf. (MILCOM), pages 1528â€“1533. IEEE, 2014.
[13] I. Menache, O. Shamir, and N. Jain. On-demand, spot, or both: Dynamic resource allocation for executing
batch jobs in the cloud. In 11th Int. Conf. on Autonomic Comput. (ICAC), 2014.
[14] M.J. Weinberger and E. Ordentlich. On delayed prediction of individual sequences. IEEE Trans. Inf.
Theory, 48(7):1959â€“1976, 2002.
[15] P. Joulani, A. GyÃ¶rgy, and C. SzepesvÃ¡ri. Online learning under delayed feedback. In Proc. 30th Int.
Conf. Mach. Learning (ICML), volume 28, 2013.
[16] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. PrenticeHall, 1989.
[17] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: a lock-free approach to parallelizing stochastic gradient
descent. In Adv. Neural Info. Proc. Sys. 24 (NIPS), pages 693â€“701, 2011.
[18] J. Duchi, M.I. Jordan, and B. McMahan. Estimation, optimization, and parallelism when data is sparse.
In Adv. Neural Info. Proc. Sys. 26 (NIPS), pages 2832â€“2840, 2013.
[19] H.B. McMahan and M. Streeter. Delay-tolerant algorithms for asynchronous distributed online learning.
In Adv. Neural Info. Proc. Sys. 27 (NIPS), pages 2915â€“2923, 2014.
[20] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. J. Mach. Learn. Research, 12:2121â€“2159, July 2011.
[21] J. Langford, A. J. Smola, and M. Zinkevich. Slow learners are fast. In Adv. Neural Info. Proc. Sys. 22
(NIPS), pages 2331â€“2339, 2009.
[22] J. Liu, S. J. Wright, C. RÃ©, V. Bittorf, and S. Sridhar. An asynchronous parallel stochastic coordiante
descent algorithm. J. Mach. Learn. Research, 16:285â€“322, 2015.
[23] J. C. Duchi, T. Chaturapruek, and C. RÃ©. Asynchronous stochastic convex optimization.
abs/1508.00882, 2015. To appear in Adv. Neural Info. Proc. Sys. 28 (NIPS), 2015.

CoRR,

[24] S. J. Wright. Coordinate descent algorithms. Math. Prog., 151(3â€“34), 2015.
[25] D. Riabko. On the flexibility of theoretical models for pattern recognition. PhD thesis, University of
London, April 2005.
[26] N. Cesa-Bianchi, Y. Freund, D. Haussler, D.P. Helmbold, R.E. Schapire, and M.K. Warmuth. How to use
expert advice. J. Assoc. Comput. Mach., 44(3):426â€“485, 1997.

9

