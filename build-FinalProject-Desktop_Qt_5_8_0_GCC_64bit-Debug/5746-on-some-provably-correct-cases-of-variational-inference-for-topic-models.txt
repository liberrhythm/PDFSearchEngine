On some provably correct cases of variational
inference for topic models

Andrej Risteski
Department of Computer Science
Princeton University
Princeton, NJ 08540
risteski@cs.princeton.edu

Pranjal Awasthi
Department of Computer Science
Rutgers University
New Brunswick, NJ 08901
pranjal.awasthi@rutgers.edu

Abstract
Variational inference is an efficient, popular heuristic used in the context of latent
variable models. We provide the first analysis of instances where variational inference algorithms converge to the global optimum, in the setting of topic models.
Our initializations are natural, one of them being used in LDA-c, the most popular
implementation of variational inference. In addition to providing intuition into
why this heuristic might work in practice, the multiplicative, rather than additive
nature of the variational inference updates forces us to use non-standard proof
arguments, which we believe might be of general theoretical interest.

1

Introduction

Over the last few years, heuristics for non-convex optimization have emerged as one of the most
fascinating phenomena for theoretical study in machine learning. Methods like alternating minimization, EM, variational inference and the like enjoy immense popularity among ML practitioners,
and with good reason: they’re vastly more efficient than alternate available methods like convex
relaxations, and are usually easily modified to suite different applications.
Theoretical understanding however is sparse and we know of very few instances where these methods come with formal guarantees. Among more classical results in this direction are the analyses of
Lloyd’s algorithm for K-means, which is very closely related to the EM algorithm for mixtures of
Gaussians [20], [13], [14]. The recent work of [9] also characterizes global convergence properties
of the EM algorithm for more general settings. Another line of recent work has focused on a different heuristic called alternating minimization in the context of dictionary learning. [1], [6] prove that
with appropriate initialization, alternating minimization can provably recover the ground truth. [22]
have proven similar results in the context of phase retreival.
Another popular heuristic which has so far eluded such attempts is known as variational inference [19]. We provide the first characterization of global convergence of variational inference based
algorithms for topic models [12]. We show that under natural assumptions on the topic-word matrix
and the topic priors, along with natural initialization, variational inference converges to the parameters of the underlying ground truth model. To prove our result we need to overcome a number
of technical hurdles which are unique to the nature of variational inference. Firstly, the difficulty
in analyzing alternating minimization methods for dictionary learning is alleviated by the fact that
one can come up with closed form expressions for the updates of the dictionary matrix. We do
not have this luxury. Second, the “norm” in which variational inference naturally operates is KL
divergence, which can be difficult to work with. We stress that the focus of this work is not to identify new instances of topic modeling that were previously not known to be efficiently solvable, but
rather providing understanding about the behaviour of variational inference, the defacto method for
learning and inference in the context of topic models.
1

2

Latent variable models and EM

We briefly review EM and variational methods. The setting is latent variable models, where
the observations Xi are generated according to a distribution P (Xi |θ) = P (Zi |θ)P (Xi |Zi , θ)
where θ are parameters of the models, and Zi is a latent variable. Given the observations
Xi , a
X
common task is to find the max likelihood value of the parameter θ: argmaxθ
log(P (Xi |θ)).
i

The EM algorithm is an iterative method to achieve this, dating all the way back to [15] and
[24] in the 70s. In the above framework it can be formulated as the following procedure, maintaining estimates θt , P̃ t (Z) of the model parameters and the posterior distribution over the hidt
t
den variables: In the E-step, we
Xcompute the distribution P̃ (Z) = P (Z|X, θ ). In the Mstep, we set θt+1 = argmaxθ
EP̃ t [log P (Xi , Zi |θ)]. Sometimes even the above two steps
i

may not be computationally feasible, in which case they can be relaxed by choosing a family of simple distributions F , and performing the following updates. In the variational E-step,
we compute the distribution P̃ t (Z) = min
KL(P t (Z)||P (Z|X, θt )). In the M-step, we set
P t ∈F
X
EP̃ t [log P (Xi , Zi |θ)]. By picking the family F appropriately, it’s often possiθt+1 = argmaxθ
i

ble to make both steps above run in polynomial time. As expected, none of the above two families
of approximations, come with any provable global convergence guarantees. With EM, the problem
is ensuring that one does not get stuck in a local optimum. With variational EM, additionally, we
are faced with the issue of in principle not even exploring the entire space of solutions.

3

Topic models and prior work

We focus on a particular, popular latent variable model - topic models [12]. The generative model
over word documents is the following. For each document in the corpus, a proportion of topics
γ1 , γ2 , . . . , γk is sampled according to a prior distribution α. Then, for each position p in the document, we pick a topic Zp according to a multinomial with parameters γ1 , . . . , γk . Conditioned on
Zp = i, we pick a word j from a multinomial with parameters (βi,1 , βi,2 , . . . , βi,k ) to put in position
p. The matrix of values {βi,j } is known as the topic-word matrix.
The body of work on topic models is vast [11]. Prior theoretical work relevant in the context of
this paper includes the sequence of works by [7],[4], as well as [2], [16], [17] and [10]. [7] and
[4] assume that the topic-word matrix contains “anchor words”. This means that each topic has a
word which appears in that topic, and no other. [2] on the other hand work with a certain expansion
assumption on the word-topic graph, which says that if one takes a subset S of topics, the number
of words in the support of these topics should be at least |S| + smax , where smax is the maximum
support size of any topic. Neither paper needs any assumption on the topic priors, and can handle
(almost) arbitrarily short documents.
The assumptions we make on the word-topic matrix will be related to the ones in the above works,
but our documents will need to be long, so that the empirical counts of the words are close to their
expected counts. Our priors will also be more structured. This is expected since we are trying to
analyze an existing heuristic rather than develop a new algorithmic strategy. The case where the
documents are short seems significantly more difficult. Namely, in that case there are two issues to
consider. One is proving the variational approximation to the posterior distribution over topics is not
too bad. The second is proving that the updates do actually reach the global optimum. Assuming
long documents allows us to focus on the second issue alone, which is already challenging. On a
high level, the instances we consider will have the following structure:
• The topics will satisfy a weighted expansion property: for any set S of topics of constant size,
for any topic i in this set, the probability mass on words which belong to i, and no other topic in
S will be large. (Similar to the expansion in [2], but only over constant sized subsets.)
• The number of topics per document will be small. Further, the probability of including a given
topic in a document is almost independent of any other topics that might be included in the
document already. Similar properties are satisfied by the Dirichlet prior, one of the most popular
2

priors in topic modeling. (Originally introduced by [12].) The documents will also have a
“dominating topic”, similarly as in [10].
• For each word j, and a topic i it appears in, there will be a decent proportion of documents that
contain topic i and no other topic containing j. These can be viewed as “local anchor documents”
for that word-pair topic.
We state below, informally, our main result. See Sections 6 and 7 for more details.
Theorem. Under the above mentioned assumptions, popular variants of variational inference for
topic models, with suitable initializations, provably recover the ground truth model in polynomial
time.

4

Variational relaxation for learning topic models

In this section we briefly review the variational relaxation for topic models, following closely [12].
Throughout the paper, we will denote by N the total number of words and K the number of topics.
We will assume that we are working with a sample set of D documents. We will also denote by
f˜d,j the fractional count of word j in document d (i.e. f˜d,j = Count(j)/Nd , where Count(j) is the
number of times word j appears in the document, and Nd is the number of words in the document).
For topic models variational updates are a way to approximate the computationally intractable
E-step [23] as described in Section 2. Recall the model parameters for topic models are the
topic prior parameters α and the topic-word matrix β. The observable X is the list of words
in the document. The latent variables are the topic assignments Zj at each position j in the
document and the topic proportions γ. The variational E-step hence becomes P̃ t (Z, γ) =
minP t ∈F KL(P t (Z, γ)||P (Z, γ|X, αt , β t ) for some family F of distributions. The family F one
0
d
usually considered is P t (γ, Z) = q(γ)ΠN
j=1 qj (Zj ), i.e. a mean field family. In [12] it’s shown
that for Dirichlet priors α the optimal distributions q, qj0 are a Dirichlet distribution for q, with some
parameter γ̃, and multinomials for qj0 , with some parameters φj . The variational EM updates are
shown to have the following form. In the E-step, one runs to convergence the following updates on
PNd
t
t
the φ and γ̃ parameters: φd,j,i ∝ βi,w
eEq [log(γd )|γ̃d ] , γ˜d,i = αd,i
+ j=1
φd,j,i . In the M-step, one
d,j
N
D
d
XX
t+1
updates the β and parameters by setting βi,j
∝
φtd,j,i wd,j,j 0 where φtd,j,i is the converged
d=1 j 0 =1

value of φd,j,i ; wd,j is the word in document d, position j; wd,j,j 0 is an indicator variable which is 1
if the word in position j 0 in document d is word j. The α Dirichlet parameters do not have a closed
form expression and are updated via gradient descent.
4.1

Simplified updates in the long document limit

From the above updates it is difficult to give assign an intuitive meaning to the γ̃ and φ parameters.
(Indeed, it’s not even clear what one would like them to be ideally at the global optimum.) We will
be however working in the large document limit - and this will simplify the updates. In particular,
in the E-step, in the large document limit, the first term in the update equation for γ̃ has a vanishing
PNd
t
contribution. In this case, we can simplify the E-update as: φd,j,i ∝ βi,j
γd,i , γd,i ∝ j=1
φd,j,i .
Notice, importantly, in the second update we now use variables γd,i instead of γ̃d,i , which are norK
X
malized such that
γd,i = 1. These correspond to the max-likelihood topic proportions, given
i=1
t
our current estimates βi,j
for the model parameters. The M-step will remain as is - but we will
focus on the β only, and ignore the α updates - as the α estimates disappeared from the E updates:
D
X
t+1
t
t
βi,j
∝
f˜d,j γd,i
, where γd,i
is the converged value of γd,i . In this case, the intuitive meand=1

ing of the β t and γ t variables is clear: they are estimates of the the model parameters, and the
max-likelihood topic proportions, given an estimate of the model parameters, respectively.
The way we derived them, these updates appear to be an approximate form of the variational updates
in [12]. However it is possible to also view them in a more principled manner. These updates
3

approximate the posterior distribution P (Z, γ|X, αt , β t ) by first approximating this posterior by
P (Z|X, γ ∗ , αt , β t ), where γ ∗ is the max-likelihood value for γ, given our current estimates of
α, β, and then setting P (Z|X, γ ∗ , αt , β t ) to be a product distribution. It is intuitively clear that
in the large document limit, this approximation should not be much worse than the one in [12],
as the posterior concentrates around the maximum likelihood value. (And in fact, our proofs will
work for finite, but long documents.) Finally, we will rewrite the above equations in a slightly
K
X
t
more convenient form. Denoting fd,j =
γd,i βi,j
, the E-step can be written as: iterate until
i=1

convergence γd,i = γd,i

N
X
j=1

t
fd,j
=

K
X

f˜d,j t
t+1
t
β . The M-step becomes: βi,j
= βi,j
fd,j i,j

f˜d,j t
t γd,i
d=1 fd,j

PD

PD

d=1

t
γd,i

where

t
t
t
γd,i
βi,j
and γd,i
is the converged value of γd,i .

i=1

4.2

Alternating KL minimization and thresholded updates

We will further modify the E and M-step update equations we derived above. In a slightly modified
form, these updates were used in a paper by [21] in the context of non-negative matrix factorization.
PD
t
There the authors proved that under these updates d=1 KL(fd,j
||f˜d,j ) is non-decreasing. One can
easily modify their arguments to show that the same property is preserved if the E-step is replaced
by a step γdt = minγdt ∈∆K KL(f˜d ||fd ), where ∆K is the K-dimensional simplex - i.e. minimizing
the KL divergence between the counts and the ”predicted counts” with respect to the γ variables. (In
fact, iterating the γ updates above is a way to solve this convex minimization problem via a version
of gradient descent which makes multiplicative updates, rather than additive updates.)
Thus the updates are performing alternating minimization using the KL divergence as the distance
measure (with the difference that for the β variables one essentially just performs a single gradient
step). In this paper, we will make a modification of the M-step which is very natural. Intuitively, the
t
update for βi,j
goes over all appearances of the word j and adds the “fractional assignment” of the
word j to topic i under our current estimates of the variables β, γ. In the modified version we will
0
t
t
> γd,i
only average over those documents d, where γd,i
0 , ∀i 6= i. The intuitive reason behind this
modification is the following. The EM updates we are studying work with the KL divergence, which
t
puts more weight on the larger entries. Thus, for the documents in Di , the estimates for γd,i
should
t
be better than they might be in the documents D \ Di . (Of course, since the terms fd,j
involve all
t
the variables γd,i
, it is not a priori clear that this modification will gain us much, but we will prove
that it in fact does.) Formally, we discuss the three modifications of variational inference specified
as Algorithm 1, 2 and 3 (we call them tEM, for thresholded EM):
Algorithm 1 KL-tEM
t
(E-step) Solve the following convex program for each document d: minγd,i
P t
t
t
γd,i ≥ 0, i γd,i = 1 and γd,i = 0 if i is not in the support of document d
t
t
0
(M-step) Let Di to be the set of documents d, s.t. γd,i
> γd,i
0 , ∀i 6= i.

P ˜
f˜d,j
j fd,j log( f t ), s.t.
d,j

f˜d,j t
γd,i
d∈Di f t
d,j
P
t
γ
d∈Di d,i

P
t+1
t
Set βi,j
= βi,j

5

Initializations

We will consider two different strategies for initialization. First, we will consider the case where
we initialize with the topic-word matrix, and the document priors having the correct support. The
analysis of tEM in this case will be the cleanest. While the main focus of the paper is tEM, we’ll
show that this initialization can actually be done for our case efficiently. Second, we will consider
an initialization that is inspired by what the current LDA-c implementation uses. Concretely, we’ll
4

Algorithm 2 Iterative tEM
(E-step) Initialize γd,i uniformly among the topics in the support of document d.
Repeat
N ˜
X
fd,j t
γd,i = γd,i
βi,j
f
j=1 d,j

(4.1)

until convergence.
(M-step) Same as above.
Algorithm 3 Incomplete tEM
(E-step) Initialize γd,i with the values gotten in the previous iteration, then perform just one step
of 4.1.
(M-step) Same as before.

assume that the user has some way of finding, for each topic i, a seed document in which the
proportion of topic i is at least Cl . Then, when initializing, one treats this document as if it were
0
pure: namely one sets βi,j
to be the fractional count of word j in this document. We do not attempt
to design an algorithm to find these documents.

6

Case study 1: Sparse topic priors, support initialization

We start with a simple case. As mentioned, all of our results only hold in the long documents
regime: we will assume for each document d, the number of sampled words is large enough, so that
∗
one can approximate the expected frequencies of the words, i.e., one can find values γd,i
, such that
P
K
∗
∗
f˜d,j = (1±) i=1 γd,i βi,j . We’ll split the rest of the assumptions into those that apply to the topicword matrix, and the topic priors. Let’s first consider the assumptions on the topic-word matrix. We
will impose conditions that ensure the topics don’t overlap too much. Namely, we assume:
• Words are discriminative: Each word appears in o(K) topics.
P
∗
• Almost disjoint supports: ∀i, i0 , if the intersection of the supports of i and i0 is S, j∈S βi,j
≤
P ∗
o(1) · j βi,j .
We also need assumptions on the topic priors. The documents will be sparse, and all topics will
be roughly equally likely to appear. There will be virtually no dependence between the topics:
conditioning on the size or presence of a certain topic will not influence much the probability of
another topic being included. These are analogues of distributions that have been analyzed for
dictionary learning [6]. Formally:
• Sparse and gapped documents: Each of the documents in our samples has at most T = O(1)
∗
topics. Furthermore, for each document d, the largest topic i0 = argmaxi γd,i
is such that for any
0
∗
∗
other topic i , γd,i0 − γd,i0 > ρ for some (arbitrarily small) constant ρ.
∗
∗
0
• Dominant topic equidistribution: The probability that topic i is such that γd,i
> γd,i
0 , ∀i 6= i is
Θ(1/K).
• Weak topic correlations and independent topic distribution: For all sets S with o(K) topics, it
∗
∗
∗
∗
∗
must be the case that: E[γd,i
|γd,i
is dominating] = (1 ± o(1))E[γd,i
|γd,i
is dominating, γd,i
0 =
0
∗
∗
0
0, i ∈ S]. Furthermore, for any set S of topics, s.t. |S| ≤ T − 1, Pr[γd,i > 0|γd,i0 ∀i ∈ S] =
1
)
Θ( K
These assumptions are a less smooth version of properties of the Dirichlet prior. Namely, it’s a
folklore result that Dirichlet draws are sparse with high probability, for a certain reasonable range of
parameters. This was formally proven by [25] - though sparsity there means a small number of large
coordinates. It’s also well known that Dirichlet essentially cannot enforce any correlation between
different topics. 1
1
We show analogues of the weak topic correlations property and equidistribution in the supplementary
material for completeness sake.

5

The above assumptions can be viewed as a local notion of separability of the model, in the following
sense. First, consider a particular document d. For each topic i that participates in that document,
consider the words j, which only appear in the support of topic i in the document. In some sense,
these words are local anchor words for that document: these words appear only in one topic of that
document. Because of the ”almost disjoint supports” property, there will be a decent mass on these
∗
words in each document. Similarly, consider a particular non-zero element βi,j
of the topic-word
∗
matrix. Let’s call Dl the set of documents where βi0 ,j = 0 for all other topics i0 6= i appearing in
that document. These documents are like local anchor documents for that word-topic pair: in those
documents, the word appears as part of only topic i. It turns out the above properties imply there is
a decent number of these for any word-topic pair.
1
∗
∗
are at least poly(N
Finally, a technical condition: we will also assume that all nonzero γd,i
, βi,j
).
Intuitively, this means if a topic is present, it needs to be reasonably large, and similarly for words
in topics. Such assumptions also appear in the context of dictionary learning [6].

We will prove the following
Theorem 1. Given an instance of topic modelling satisfying the properties specified above, where
2
N
t
t
the number of documents is Ω( K log
), if we initialize the supports of the βi,j
and γd,i
variables
2
0
correctly, after O (log(1/ ) + log N ) KL-tEM, iterative-tEM updates or incomplete-tEM updates,
we recover the topic-word matrix and topic proportions to multiplicative accuracy 1 + 0 , for any 0
1
s.t. 1 + 0 ≤ (1−)
7.
Theorem 2. If the number of documents is Ω(K 4 log2 K), there is a polynomial-time procedure
1
∗
∗
which with probability 1 − Ω( K
) correctly identifies the supports of the βi,j
and γd,i
variables.
Provable convergence of tEM: The correctness of the tEM updates is proven in 3 steps:
t
• Identifying dominating topic: First, we prove that if γd,i
is the largest one among all topics in the
document, topic i is actually the largest topic.
• Phase I: Getting constant multiplicative factor estimates: After initialization, after O(log N )
t
t
rounds, we will get to variables βi,j
, γd,i
which are within a constant multiplicative factor from
∗
∗
βi,j , γd,i .
• Phase II (Alternating minimization - lower and upper bound evolution): Once the β and γ estimates are within a constant factor of their true values, we show that the lone words and documents have a boosting effect: they cause the multiplicative upper and lower bounds to improve
at each round.

The updates we are studying are multiplicative, not additive in nature, and the objective they are
optimizing is non-convex, so the standard techniques do not work. The intuition behind our proof in
t
Phase II can be described as follows. Consider one update for one of the variables, say βi,j
. We show
t+1
∗
t ∗
t
that βi,j ≈ αβi,j + (1 − α)C βi,j for some constant C at time step t. α is something fairly large
(one should think of it as 1 − o(1)), and comes from the existence of the local anchor documents.
A similar equation holds for the γ variables, in which case the “good” term comes from the local
anchor words. Furthermore, we show that the error in the ≈ decreases over time, as does the value
∗
of C t , so that eventually we can reach βi,j
. The analysis bears a resemblance to the state evolution
and density evolution methods in error decoding algorithm analysis - in the sense that we maintain
a quantity about the evolving system, and analyze how it evolves under the specified iterations. The
quantities we maintain are quite simple - upper and lower multiplicative bounds on our estimates at
any round t.
Initialization: Recall the goal of this phase is to recover the supports - i.e. to find out which topics
are present in a document, and identify the support of each topic. We will find the topic supports
first. This uses an idea inspired by [8] in the setting of dictionary learning. Roughly, we devise a
test, which will take as input two documents d, d0 , and will try to determine if the two documents
have a topic in common or not. The test will have no false positives, i.e., will never say YES, if the
documents don’t have a topic in common, but might say NO even if they do. We then ensure that
with high probability, for each topic we find a pair of documents intersecting in that topic, such that
the test says YES. 2
2

The detailed initialization algorithm is included in the supplementary material.

6

7

Case study 2: Dominating topics, seeded initialization

Next, we’ll consider an initialization which is essentially what the current implementation of LDA-c
uses. Namely, we will call the following initialization a seeded initialization:
∗
• For each topic i, the user supplies a document d, in which γd,i
≥ Cl .
0
∗
• We treat the document as if it only contains topic i and initialize with βi,j
= fd,j
.

We show how to modify the previous analysis to show that with a few more assumptions, this
strategy works as well. Firstly, we will have to assume anchor words, that make up a decent fraction
of the mass of each topic. Second, we also assume that the words have a bounded dynamic range, i.e.
the values of a word in two different topics are within a constant B from each other. The documents
are still gapped, but the gap now must be larger. Finally, in roughly 1/B fraction of the documents
where topic i is dominant, that topic has proportion 1 − δ, for some small (but still constant) δ. A
similar assumption (a small fraction of almost pure documents) appeared in a recent paper by [10].
Formally, we have:
∗
• Small dynamic range and large fraction of anchors: For each discriminative words, if βi,j
6= 0
∗
and βi∗0 ,j 6= 0, βi,j
≤ Bβi∗0 ,j . Furthermore, each topic i has anchor words, such that their total
weight is at least p.
• Gapped documents: In each document, the largest topic has proportion at least Cl , and all the
other topics are at most Cs , s.t.
s 
!

p
1
1
Cl − Cs ≥
2 p log( ) + (1 − p) log(BCl ) + log(1 + ) + 
p
Cl

• Small fraction of 1 − δ dominant documents: Among all the documents where topic i is domi∗
nating, in a 8/B fraction of them, γd,i
≥ 1 − δ, where
s 
!
!

p
p
Cl2
1
1
δ := min
−
2 p log( ) + (1 − p) log(BCl ) + log(1 + ) − , 1 − Cl
2B 3
p
Cl
The dependency between the parameters B, p, Cl is a little difficult to parse, but if one thinks of√Cl
as 1−η for η small, and p ≥ 1− logη B , since log( C1l ) ≈ 1+η, roughly we want that Cl −Cs  p2 η.
(In other words, the weight we require to have on the anchors depends only logarithmically on the
range B.) In the documents where the dominant topic has proportion 1 − δ, a similar reasoning as
2√
1 − 2η
∗
+
η. The precise statement is as
above gives that we want is approximately γd,i
≥ 1−
2B 3
p
follows:
Theorem 3. Given an instance of topic modelling satisfying the properties specified above,
2
N
where the number of documents is Ω( K log
), if we initialize with seeded initialization, after
2
0
O (log(1/ ) + log N ) of KL-tEM updates, we recover the topic-word matrix and topic proportions
1
to multiplicative accuracy 1 + 0 , if 1 + 0 ≥ (1−)
7.
The proof is carried out in a few phases:
• Phase I: Anchor identification: We show that as long as we can identify the dominating topic in
each of the documents, anchor words will make progress: after O(log N ) number of rounds, the
values for the topic-word estimates will be almost zero for the topics for which word w is not an
anchor. For topic for which a word is an anchor we’ll have a good estimate.
• Phase II: Discriminative word identification: After the anchor words are properly identified in
∗
t
the previous phase, if βi,j
= 0, βi,j
will keep dropping and quickly reach almost zero. The
∗
values corresponding to βi,j 6= 0 will be decently estimated.
• Phase III: Alternating minimization: After Phase I and II above, we are back to the scenario of
the previous section: namely, there is improvement in each next round.
During Phase I and II the intuition is the following: due to our initialization, even in the beginning,
each topic is ”correlated” with the correct values. In a γ update, we are minimizing KL(f˜d ||fd )
with respect to the γd variables, so we need a way to argue that whenever the β estimates are not too
bad, minimizing this quantity provides an estimate about how far the optimal γd variables are from
γd∗ . We show the following useful claim:
7

Lemma 4. If, for all topics i, KL(βi∗ ||βit ) ≤ Rβ , and minγd ∈∆K KL(f˜d,j ||fd,j ) ≤ Rf , after
running
minimization step with respect to the γd variables, we get that ||γd∗ −γd ||1 ≤
q a KL divergence
p
1
1
1
Rf ) + .
p(
2 Rβ + 2
This lemma critically uses the existence of anchor words - namely we show ||β ∗ v||1 ≥ p||v||1 .
Intuitively, if one thinks of v as γ ∗ − γ t , ||β ∗ v||1 will be large if ||v||1 is large. Hence, if ||β ∗ − β t ||1
is not too large, whenever ||f ∗ − f t ||1 is small, so is ||γ ∗ − γ t ||1 . We will be able to maintain Rβ
and Rf small enough throughout the iterations, so that we can identify the largest topic in each of
the documents.

8

On common words

∗
We briefly remark on common words: words such that βi,j
≤ κβi∗0 ,j , ∀i, i0 , κ ≤ B. In this case, the
3
proofs above, as they are, will not work, since common words do not have any lone documents.
1
However, if 1 − κ100
fraction of the documents where topic i is dominant contains topic i with
1
proportion 1 − κ100 and furthermore, in each topic, the weight on these words is no more than
1
4
κ100 , then our proofs still work with either initialization The idea for the argument is simple: when
f∗

the dominating topic is very large, we show that fd,j
is very highly correlated with
t
d,j
documents behave like anchor documents. Namely, one can show:

∗
βi,j
t ,
βi,j

so these

Theorem 5. If we additionally have common words satisfying the properties specified above, after
O(log(1/0 ) + log N ) KL-tEM updates in Case Study 2, or any of the tEM variants in Case Study 1,
and we use the same initializations as before, we recover the topic-word matrix and topic proportions
1
to multiplicative accuracy 1 + 0 , if 1 + 0 ≥ (1−)
7.

9

Discussion and open problems

In this work we provide the first characterization of sufficient conditions when variational inference
leads to optimal parameter estimates for topic models. Our proofs also suggest possible hard cases
for variational inference, namely instances with large dynamic range compared to the proportion of
anchor words and/or correlated topic priors. It’s not hard to hand-craft such instances where support
initialization performs very badly, even with only anchor and common words. We made no effort to
explore the optimal relationship between the dynamic range and the proportion of anchor words, as
it’s not clear what are the “worst case” instances for this trade-off.
Seeded initialization, on the other hand, empirically works much better. We found that when Cl ≥
0.6, and when the proportion of anchor words is as low as 0.2, variational inference recovers the
ground truth, even on instances with fairly large dynamic range. Our current proof methods are too
weak to capture this observation. (In fact, even the largest topic is sometimes misidentified in the
initial stages, so one cannot even run tEM, only the vanilla variational inference updates.) Analyzing
the dynamics of variational inference in this regime seems like a challenging problem which would
require significantly new ideas.

References
[1] A. Agarwal, A. Anandkumar, P. Jain, and P. Netrapalli. Learning sparsely used overcomplete
dictionaries via alternating minimization. In Proceedings of The 27th Conference on Learning
Theory (COLT), 2013.
[2] A. Anandkumar, D. Hsu, A. Javanmard, and S. Kakade. Learning latent bayesian networks and
topic models under expansion constraints. In Proceedings of the 30th International Conference
on Machine Learning (ICML), 2013.
3

We stress we want to analyze whether variational inference will work or not. Handling common words
algorithmically is easy: they can be detected and ”filtered out” initially. Then we can perform the variational
inference updates over the rest of the words only. This is in fact often done in practice.
4
See supplementary material.

8

[3] A. Anandkumar, S. Kakade, D. Foster, Y. Liu, and D. Hsu. Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation. Technical report,
2012.
[4] S. Arora, R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, and M. Zhu. A practical
algorithm for topic modeling with provable guarantees. In Proceedings of the 30th International Conference on Machine Learning (ICML), 2013.
[5] S. Arora, R. Ge, R. Kanna, and A. Moitra. Computing a nonnegative matrix factorization–
provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of Computing,
pages 145–162. ACM, 2012.
[6] S. Arora, R. Ge, T. Ma, and A. Moitra. Simple, efficient, and neural algorithms for sparse
coding. In Proceedings of The 28th Conference on Learning Theory (COLT), 2015.
[7] S. Arora, R. Ge, and A. Moitra. Learning topic models – going beyond svd. In Proceedings of
the 53rd Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2012.
[8] S. Arora, R. Ge, and A. Moitra. New algorithms for learning incoherent and overcomplete
dictionaries. In Proceedings of The 27th Conference on Learning Theory (COLT), 2014.
[9] S. Balakrishnan, M.J. Wainwright, and B. Yu. Statistical guarantees for the em algorithm:
From population to sample-based analysis. arXiv preprint arXiv:1408.2156, 2014.
[10] T. Bansal, C. Bhattacharyya, and R. Kannan. A provable svd-based algorithm for learning
topics in dominant admixture corpus. In Advances in Neural Information Processing Systems
(NIPS), 2014.
[11] D. Blei and J.D. Lafferty. Topic models. Text mining: classification, clustering, and applications, 10:71, 2009.
[12] D. Blei, A. Ng, , and M. Jordan. Latent dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022, 2003.
[13] S. Dasgupta and L. Schulman. A two-round variant of em for gaussian mixtures. In Proceedings of Uncertainty in Artificial Intelligence (UAI), 2000.
[14] S. Dasgupta and L. Schulman. A probabilistic analysis of em for mixtures of separated, spherical gaussians. Journal of Machine Learning Research, 8:203–226, 2007.
[15] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society, Series B, 39:1–38, 1977.
[16] W. Ding, M.H. Rohban, P. Ishwar, and V. Saligrama. Topic discovery through data dependent
and random projections. arXiv preprint arXiv:1303.3664, 2013.
[17] W. Ding, M.H. Rohban, P. Ishwar, and V. Saligrama. Efficient distributed topic modeling
with provable guarantees. In Proceedings ot the 17th International Conference on Artificial
Intelligence and Statistics, pages 167–175, 2014.
[18] M. Hoffman, D. Blei, J. Paisley, and C. Wan. Stochastic variational inference. Journal of
Machine Learning Research, 14:1303–1347, 2013.
[19] M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. An introduction to variational methods
for graphical models. Machine learning, 37(2):183–233, 1999.
[20] A. Kumar and R. Kannan. Clustering with spectral norm and the k-means algorithm. In
Proceedings of Foundations of Computer Science (FOCS), 2010.
[21] D. Lee and S. Seung. Algorithms for non-negative matrix factorization. In Advances in Neural
Information Processing Systems (NIPS), 2000.
[22] P. Netrapalli, P. Jain, and S. Sanghavi. Phase retrieval using alternating minimization. In
Advances in Neural Information Processing Systems (NIPS), 2013.
[23] D. Sontag and D. Roy. Complexity of inference in latent dirichlet allocation. In Advances in
Neural Information Processing Systems (NIPS), 2000.
[24] R. Sundberg. Maximum likelihood from incomplete data via the em algorithm. Scandinavian
Journal of Statistics, 1:49–58, 1974.
[25] M. Telgarsky. Dirichlet draws are sparse with high probability. Manuscript, 2013.

9

