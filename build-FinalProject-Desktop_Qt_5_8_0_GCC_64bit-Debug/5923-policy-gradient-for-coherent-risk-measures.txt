Policy Gradient for Coherent Risk Measures

Yinlam Chow
Stanford University
ychow@stanford.edu

Aviv Tamar
UC Berkeley
avivt@berkeley.edu
Mohammad Ghavamzadeh
Adobe Research & INRIA
mohammad.ghavamzadeh@inria.fr

Shie Mannor
Technion
shie@ee.technion.ac.il

Abstract
Several authors have recently developed risk-sensitive policy gradient methods
that augment the standard expected cost minimization problem with a measure of
variability in cost. These studies have focused on specific risk-measures, such as
the variance or conditional value at risk (CVaR). In this work, we extend the policy gradient method to the whole class of coherent risk measures, which is widely
accepted in finance and operations research, among other fields. We consider
both static and time-consistent dynamic risk measures. For static risk measures,
our approach is in the spirit of policy gradient algorithms and combines a standard
sampling approach with convex programming. For dynamic risk measures, our approach is actor-critic style and involves explicit approximation of value function.
Most importantly, our contribution presents a unified approach to risk-sensitive
reinforcement learning that generalizes and extends previous results.

1

Introduction

Risk-sensitive optimization considers problems in which the objective involves a risk measure of
the random cost, in contrast to the typical expected cost objective. Such problems are important
when the decision-maker wishes to manage the variability of the cost, in addition to its expected
outcome, and are standard in various applications of finance and operations research. In reinforcement learning (RL) [27], risk-sensitive objectives have gained popularity as a means to regularize
the variability of the total (discounted) cost/reward in a Markov decision process (MDP).
Many risk objectives have been investigated in the literature and applied to RL, such as the celebrated Markowitz mean-variance model [16], Value-at-Risk (VaR) and Conditional Value at Risk
(CVaR) [18, 29, 21, 10, 8, 30]. The view taken in this paper is that the preference of one risk measure
over another is problem-dependent and depends on factors such as the cost distribution, sensitivity to
rare events, ease of estimation from data, and computational tractability of the optimization problem.
However, the highly influential paper of Artzner et al. [2] identified a set of natural properties that
are desirable for a risk measure to satisfy. Risk measures that satisfy these properties are termed coherent and have obtained widespread acceptance in financial applications, among others. We focus
on such coherent measures of risk in this work.
For sequential decision problems, such as MDPs, another desirable property of a risk measure is
time consistency. A time-consistent risk measure satisfies a ‚Äúdynamic programming‚Äù style property:
if a strategy is risk-optimal for an n-stage problem, then the component of the policy from the t-th
time until the end (where t < n) is also risk-optimal (see principle of optimality in [5]). The recently
proposed class of dynamic Markov coherent risk measures [24] satisfies both the coherence and time
consistency properties.
In this work, we present policy gradient algorithms for RL with a coherent risk objective. Our
approach applies to the whole class of coherent risk measures, thereby generalizing and unifying
previous approaches that have focused on individual risk measures. We consider both static coherent
1

risk of the total discounted return from an MDP and time-consistent dynamic Markov coherent
risk. Our main contribution is formulating the risk-sensitive policy-gradient under the coherent-risk
framework. More specifically, we provide:
‚Ä¢ A new formula for the gradient of static coherent risk that is convenient for approximation
using sampling.
‚Ä¢ An algorithm for the gradient of general static coherent risk that involves sampling with
convex programming and a corresponding consistency result.
‚Ä¢ A new policy gradient theorem for Markov coherent risk, relating the gradient to a suitable
value function and a corresponding actor-critic algorithm.
Several previous results are special cases of the results presented here; our approach allows to rederive them in greater generality and simplicity.
Related Work Risk-sensitive optimization in RL for specific risk functions has been studied recently by several authors. [6] studied exponential utility functions, [18], [29], [21] studied meanvariance models, [8], [30] studied CVaR in the static setting, and [20], [9] studied dynamic coherent
risk for systems with linear dynamics. Our paper presents a general method for the whole class of
coherent risk measures (both static and dynamic) and is not limited to a specific choice within that
class, nor to particular system dynamics.
Reference [19] showed that an MDP with a dynamic coherent risk objective is essentially a robust MDP. The planning for large scale MDPs was considered in [31], using an approximation of
the value function. For many problems, approximation in the policy space is more suitable (see,
e.g., [15]). Our sampling-based RL-style approach is suitable for approximations both in the policy
and value function, and scales-up to large or continuous MDPs. We do, however, make use of a
technique of [31] in a part of our method.
Optimization of coherent risk measures was thoroughly investigated by Ruszczynski and
Shapiro [25] (see also [26]) for the stochastic programming case in which the policy parameters
do not affect the distribution of the stochastic system (i.e., the MDP trajectory), but only the reward
function, and thus, this approach is not suitable for most RL problems. For the case of MDPs and
dynamic risk, [24] proposed a dynamic programming approach. This approach does not scale-up
to large MDPs, due to the ‚Äúcurse of dimensionality‚Äù. For further motivation of risk-sensitive policy
gradient methods, we refer the reader to [18, 29, 21, 8, 30].

2

Preliminaries

Consider a probability space (‚Ñ¶, F, PŒ∏ ), where ‚Ñ¶ is the set of outcomes (sample space), F is a
œÉ-algebra
over ‚Ñ¶ representing
 R
	 the set of events we are interested in, and PŒ∏ ‚àà B, where B :=
Œæ : œâ‚àà‚Ñ¶ Œæ(œâ) = 1, Œæ ‚â• 0 is the set of probability distributions, is a probability measure over F
parameterized by some tunable parameter Œ∏ ‚àà RK . In the following, we suppress the notation of Œ∏
in Œ∏-dependent quantities.
To ease the technical exposition, in this paper we restrict our attention to finite probability spaces,
i.e., ‚Ñ¶ has a finite number of elements. Our results can be extended to the Lp -normed spaces without
loss of generality, but the details are omitted for brevity.
Denote by Z the space of random variables Z : ‚Ñ¶ 7‚Üí (‚àí‚àû, ‚àû) defined over the probability space
(‚Ñ¶, F, PŒ∏ ). In this paper, a random variable Z ‚àà Z is interpreted as a cost, i.e., the smaller the
realization of Z, the better. For Z, W ‚àà Z, we denote by Z
‚â§ W the point-wise partial order,
. P
i.e., Z(œâ) ‚â§ W (œâ) for all œâ ‚àà ‚Ñ¶. We denote by EŒæ [Z] = œâ‚àà‚Ñ¶ PŒ∏ (œâ)Œæ(œâ)Z(œâ) a Œæ-weighted
expectation of Z.
An MDP is a tuple M = (X , A, C, P, Œ≥, x0 ), where X and A are the state and action spaces;
C(x) ‚àà [‚àíCmax , Cmax ] is a bounded, deterministic, and state-dependent cost; P (¬∑|x, a) is the transition probability distribution; Œ≥ is a discount factor; and x0 is the initial state.1 Actions are chosen
according to a Œ∏-parameterized stationary Markov2 policy ¬µŒ∏ (¬∑|x). We denote by x0 , a0 , . . . , xT , aT
a trajectory of length T drawn by following the policy ¬µŒ∏ in the MDP.
1

Our results may easily be extended to random costs, state-action dependent costs, and random initial states.
For Markov coherent risk, the class of optimal policies is stationary Markov [24], while this is not necessarily true for static risk. Our results can be extended to history-dependent policies or stationary Markov
2

2

2.1 Coherent Risk Measures
A risk measure is a function œÅ : Z ‚Üí R that maps an uncertain outcome Z to the extended real line
R ‚à™ {+‚àû, ‚àí‚àû},
	 e.g., the expectation E [Z] or the conditional value-at-risk (CVaR) minŒΩ‚ààR ŒΩ +
1
+
E
(Z
‚àí
ŒΩ)
. A risk measure is called coherent, if it satisfies the following conditions for all
Œ±
Z, W ‚àà Z [2]:

A1 Convexity: ‚àÄŒª ‚àà [0, 1], œÅ ŒªZ + (1 ‚àí Œª)W ‚â§ ŒªœÅ(Z) + (1 ‚àí Œª)œÅ(W );
A2 Monotonicity: if Z ‚â§ W , then œÅ(Z) ‚â§ œÅ(W );
A3 Translation invariance: ‚àÄa ‚àà R, œÅ(Z + a) = œÅ(Z) + a;
A4 Positive homogeneity: if Œª ‚â• 0, then œÅ(ŒªZ) = ŒªœÅ(Z).
Intuitively, these condition ensure the ‚Äúrationality‚Äù of single-period risk assessments: A1 ensures
that diversifying an investment will reduce its risk; A2 guarantees that an asset with a higher cost
for every possible scenario is indeed riskier; A3, also known as ‚Äòcash invariance‚Äô, means that the
deterministic part of an investment portfolio does not contribute to its risk; the intuition behind A4
is that doubling a position in an asset doubles its risk. We further refer the reader to [2] for a more
detailed motivation of coherent risk.
The following representation theorem [26] shows an important property of coherent risk measures
that is fundamental to our gradient-based approach.
Theorem 2.1. A risk measure œÅ : Z ‚Üí R is coherent if and only if there exists a convex bounded
and closed set U ‚äÇ B such that3
œÅ(Z) =

max

Œæ : ŒæPŒ∏ ‚ààU (PŒ∏ )

EŒæ [Z].

(1)

The result essentially states that any coherent risk measure is an expectation w.r.t. a worst-case
density function ŒæPŒ∏ , i.e., a re-weighting of PŒ∏ by Œæ, chosen adversarially from a suitable set of test
density functions U(PŒ∏ ), referred to as risk envelope. Moreover, a coherent risk measure is uniquely
represented by its risk envelope. In the sequel, we shall interchangeably refer to coherent risk
measures either by their explicit functional representation, or by their corresponding risk-envelope.
In this paper, we assume that the risk envelope U(PŒ∏ ) is given in a canonical convex programming
formulation, and satisfies the following conditions.
Assumption 2.2 (The General Form of Risk Envelope). For each given policy parameter Œ∏ ‚àà RK ,
the risk envelope U of a coherent risk measure can be written as


X
U(PŒ∏ ) = ŒæPŒ∏ : ge (Œæ, PŒ∏ ) = 0, ‚àÄe ‚àà E, fi (Œæ, PŒ∏ ) ‚â§ 0, ‚àÄi ‚àà I,
Œæ(œâ)PŒ∏ (œâ) = 1, Œæ(œâ) ‚â• 0 , (2)
œâ‚àà‚Ñ¶

where each constraint ge (Œæ, PŒ∏ ) is an affine function in Œæ, each constraint fi (Œæ, PŒ∏ ) is a convex
function in Œæ, and there exists a strictly feasible point Œæ. E and I here denote the sets of equality
and inequality constraints, respectively. Furthermore, for any given Œæ ‚àà B, fi (Œæ, p) and ge (Œæ, p) are
twice differentiable in p, and there exists a M > 0 such that







 dfi (Œæ, p) 
 , max  dge (Œæ, p)  ‚â§ M, ‚àÄœâ ‚àà ‚Ñ¶.
max max 


i‚ààI
e‚ààE
dp(œâ)
dp(œâ) 

Assumption 2.2 implies that the risk envelope U(PŒ∏ ) is known in an explicit form. From Theorem
6.6 of [26], in the case of a finite probability space, œÅ is a coherent risk if and only if U(PŒ∏ ) is a
convex and compact set. This justifies the affine assumption of ge and the convex assumption of fi .
Moreover, the additional assumption on the smoothness of the constraints holds for many popular
coherent risk measures, such as the CVaR, the mean-semi-deviation, and spectral risk measures [1].
2.2 Dynamic Risk Measures
The risk measures defined above do not take into account any temporal structure that the random
variable might have, such as when it is associated with the return of a trajectory in the case of
MDPs. In this sense, such risk measures are called static. Dynamic risk measures, on the other hand,
policies on a state space augmented with accumulated cost. The latter has shown to be sufficient for optimizing
the CVaR risk [4].
3
When we study risk in MDPs, the risk envelope U(PŒ∏ ) in Eq. 1 also depends on the state x.

3

explicitly take into account the temporal nature of the stochastic outcome. A primary motivation for
considering such measures is the issue of time consistency, usually defined as follows [24]: if a
certain outcome is considered less risky in all states of the world at stage t + 1, then it should also
be considered less risky at stage t. Example 2.1 in [13] shows the importance of time consistency
in the evaluation of risk in a dynamic setting. It illustrates that for multi-period decision-making,
optimizing a static measure can lead to ‚Äútime-inconsistent‚Äù behavior. Similar paradoxical results
could be obtained with other risk metrics; we refer the readers to [24] and [13] for further insights.
Markov Coherent Risk Measures. Markov risk measures were introduced in [24] and constitute
a useful class of dynamic time-consistent risk measures that are important to our study of risk in
MDPs. For a T -length horizon and MDP M, the Markov coherent risk measure œÅT (M) is
!


,
œÅT (M) = C(x0 ) + Œ≥œÅ C(x1 ) + . . . + Œ≥œÅ C(xT ‚àí1 ) + Œ≥œÅ C(xT )

(3)

where œÅ is a static coherent risk measure that satisfies Assumption 2.2 and x0 , . . . , xT is a trajectory
drawn from the MDP M under policy ¬µŒ∏ . It is important to note that P
in (3), each static coherent risk
œÅ at state x ‚àà X is induced by the transition probability PŒ∏ (¬∑|x) = a‚ààA P (x0 |x, a)¬µŒ∏ (a|x). We
.
also define œÅ‚àû (M) = limT ‚Üí‚àû œÅT (M), which is well-defined since Œ≥ < 1 and the cost is bounded.
We further assume that œÅ in (3) is a Markov risk measure, i.e., the evaluation of each static coherent
risk measure œÅ is not allowed to depend on the whole past.

3

Problem Formulation

In this paper, we are interested in solving two risk-sensitive optimization problems. Given a random
variable Z and a static coherent risk measure œÅ as defined in Section 2, the static risk problem (SRP)
is given by
min œÅ(Z).
(4)
Œ∏

For example, in an RL setting, Z may correspond to the cumulative discounted cost Z = C(x0 ) +
Œ≥C(x1 ) + ¬∑ ¬∑ ¬∑ + Œ≥ T C(xT ) of a trajectory induced by an MDP with a policy parameterized by Œ∏.
For an MDP M and a dynamic Markov coherent risk measure œÅT as defined by Eq. 3, the dynamic
risk problem (DRP) is given by
min œÅ‚àû (M).
(5)
Œ∏

Except for very limited cases, there is no reason to hope that neither the SRP in (4) nor the DRP
in (5) should be tractable problems, since the dependence of the risk measure on Œ∏ may be complex
and non-convex. In this work, we aim towards a more modest goal and search for a locally optimal
Œ∏. Thus, the main problem that we are trying to solve in this paper is how to calculate the gradients
of the SRP‚Äôs and DRP‚Äôs objective functions
‚àáŒ∏ œÅ(Z)
and
‚àáŒ∏ œÅ‚àû (M).
We are interested in non-trivial cases in which the gradients cannot be calculated analytically. In
the static case, this would correspond to a non-trivial dependence of Z on Œ∏. For dynamic risk, we
also consider cases where the state space is too large for a tractable computation. Our approach for
dealing with such difficult cases is through sampling. We assume that in the static case, we may
obtain i.i.d. samples of the random variable Z. For the dynamic case, we assume that for each state
and action (x, a) of the MDP, we may obtain i.i.d. samples of the next state x0 ‚àº P (¬∑|x, a). We
show that sampling may indeed be used in both cases to devise suitable estimators for the gradients.
To finally solve the SRP and DRP problems, a gradient estimate may be plugged into a standard
stochastic gradient descent (SGD) algorithm for learning a locally optimal solution to (4) and (5).
From the structure of the dynamic risk in Eq. 3, one may think that a gradient estimator for œÅ(Z) may
help us to estimate the gradient ‚àáŒ∏ œÅ‚àû (M). Indeed, we follow this idea and begin with estimating
the gradient in the static risk case.

4

Gradient Formula for Static Risk

In this section, we consider a static coherent risk measure œÅ(Z) and propose sampling-based estimators for ‚àáŒ∏ œÅ(Z). We make the following assumption on the policy parametrization, which is
standard in the policy gradient literature [15].
Assumption 4.1. The likelihood ratio ‚àáŒ∏ log P (œâ) is well-defined and bounded for all œâ ‚àà ‚Ñ¶.
4

Moreover, our approach implicitly assumes that given some œâ ‚àà ‚Ñ¶, ‚àáŒ∏ log P (œâ) may be easily
calculated. This is also a standard requirement for policy gradient algorithms [15] and is satisfied
in various applications such as queueing systems, inventory management, and financial engineering
(see, e.g., the survey by Fu [11]).
Using Theorem 2.1 and Assumption 2.2, for each Œ∏, we have that œÅ(Z) is the solution to the convex optimization problem (1) (for that value of Œ∏). The Lagrangian function of (1), denoted by
LŒ∏ (Œæ, ŒªP , ŒªE , ŒªI ), may be written as
!
P

E

I

LŒ∏ (Œæ, Œª , Œª , Œª ) =

X

Œæ(œâ)PŒ∏ (œâ)Z(œâ)‚àíŒª

P

œâ‚àà‚Ñ¶

X

Œæ(œâ)PŒ∏ (œâ)‚àí1 ‚àí

œâ‚àà‚Ñ¶

X

X I
ŒªE (e)ge (Œæ,PŒ∏ )‚àí
Œª (i)fi (Œæ,PŒ∏ ).

e‚ààE

i‚ààI

(6)

The convexity of (1) and its strict feasibility due to Assumption 2.2 implies that LŒ∏ (Œæ, ŒªP , ŒªE , ŒªI )
has a non-empty set of saddle points S. The next theorem presents a formula for the gradient
‚àáŒ∏ œÅ(Z). As we shall subsequently show, this formula is particularly convenient for devising sampling based estimators for ‚àáŒ∏ œÅ(Z).
‚àó,E
‚àó,I
Theorem 4.2. Let Assumptions 2.2 and 4.1 hold. For any saddle point (ŒæŒ∏‚àó , Œª‚àó,P
Œ∏ , ŒªŒ∏ , ŒªŒ∏ ) ‚àà S
of (6), we have
h
i X
X ‚àó,I
‚àó
‚àáŒ∏ œÅ(Z) = EŒæŒ∏‚àó ‚àáŒ∏ log P (œâ)(Z ‚àí Œª‚àó,P
Œª‚àó,E
ŒªŒ∏ (i)‚àáŒ∏ fi (ŒæŒ∏‚àó ; PŒ∏ ).
Œ∏ ) ‚àí
Œ∏ (e)‚àáŒ∏ ge (ŒæŒ∏ ; PŒ∏ ) ‚àí
e‚ààE

i‚ààI

The proof of this theorem, given in the supplementary material, involves an application of the Envelope theorem [17] and a standard ‚Äòlikelihood-ratio‚Äô trick. We now demonstrate the utility of Theorem
4.2 with several examples in which we show that it generalizes previously known results, and also
enables deriving new useful gradient formulas.
4.1

Example 1: CVaR

The CVaR at level Œ± ‚àà [0, 1] of a random variable Z, denoted by œÅCVaR (Z; Œ±), is a very popular
coherent risk measure [23], defined as

	
.
œÅCVaR (Z; Œ±) = inf t + Œ±‚àí1 E [(Z ‚àí t)+ ] .
t‚ààR

When Z is continuous, œÅCVaR (Z; Œ±) is well-known to be the mean of the Œ±-tail distribution of Z,
E [ Z| Z > qŒ± ], where qŒ± is a (1 ‚àí Œ±)-quantile of Z. Thus, selecting a small Œ± makes CVaR particularly sensitive to rare, but very high costs.

The risk envelope for CVaR 	is known to be [26] U
=
ŒæPŒ∏
: Œæ(œâ) ‚àà
P
[0, Œ±‚àí1 ],
Œæ(œâ)P
(œâ)
=
1
.
Furthermore,
[26]
show
that
the
saddle
points
of (6) satisfy
Œ∏
œâ‚àà‚Ñ¶
‚àó,P
‚àó,P
‚àó
ŒæŒ∏‚àó (œâ) = Œ±‚àí1 when Z(œâ) > Œª‚àó,P
,
and
Œæ
(œâ)
=
0
when
Z(œâ)
<
Œª
,
where
Œª
is
any (1 ‚àí Œ±)Œ∏
Œ∏
Œ∏
Œ∏
quantile of Z. Plugging this result into Theorem 4.2, we can easily show that
‚àáŒ∏ œÅCVaR (Z; Œ±) = E [ ‚àáŒ∏ log P (œâ)(Z ‚àí qŒ± )| Z(œâ) > qŒ± ] .
This formula was recently proved in [30] for the case of continuous distributions by an explicit
calculation of the conditional expectation, and under several additional smoothness assumptions.
Here we show that it holds regardless of these assumptions and in the discrete case as well. Our
proof is also considerably simpler.
4.2

Example 2: Mean-Semideviation


1/2
.
The semi-deviation of a random variable Z is defined as SD[Z] = E (Z ‚àí E [Z])2+
. The
semi-deviation captures the variation of the cost only above its mean, and is an appealing alternative
to the standard deviation, which does not distinguish between the variability of upside and downside
.
deviations. For some Œ± ‚àà [0, 1], the mean-semideviation risk measure is defined as œÅMSD (Z; Œ±) =
E [Z] + Œ±SD[Z], and is a coherent risk measure [26]. We have the following result:
Proposition 4.3. Under Assumption 4.1, with ‚àáŒ∏ E [Z] = E [‚àáŒ∏ log P (œâ)Z], we have
Œ±E [(Z ‚àíE [Z])+ (‚àáŒ∏ log P (œâ)(Z ‚àíE [Z])‚àí‚àáŒ∏ E [Z])]
‚àáŒ∏ œÅMSD (Z; Œ±) = ‚àáŒ∏ E [Z] +
.
SD(Z)
This proposition can be used to devise a sampling based estimator for ‚àáŒ∏ œÅMSD (Z; Œ±) by replacing
all the expectations with sample averages. The algorithm along with the proof of the proposition are
in the supplementary material. In Section 6 we provide a numerical illustration of optimization with
a mean-semideviation objective.
5

4.3

General Gradient Estimation Algorithm

In the two previous examples, we obtained a gradient formula by analytically calculating the Lagrangian saddle point (6) and plugging it into the formula of Theorem 4.2. We now consider a
general coherent risk œÅ(Z) for which, in contrast to the CVaR and mean-semideviation cases, the
Lagrangian saddle-point is not known analytically. We only assume that we know the structure of the
risk-envelope as given by (2). We show that in this case, ‚àáŒ∏ œÅ(Z) may be estimated using a sample
average approximation (SAA; [26]) of the formula in Theorem 4.2.
.
Assume that we are given N i.i.d. samples œâi ‚àº PŒ∏ , i = 1, . . . , N , and let PŒ∏;N (œâ) =
P
N
1
i=1 I {œâi = œâ} denote the corresponding empirical distribution. Also, let the sample risk enN
velope U(PŒ∏;N ) be defined according to Eq. 2 with PŒ∏ replaced by PŒ∏;N . Consider the following
SAA version of the optimization in Eq. 1:
X
œÅN (Z) =
max
PŒ∏;N (œâi )Œæ(œâi )Z(œâi ).
(7)
Œæ:ŒæPŒ∏;N ‚ààU (PŒ∏;N )

i‚àà1,...,N

Note that (7) defines a convex optimization problem with O(N ) variables and constraints. In
the following, we assume that a solution to (7) may be computed efficiently using standard con‚àó
denote a solution to (7) and
vex programming tools such as interior point methods [7]. Let ŒæŒ∏;N
‚àó,P
‚àó,E
‚àó,I
ŒªŒ∏;N , ŒªŒ∏;N , ŒªŒ∏;N denote the corresponding KKT multipliers, which can be obtained from the convex programming algorithm [7]. We propose the following estimator for the gradient-based on
Theorem 4.2:
N
X
‚àó
‚àáŒ∏;N œÅ(Z) =
PŒ∏;N (œâi )ŒæŒ∏;N
(œâi )‚àáŒ∏ log P (œâi )(Z(œâi ) ‚àí Œª‚àó,P
(8)
Œ∏;N )
i=1

‚àí

X

‚àó
Œª‚àó,E
Œ∏;N (e)‚àáŒ∏ ge (ŒæŒ∏;N ; PŒ∏;N ) ‚àí

e‚ààE

X

‚àó
Œª‚àó,I
Œ∏;N (i)‚àáŒ∏ fi (ŒæŒ∏;N ; PŒ∏;N ).

i‚ààI

Thus, our gradient estimation algorithm is a two-step procedure involving both sampling and convex
programming. In the following, we show that under some conditions on the set U(PŒ∏ ), ‚àáŒ∏;N œÅ(Z)
is a consistent estimator of ‚àáŒ∏ œÅ(Z). The proof has been reported in the supplementary material.
Proposition 4.4. Let Assumptions 2.2 and 4.1 hold. Suppose there exists a compact set C = CŒæ √óCŒª
such that: (I) The set of Lagrangian saddle points S ‚äÇ C is non-empty and bounded. (II) The
functions fe (Œæ, PŒ∏ ) for all e ‚àà E and fi (Œæ, PŒ∏ ) for all i ‚àà I are finite-valued and continuous (in Œæ)
on CŒæ . (III) For N large enough, the set SN is non-empty and SN ‚äÇ C w.p. 1. Further assume that:
(IV) If ŒæN PŒ∏;N ‚àà U(PŒ∏;N ) and ŒæN converges w.p. 1 to a point Œæ, then ŒæPŒ∏ ‚àà U(PŒ∏ ). We then have
that limN ‚Üí‚àû œÅN (Z) = œÅ(Z) and limN ‚Üí‚àû ‚àáŒ∏;N œÅ(Z) = ‚àáŒ∏ œÅ(Z) w.p. 1.
The set of assumptions for Proposition 4.4 is large, but rather mild. Note that (I) is implied by
the Slater condition of Assumption 2.2. For satisfying (III), we need that the risk be well-defined
for every empirical distribution, which is a natural requirement. Since PŒ∏;N always converges to PŒ∏
uniformly on ‚Ñ¶, (IV) essentially requires smoothness of the constraints. We remark that in particular,
constraints (I) to (IV) are satisfied for the popular CVaR, mean-semideviation, and spectral risk.
It is interesting to compare the performance of the SAA estimator (8) with the analytical-solution
based estimator, as in Sections 4.1 and 4.2. In the supplementary material, we report an empirical
comparison between the two approaches for the case of CVaR risk, which showed that the two
approaches performed very similarly. This is well-expected, since in general, both SAA and‚àöstandard
likelihood-ratio based estimators obey a law-of-large-numbers variance bound of order 1/ N [26].
To summarize this section, we have seen that by exploiting the special structure of coherent risk
measures in Theorem 2.1 and by the envelope-theorem style result of Theorem 4.2, we are able to
derive sampling-based, likelihood-ratio style algorithms for estimating the policy gradient ‚àáŒ∏ œÅ(Z)
of coherent static risk measures. The gradient estimation algorithms developed here for static risk
measures will be used as a sub-routine in our subsequent treatment of dynamic risk measures.

5

Gradient Formula for Dynamic Risk

In this section, we derive a new formula for the gradient of the Markov coherent dynamic risk measure, ‚àáŒ∏ œÅ‚àû (M). Our approach is based on combining the static gradient formula of Theorem 4.2,
with a dynamic-programming decomposition of œÅ‚àû (M).
6

The risk-sensitive value-function for an MDP M under the policy Œ∏ is defined as VŒ∏ (x) =
œÅ‚àû (M|x0 = x), where with a slight abuse of notation, œÅ‚àû (M|x0 = x) denotes the Markovcoherent dynamic risk in (3) when the initial state x0 is x. It is shown in [24] that due to the structure
of the Markov dynamic risk œÅ‚àû (M), the value function is the unique solution to the risk-sensitive
Bellman equation
VŒ∏ (x) = C(x) + Œ≥
max
EŒæ [VŒ∏ (x0 )],
(9)
ŒæPŒ∏ (¬∑|x)‚ààU (x,PŒ∏ (¬∑|x))

where the expectation is taken over the next state transition. Note that by definition, we have
œÅ‚àû (M) = VŒ∏ (x0 ), and thus, ‚àáŒ∏ œÅ‚àû (M) = ‚àáŒ∏ VŒ∏ (x0 ).
We now develop a formula for ‚àáŒ∏ VŒ∏ (x); this formula extends the well-known ‚Äúpolicy gradient
theorem‚Äù [28, 14], developed for the expected return, to Markov-coherent dynamic risk measures.
We make a standard assumption, analogous to Assumption 4.1 of the static case.
Assumption 5.1. The likelihood ratio ‚àáŒ∏ log ¬µŒ∏ (a|x) is well-defined and bounded for all x ‚àà X
and a ‚àà A.
‚àó,E
‚àó,I
‚àó
For each state x ‚àà X , let (ŒæŒ∏,x
, Œª‚àó,P
Œ∏,x , ŒªŒ∏,x , ŒªŒ∏,x ) denote a saddle point of (6), corresponding to the
state x, with PŒ∏ (¬∑|x) replacing PŒ∏ in (6) and VŒ∏ replacing Z. The next theorem presents a formula
for ‚àáŒ∏ VŒ∏ (x); the proof is in the supplementary material.
Theorem 5.2. Under Assumptions"2.2 and 5.1, we have

#
‚àû

X

‚àáVŒ∏ (x) = EŒæŒ∏‚àó
Œ≥ t ‚àáŒ∏ log ¬µŒ∏ (at |xt )hŒ∏ (xt , at ) x0 = x ,

t=0

where EŒæŒ∏‚àó [¬∑] denotes the expectation w.r.t. trajectories generated by the Markov chain with transition
‚àó
probabilities PŒ∏ (¬∑|x)ŒæŒ∏,x
(¬∑), and the stage-wise cost function hŒ∏ (x, a) is defined as
"
hŒ∏ (x, a) = C(x)+

X

0

P (x

‚àó
|x, a)ŒæŒ∏,x
(x0 )

0

Œ≥VŒ∏ (x

)‚àíŒª‚àó,P
Œ∏,x ‚àí

x0 ‚ààX

#
‚àó
‚àó
X ‚àó,I dfi (ŒæŒ∏,x
, p) X ‚àó,E
dge (ŒæŒ∏,x
, p)
ŒªŒ∏,x (i)
‚àí
ŒªŒ∏,x (e)
.
dp(x0 )
dp(x0 )
i‚ààI
e‚ààE

Theorem 5.2 may be used to develop an actor-critic style [28, 14] sampling-based algorithm for
solving the DRP problem (5), composed of two interleaved procedures:
Critic: For a given policy Œ∏, calculate the risk-sensitive value function VŒ∏ , and
Actor: Using the critic‚Äôs VŒ∏ and Theorem 5.2, estimate ‚àáŒ∏ œÅ‚àû (M) and update Œ∏.
Space limitation restricts us from specifying the full details of our actor-critic algorithm and its
analysis. In the following, we highlight only the key ideas and results. For the full details, we refer
the reader to the full paper version, provided in the supplementary material.
For the critic, the main challenge is calculating the value function when the state space X is large
and dynamic programming cannot be applied due to the ‚Äòcurse of dimensionality‚Äô. To overcome
this, we exploit the fact that VŒ∏ is equivalent to the value function in a robust MDP [19] and modify
a recent algorithm in [31] to estimate it using function approximation.
For the actor, the main challenge is that in order to estimate the gradient using Thm. 5.2, we need to
sample from an MDP with ŒæŒ∏‚àó -weighted transitions. Also, hŒ∏ (x, a) involves an expectation for each
s and a. Therefore, we propose a two-phase sampling procedure to estimate ‚àáVŒ∏ in which we first
use the critic‚Äôs estimate of VŒ∏ to derive ŒæŒ∏‚àó , and sample a trajectory from an MDP with ŒæŒ∏‚àó -weighted
transitions. For each state in the trajectory, we then sample several next states to estimate hŒ∏ (x, a).
The convergence analysis of the actor-critic algorithm and the gradient error incurred from function
approximation of VŒ∏ are reported in the supplementary material. We remark that our actor-critic
algorithm requires a simulator for sampling multiple state-transitions from each state. Extending
our approach to work with a single trajectory roll-out is an interesting direction for future research.

6

Numerical Illustration

In this section, we illustrate our approach with a numerical example. The purpose of this illustration
is to emphasize the importance of flexibility in designing risk criteria for selecting an appropriate
risk-measure ‚Äì such that suits both the user‚Äôs risk preference and the problem-specific properties.
We consider a trading agent that can invest in one of three assets (see Figure 1 for their distributions).
The returns of the first two assets, A1 and A2, are normally distributed: A1 ‚àº N (1, 1) and A2 ‚àº
7

Figure 1: Numerical illustration - selection between 3 assets. A: Probability density of asset return.
B,C,D: Bar plots of the probability of selecting each asset vs. training iterations, for policies œÄ1 , œÄ2 ,
and œÄ3 , respectively. At each iteration, 10,000 samples were used for gradient estimation.
Œ±
‚àÄz > 1, with Œ± =
N (4, 6). The return of the third asset A3 has a Pareto distribution: f (z) = zŒ±+1
1.5. The mean of the return from A3 is 3 and its variance is infinite; such heavy-tailed distributions
are widely used in financial modeling [22]. The agent selects an action randomly, with probability
P (Ai ) ‚àù exp(Œ∏i ), where Œ∏ ‚àà R3 is the policy parameter. We trained three different policies œÄ1 , œÄ2 ,
and œÄ3 . Policy œÄ1 is risk-neutral, i.e., maxŒ∏ E [Z], and it was trained using standard policy gradient
[15]. Policy œÄ2 is risk-averse and had a mean-semideviation objective maxŒ∏ E [Z] ‚àí SD[Z], and
was trained using the algorithm in Section 4. Policy œÄ3 is also
p risk-averse, with a mean-standarddeviation objective, as proposed in [29, 21], maxŒ∏ E [Z] ‚àí Var[Z], and was trained using the
algorithm of [29]. For each of these policies, Figure 1 shows the probability of selecting each asset
vs. training iterations. Although A2 has the highest mean return, the risk-averse policy œÄ2 chooses
A3, since it has a lower downside, as expected. However, because of the heavy upper-tail of A3,
policy œÄ3 opted to choose A1 instead. This is counter-intuitive as a rational investor should not avert
high returns. In fact, in this case A3 stochastically dominates A1 [12].

7

Conclusion

We presented algorithms for estimating the gradient of both static and dynamic coherent risk measures using two new policy gradient style formulas that combine sampling with convex programming. Thereby, our approach extends risk-sensitive RL to the whole class of coherent risk measures,
and generalizes several recent studies that focused on specific risk measures.
On the technical side, an important future direction is to improve the convergence rate of gradient
estimates using importance sampling methods. This is especially important for risk criteria that are
sensitive to rare events, such as the CVaR [3].
From a more conceptual point of view, the coherent-risk framework explored in this work provides
the decision maker with flexibility in designing risk preference. As our numerical example shows,
such flexibility is important for selecting appropriate problem-specific risk measures for managing
the cost variability. However, we believe that our approach has much more potential than that.
In almost every real-world application, uncertainty emanates from stochastic dynamics, but also,
and perhaps more importantly, from modeling errors (model uncertainty). A prudent policy should
protect against both types of uncertainties. The representation duality of coherent-risk (Theorem
2.1), naturally relates the risk to model uncertainty. In [19], a similar connection was made between
model-uncertainty in MDPs and dynamic Markov coherent risk. We believe that by carefully shaping the risk-criterion, the decision maker may be able to take uncertainty into account in a broad
sense. Designing a principled procedure for such risk-shaping is not trivial, and is beyond the scope
of this paper. However, we believe that there is much potential to risk shaping as it may be the key
for handling model misspecification in dynamic decision making.
Acknowledgments
The research leading to these results has received funding from the European Research Council
under the European Unions Seventh Framework Program (FP7/2007-2013) / ERC Grant Agreement
n. 306638. Yinlam Chow is partially supported by Croucher Foundation Doctoral Scholarship.

8

References
[1] C. Acerbi. Spectral measures of risk: a coherent representation of subjective risk aversion. Journal of
Banking & Finance, 26(7):1505‚Äì1518, 2002.
[2] P. Artzner, F. Delbaen, J. Eber, and D. Heath. Coherent measures of risk. Mathematical finance, 9(3):203‚Äì
228, 1999.
[3] O. Bardou, N. Frikha, and G. PageÃÄs. Computing VaR and CVaR using stochastic approximation and
adaptive unconstrained importance sampling. Monte Carlo Methods and Applications, 15(3):173‚Äì210,
2009.
[4] N. BaÃàuerle and J. Ott. Markov decision processes with average-value-at-risk criteria. Mathematical
Methods of Operations Research, 74(3):361‚Äì379, 2011.
[5] D. Bertsekas. Dynamic programming and optimal control. Athena Scientific, 4th edition, 2012.
[6] V. Borkar. A sensitivity formula for risk-sensitive cost and the actor‚Äìcritic algorithm. Systems & Control
Letters, 44(5):339‚Äì346, 2001.
[7] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2009.
[8] Y. Chow and M. Ghavamzadeh. Algorithms for CVaR optimization in MDPs. In NIPS 27, 2014.
[9] Y. Chow and M. Pavone. A unifying framework for time-consistent, risk-averse model predictive control:
theory and algorithms. In American Control Conference, 2014.
[10] E. Delage and S. Mannor. Percentile optimization for Markov decision processes with parameter uncertainty. Operations Research, 58(1):203213, 2010.
[11] M. Fu. Gradient estimation. In Simulation, volume 13 of Handbooks in Operations Research and Management Science, pages 575 ‚Äì 616. Elsevier, 2006.
[12] J. Hadar and W. R. Russell. Rules for ordering uncertain prospects. The American Economic Review,
pages 25‚Äì34, 1969.
[13] D. Iancu, M. Petrik, and D. Subramanian.
arXiv:1106.6102, 2011.

Tight approximations of dynamic risk measures.

[14] V. Konda and J. Tsitsiklis. Actor-critic algorithms. In NIPS, 2000.
[15] P. Marbach and J. Tsitsiklis. Simulation-based optimization of Markov reward processes. IEEE Transactions on Automatic Control, 46(2):191‚Äì209, 1998.
[16] H. Markowitz. Portfolio selection: Efficient diversification of investment. John Wiley and Sons, 1959.
[17] P. Milgrom and I. Segal. Envelope theorems for arbitrary choice sets. Econometrica, 70(2):583‚Äì601,
2002.
[18] J. Moody and M. Saffell. Learning to trade via direct reinforcement. Neural Networks, IEEE Transactions
on, 12(4):875‚Äì889, 2001.
[19] T. Osogami. Robustness and risk-sensitivity in Markov decision processes. In NIPS, 2012.
[20] M. Petrik and D. Subramanian. An approximate solution method for large risk-averse Markov decision
processes. In UAI, 2012.
[21] L. Prashanth and M. Ghavamzadeh. Actor-critic algorithms for risk-sensitive MDPs. In NIPS 26, 2013.
[22] S. Rachev and S. Mittnik. Stable Paretian models in finance. John Willey & Sons, New York, 2000.
[23] R. Rockafellar and S. Uryasev. Optimization of conditional value-at-risk. Journal of risk, 2:21‚Äì42, 2000.
[24] A. RuszczynÃÅski. Risk-averse dynamic programming for Markov decision processes. Mathematical Programming, 125(2):235‚Äì261, 2010.
[25] A. RuszczynÃÅski and A. Shapiro. Optimization of convex risk functions. Math. OR, 31(3):433‚Äì452, 2006.
[26] A. Shapiro, D. Dentcheva, and A. RuszczynÃÅski. Lectures on stochastic programming, chapter 6, pages
253‚Äì332. SIAM, 2009.
[27] R. Sutton and A. Barto. Reinforcement learning: An introduction. Cambridge Univ Press, 1998.
[28] R. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning
with function approximation. In NIPS 13, 2000.
[29] A. Tamar, D. Di Castro, and S. Mannor. Policy gradients with variance related risk criteria. In International Conference on Machine Learning, 2012.
[30] A. Tamar, Y. Glassner, and S. Mannor. Optimizing the CVaR via sampling. In AAAI, 2015.
[31] A. Tamar, S. Mannor, and H. Xu. Scaling up robust MDPs using function approximation. In International
Conference on Machine Learning, 2014.

9

