Mixed Robust/Average Submodular Partitioning:
Fast Algorithms, Guarantees, and Applications

Kai Wei1

Rishabh Iyer1

Shengjie Wang2

Wenruo Bai1

Jeff Bilmes1

1

Department of Electrical Engineering, University of Washington
2
Department of Computer Science, University of Washington
{kaiwei, rkiyer, wangsj, wrbai, bilmes}@u.washington.edu

Abstract
We investigate two novel mixed robust/average-case submodular data partitioning
problems that we collectively call Submodular Partitioning. These problems generalize purely robust instances of the problem, namely max-min submodular fair
allocation (SFA) [12] and min-max submodular load balancing (SLB) [25], and
also average-case instances, that is the submodular welfare problem (SWP) [26]
and submodular multiway partition (SMP) [5]. While the robust versions have been
studied in the theory community [11, 12, 16, 25, 26], existing work has focused
on tight approximation guarantees, and the resultant algorithms are not generally
scalable to large real-world applications. This is in contrast to the average case,
where most of the algorithms are scalable. In the present paper, we bridge this gap,
by proposing several new algorithms (including greedy, majorization-minimization,
minorization-maximization, and relaxation algorithms) that not only scale to large
datasets but that also achieve theoretical approximation guarantees comparable
to the state-of-the-art. We moreover provide new scalable algorithms that apply
to additive combinations of the robust and average-case objectives. We show that
these problems have many applications in machine learning (ML), including data
partitioning and load balancing for distributed ML, data clustering, and image segmentation. We empirically demonstrate the efficacy of our algorithms on real-world
problems involving data partitioning for distributed optimization (of convex and
deep neural network objectives), and also purely unsupervised image segmentation.

1

Introduction

The problem of data partitioning is of great importance to many machine learning (ML) and data
science applications as is evidenced by the wealth of clustering procedures that have been and continue
to be developed and used. Most data partitioning problems are based on expected, or average-case,
utility objectives where the goal is to optimize a sum of cluster costs, and this includes the ubiquitous
k-means procedure [1]. Other algorithms are based on robust objective functions [10], where the
goal is to optimize the worst-case cluster cost. Such robust algorithms are particularly important
in mission critical applications, such as parallel and distributed computing, where one single poor
partition block can significantly slow down an entire parallel machine (as all compute nodes might
need to spin while waiting for a slow node to complete a round of computation). Taking a weighted
combination of both robust and average case objective functions allows one to balance between
optimizing worst-case and overall performance. We are unaware, however, of any previous work that
allows for a mixing between worst- and average-case objectives in the context of data partitioning.
This paper studies two new mixed robust/average-case partitioning problems of the following form:
1

m
m
h
i
h
i
Î» X
Î» X
Prob. 1: max Î»Ì„ min fi (AÏ€i ) +
fj (AÏ€j ) , Prob. 2: min Î»Ì„ max fi (AÏ€i ) +
fj (AÏ€j ) ,
i
i
Ï€âˆˆÎ 
Ï€âˆˆÎ 
m j=1
m j=1

where 0 â‰¤ Î» â‰¤ 1, Î»Ì„ , 1 âˆ’ Î», the set of sets Ï€ = (AÏ€1 , AÏ€2 , Â· Â· Â· , AÏ€m ) is a partition of a finite
set V (i.e, âˆªi AÏ€i = V and âˆ€i 6= j, AÏ€i âˆ© AÏ€j = âˆ…), and Î  refers to the set of all partitions of
V into m blocks. The parameter Î» controls the objective: Î» = 1 is the average case, Î» = 0 is
the robust case, and 0 < Î» < 1 is a mixed case. In general, Problems 1 and 2 are hopelessly
intractable, even to approximate, but we assume that the f1 , f2 , Â· Â· Â· , fm are all monotone nondecreasing (i.e., fi (S) â‰¤ fi (T ) whenever S âŠ† T ), normalized (fi (âˆ…) = 0), and submodular [9] (i.e.,
âˆ€S, T âŠ† V , fi (S) + fi (T ) â‰¥ fi (S âˆª T ) + fi (S âˆ© T )). These assumptions allow us to develop fast,
simple, and scalable algorithms that have approximation guarantees, as is done in this paper. These
assumptions, moreover, allow us to retain the naturalness and applicability of Problems 1 and 2 to
a wide variety of practical problems. Submodularity is a natural property in many real-world ML
applications [20, 15, 18, 27]. When minimizing, submodularity naturally model notions of interacting
costs and complexity, while when maximizing it readily models notions of diversity, summarization
quality, and information. Hence, Problem 1 asks for a partition whose blocks each (and that
collectively) are a good, say, summary of the whole. Problem 2 on the other hand, asks for a partition
whose blocks each (and that collectively) are internally homogeneous (as is typical in clustering).
Taken together, we call Problems 1 and 2 Submodular Partitioning. We further categorize these
problems depending on if the fi â€™s are identical to each other (homogeneous) or not (heterogeneous).1
The heterogeneous case clearly generalizes the homogeneous setting, but as we will see, the additional
homogeneous structure can be exploited to provide more efficient and/or tighter algorithms.
Problem 1 (Max-(Min+Avg))
Approximation factor
Î» = 0, B IN S RCH [16]
1/(2m âˆ’ 1)
Î» = 0, M ATCHING [12]
1/(n âˆ’ m + 1)
1
3
âˆš
Î» = 0, E LLIPSOID [11]
O( nm 4 log n log 2 m)
Î» = 1, G REEDW ELFARE [8]
1/2
Î´
)
Î» = 0, G REED S ATâˆ—
(1/2 âˆ’ Î´, 1/2+Î´
Î» = 0, MM AXâˆ—

O(min
i

â€ âˆ—

Problem 2 (Min-(Max+Avg))
Approximation factor
Î» = 0, BALANCEDâ€  [25]
min{m, n/m}
âˆš
Î» = 0, S AMPLING [25]
O( n log n)
âˆš
Î» = 0, E LLIPSOID [11]
O( n log n)
Î» = 1, G REED S PLITâ€  [29, 22]
2
Î» = 1, R ELAX [5]
O(log n)
âˆ—

Î» = 0, MM INâˆ—

1
)
âˆš
|AÏ€Ì‚ | m log3 m
i

2max|AÏ€
i |
i

âˆ—

Î» = 0, G REED M AX
0 < Î» < 1, C OMB S FA S WPâˆ—

1/m
max{ Î²Î± , Î»Î²}

Î» = 0, L OV AÌSZ ROUND
0 < Î» < 1, C OMB S LB S MPâˆ—

m
min{ mÎ± , Î²(mÎ»Ì„ + Î»)}

0 < Î» < 1, G ENERAL G REED S ATâˆ—

Î»/2

0 < Î» < 1, G ENERAL L OV AÌSZ ROUNDâˆ—

m

Î» = 0, Hardness
Î» = 1, Hardness

Î»Ì„Î²+Î±

âˆ—

1/2 [12]
1 âˆ’ 1/e [26]

Î» = 0, Hardness
Î» = 1, Hardness

mÎ»Ì„+Î»

m
2 âˆ’ 2/m [7]

Table 1: Summary of our contributions and existing work on Problems 1 and 2.2 See text for details.
Previous work: Special cases of Problems 1 and 2 have appeared previously. Problem 1 with Î» = 0
is called submodular fair allocation (SFA), and Problem 2 with Î» = 0 is called submodular load
balancing (SLB), robust optimization problems both of which previously have been studied. When
fi â€™s are all modular, SLB is called minimum makespan scheduling. An LP relaxation algorithm
provides a 2-approximation for the heterogeneous setting [19]. When the objectives are submodular,
the problem becomes much harder. Even in the homogeneous
p setting, [25] show that the problem
is information theoretically hard to approximate within o( n/ log n). They provide a balanced
partitioning algorithm yielding a factor of min{m,
p n/m} under the homogeneous setting. They also
give a sampling-based algorithm achieving O( n/ log n) for the homogeneous setting. However,
the sampling-based algorithm is not practical and scalable since it involves solving, in the worst-case,
O(n3 log n) instances of submodular function minimization each of which requires O(n5 Î³ + n6 )
computation [23], where Î³ is the cost of a function valuation. Another approach approximates
each submodular function by its ellipsoid approximation (again non-scalable) and reduces SLB
to its
âˆš modular version (minimum makespan scheduling) leading to an approximation factor of
O( n log n) [11]. SFA, on the other hand, has been studied mostly in the heterogeneous setting.
When fâˆši â€™s are all modular, the tightest algorithm, so far, is to iteratively round an LP solution achieving
O(1/( m log3 m)) approximation [2], whereas the problem is NP-hard to 1/2 +  approximate for
any  > 0 [12]. When fi â€™s are submodular, [12] gives a matching-based algorithm with a factor
1/(n âˆ’ m + 1) approximation that performs poorly when m  n. [16] proposes a binary search
algorithm yielding an improved factor of 1/(2m âˆ’ 1). Similar to SLB, [11] applies the same ellipsoid
1
2

Similar sub-categorizations have been called the â€œuniformâ€ vs. the â€œnon-uniformâ€ case in the past [25, 11].
Results obtained in this paper are marked as âˆ—. Methods for only the homogeneous setting are marked as â€ .

2

âˆš
approximation techniques leading to a factor of O( nm1/4 log n log3/2 m). These approaches are
theoretically interesting, but they do not scale to large problems. Problems 1 and 2, when Î» = 1, have
also been previously studied. Problem 2 becomes the submodular multiway partition (SMP) for which
one can obtain a relaxation based 2-approximation [5] in the homogeneous case. In the heterogeneous
case, the guarantee is O(log n) [6]. Similarly, [29, 22] propose a greedy splitting 2-approximation
algorithm for the homogeneous setting. Problem 1 becomes the submodular welfare [26] for which a
scalable greedy algorithm achieves a 1/2 approximation [8]. Unlike the worst case (Î» = 0), many of
the algorithms proposed for these problems are scalable. The general case (0 < Î» < 1) of Problems 1
and 2 differs from either of these extreme cases since we wish both for a robust (worst-case) and
average case partitioning, and controlling Î» allows one to trade off between the two. As we shall see,
the flexibility of a mixture can be more natural in certain applications.
Applications: There are a number of applications of submodular partitioning in ML as outlined below.
Some of these we evaluate in Section 4. Submodular functions naturally capture notions of interacting
cooperative costs and homogeneity and thus are useful for clustering and image segmentation [22,
17]. While the average case instance has been used before, a more worst-case variant (i.e., Problem 2
with Î» â‰ˆ 0) is useful to produce balanced clusterings (i.e., the submodular valuations of all the
blocks should be similar to each other). Problem 2 also addresses a problem in image segmentation,
namely how to use only submodular functions (which are instances of pseudo-Boolean functions) for
multi-label (i.e., non-Boolean) image segmentation. Problem 2 addresses this problem by allowing
each segment j to have its own submodular function fj , and the objective measures the homogeneity
fj (AÏ€j ) of segment j based on the image pixels AÏ€j assigned to it. Moreover, by combining the average
case and the worst case objectives, one can achieve a tradeoff between the two. Empirically, we
evaluate our algorithms on unsupervised image segmentation (Section 4) and find that it outperforms
other clustering methods including k-means, k-medoids, spectral clustering, and graph cuts.
Submodularity also accurately represents computational costs in distributed systems, as shown
in [20]. In fact, [20] considers two separate problems: 1) text data partitioning for balancing memory
demands; and 2) parameter partitioning for balancing communication costs. Both are treated by
solving an instance of SLB (Problem 2, Î» = 0) where memory costs are modeled using a set-cover
submodular function and the communication costs are modeled using a modular (additive) function.
Another important ML application, evaluated in Section 4, is distributed training of statistical
models. As data set sizes grow, the need for statistical training procedures tolerant of the distributed
data partitioning becomes more important. Existing schemes are often developed and performed
assuming data samples are distributed in an arbitrary or random fashion. As an alternate strategy,
if the data is intelligently partitioned such that each block of samples can itself lead to a good
approximate solution, a consensus amongst the distributed results could be reached more quickly
than when under a poor partitioning. Submodular functions can in fact express the value of a
subset of training data for certain machine learning risk functions, e.g., [27]. Using these functions
within Problem 1, one can expect a partitioning (by formulating the problem as an instance of
Problem 1, Î» â‰ˆ 0) where each block is a good representative of the entire set, thereby achieving
faster convergence in distributed settings. We demonstrate empirically, in Section 4, that this provides
better results on several machine learning tasks, including the training of deep neural networks.
Our Contributions: In contrast to Problems 1 and 2 in the average case (i.e., Î» = 1), existing
algorithms for the worst case (Î» = 0) are not scalable. This paper closes this gap, by proposing
three new classes of algorithmic frameworks to solve SFA and SLB: (1) greedy algorithms; (2)
semigradient-based algorithms; and (3) a LovaÌsz extension based relaxation algorithm. For SFA,
when m = 2, we formulate the problem as non-monotone submodular maximization, which can
be approximated up to a factor of 1/2 with O(n) function evaluations [4]. For general m, we give a
simple and scalable greedy algorithm (G REED M AX), and show a factor of 1/m in the homogeneous
setting, improving the state-of-the-art factor of 1/(2m âˆ’ 1) under the heterogeneous setting [16].
For the heterogeneous setting, we propose a â€œsaturateâ€ greedy algorithm (G REED S AT) that iteratively
solves instances of submodular welfare problems. We show G REED S AT has a bi-criterion guarantee
of (1/2 âˆ’ Î´, Î´/(1/2 + Î´)), which ensures at least dm(1/2 âˆ’ Î´)e blocks receive utility at least
Î´/(1/2 + Î´)OP T for any 0 < Î´ < 1/2. For SLB, we first generalize thephardness result in [25]
and show that it is hard to approximate better than m for any m = o( n/ log n) even in the
homogeneous setting. We then give a LovaÌsz extension based relaxation algorithm (L OV AÌSZ ROUND)
yielding a tight factor of m for the heterogeneous setting. As far as we know, this is the first algorithm
achieving a factor of m for SLB in this setting. For both SFA and SLB, we also obtain more efficient

3

algorithms with bounded approximation factors, which we call majorization-minimization (MM IN)
and minorization-maximization (MM AX).
Next we show algorithms to handle Problems 1 and 2 with general 0 < Î» < 1. We first give two simple and generic schemes (C OMB S FA S WP and C OMB S LB S MP), both of which efficiently combines an
algorithm for the worst-case problem (special case with Î» = 0), and an algorithm for the average case
(special case with Î» = 1) to provide a guarantee interpolating between the two bounds. For Problem 1
we generalize G REED S AT leading to G ENERAL G REED S AT, whose guarantee smoothly interpolates in
terms of Î» between the bi-criterion factor by G REED S AT in the case of Î» = 0 and the constant factor of
1/2 by the greedy algorithm in the case of Î» = 1. For Problem 2 we generalize L OV AÌSZ ROUND to obtain a relaxation algorithm (G ENERAL L OV AÌSZ ROUND) that achieves an m-approximation for general
Î». The theoretical contributions and the existing work for Problems 1 and 2 are summarized in Table 1.
Lastly, we demonstrate the efficacy of Problem 2 on unsupervised image segmentation, and the
success of Problem 1 to distributed machine learning, including ADMM and neural network training.

2

Robust Submodular Partitioning (Problems 1 and 2 when Î» = 0)

Notation: we define f (j|S) , f (S âˆª j) âˆ’ f (S) as the gain of j âˆˆ V in the context of S âŠ† V . We
assume w.l.o.g. that the ground set is V = {1, 2, Â· Â· Â· , n}.
2.1 Approximation Algorithms for SFA (Problem 1 with Î» = 0)
We first study approximation algorithms for SFA. When m = 2, the problem becomes maxAâŠ†V g(A)
where g(A) = min{f1 (A), f2 (V \ A)} and is submodular thanks to Theorem 2.1.
Theorem 2.1. If f1 and f2 are monotone submodular, min{f1 (A), f2 (V \ A)} is also submodular.
Proofs for all theorems in this paper are given in [28]. The simple bi-directional randomized greedy
algorithm [4] therefore approximates SFA with m = 2 to a factor of 1/2 matching the problemâ€™s
hardness. For general m, we approach SFA from the perspective of the greedy algorithms. In
this work we introduce two variants of a greedy algorithm â€“ G REED M AX (Alg. 1) and G REED S AT
(Alg. 2), suited to the homogeneous and heterogeneous settings, respectively.
G REED M AX: The key idea of G REED M AX (see Alg. 1) is to greedily add an item with the
maximum marginal gain to the block whose current solution is minimum. Initializing {Ai }m
i=1 with
the empty sets, the greedy flavor also comes from that it incrementally grows the solution by greedily
improving the overall objective mini=1,...,m fi (Ai ) until {Ai }m
i=1 forms a partition. Besides its
simplicity, Theorem 2.2 offers the optimality guarantee.
Theorem 2.2. G REED M AX achieves a guarantee of 1/m under the homogeneous setting.
By assuming the homogeneity of the fi â€™s, we obtain a very simple 1/m-approximation algorithm
improving upon the state-of-the-art factor 1/(2m âˆ’ 1) [16]. Thanks to the lazy evaluation trick as
described in [21], Line 5 in Alg. 1 need not to recompute the marginal gain for every item in each
round, leading G REED M AX to scale to large data sets.
G REED S AT: Though simple and effective in the homogeneous setting, G REED M AX performs
arbitrarily poorly under the heterogeneous setting. To this end we provide another algorithm â€“
â€œSaturateâ€ Greedy (G REED S AT, see Alg. 2). The key idea of G REED S AT is to relax SFA to a much
simpler problem â€“ Submodular Welfare (SWP), i.e., Problem 1 with Î» = 0. Similar
Pm in flavor to the
one proposed in [18] G REED S AT defines an intermediate objective FÌ„ c (Ï€) = i=1 fic (AÏ€i ), where
1
fic (A) = m
min{fi (A), c} (Line 2). The parameter c controls the saturation in each block. fic satisfies submodularity for each i. Unlike SFA, the combinatorial optimization problem maxÏ€âˆˆÎ  FÌ„ c (Ï€)
(Line 6) is much easier and is an instance of SWP. In this work, we solve Line 6 by the efficient
greedy algorithm as described in [8] with a factor 1/2. One can also use a more computationally
expensive multi-linear relaxation algorithm as given in [26] to solve Line 6 with a tight factor
Î± = (1 âˆ’ 1/e). Setting the input argument Î± as the approximation factor for Line 6, the essential idea
of G REED S AT is to perform a binary search over the parameter c to find the largest câˆ— such that the
âˆ—
âˆ—
âˆ—
returned solution Ï€Ì‚ c for the instance of SWP satisfies FÌ„ c (Ï€Ì‚ c ) â‰¥ Î±câˆ— . G REED S AT terminates after
mini fi (V )
solving O(log(
)) instances of SWP. Theorem 2.3 gives a bi-criterion optimality guarantee.

Theorem 2.3. Given  > 0, 0 â‰¤ Î± â‰¤ 1 and any 0 < Î´ < Î±, G REED S AT finds a partition such that
Î´
at least dm(Î± âˆ’ Î´)e blocks receive utility at least 1âˆ’Î±+Î´
(maxÏ€âˆˆÎ  mini fi (AÏ€i ) âˆ’ ).
4

Algorithm 4: G REED M IN

Algorithm 1: G REED M AX
1:
2:
3:
4:
5:
6:
7:
8:

1:
2:
3:
4:
5:
6:
7:
8:

Input: f , m, V .
Let A1 =, . . . , = Am = âˆ…; R = V .
while R 6= âˆ… do
j âˆ— âˆˆ argminj f (Aj );
aâˆ— âˆˆ argmaxaâˆˆR f (a|Aj âˆ— )
Aj âˆ— â† Aj âˆ— âˆª {aâˆ— }; R â† R \ aâˆ—
end while
Output {Ai }m
i=1 .

Algorithm 2: G REED S AT

Algorithm 5: MM IN

1: Input: {fi }m
V , Î±.
i=1 , m,
1 Pm
Ï€
2: Let FÌ„ c (Ï€) = m
i=1 min{fi (Ai ), c}.
3: Let cmin = 0, cmax = mini fi (V )
4: while cmax âˆ’ cmin â‰¥  do
5:
c = 21 (cmax + cmin )
6:
Ï€Ì‚ c âˆˆ argmaxÏ€âˆˆÎ  FÌ„ c (Ï€)
7:
if FÌ„ c (Ï€Ì‚ c ) < Î±c then
8:
cmax = c
9:
else
10:
cmin = c; Ï€Ì‚ â† Ï€Ì‚ c
11:
end if
12: end while
13: Output: Ï€Ì‚.

0
1: Input: {fi }m
i=1 , m, V , partition Ï€ .
2: Let t = 0
3: repeat
4:
for i = 1, . . . , m do
t
5:
Pick a supergradient mi at AÏ€
i for fi .
6:
end for
7:
Ï€ t+1 âˆˆ argminÏ€âˆˆÎ  maxi mi (AÏ€
i )
8:
t = t + 1;
9: until Ï€ t = Ï€ tâˆ’1
10: Output: Ï€ t .

Algorithm 6: MM AX
0
1: Input: {fi }m
i=1 , m, V , partition Ï€ .
2: Let t = 0.
3: repeat
4:
for i = 1, . . . , m do
t
5:
Pick a subgradient hi at AÏ€
i for fi .
6:
end for
7:
Ï€ t+1 âˆˆ argmaxÏ€âˆˆÎ  mini hi (AÏ€
i )
8:
t = t + 1;
9: until Ï€ t = Ï€ tâˆ’1
10: Output: Ï€ t .

Algorithm 3: L OV AÌSZ ROUND
1:
2:
3:
4:
5:
6:
7:

Input: f , m, V ;
Let A1 =, . . . , = Am = âˆ…; R = V .
while R 6= âˆ… do
j âˆ— âˆˆ argminj f (Aj )
aâˆ— âˆˆ minaâˆˆR f (a|Aj âˆ— )
Aj âˆ— â† Aj âˆ— âˆª aâˆ— ; R â† R \ aâˆ—
end while
Output {Ai }m
i=1 .

Ëœ m
Input: {fi }m
i=1 , {fi }i=1 , m, V .
Solve for {xâˆ—i }m
i=1 via convex relaxation.
Rounding: Let A1 =, . . . , = Am = âˆ….
for j = 1, . . . , n do
iÌ‚ âˆˆ argmaxi xâˆ—i (j); AiÌ‚ = AiÌ‚ âˆª j
end for
Output {Ai }m
i=1 .

For any 0 < Î´ < Î± Theorem 2.3 ensures that the top dm(Î± âˆ’ Î´)e valued blocks in the partition
returned by G REED S AT are (Î´/(1âˆ’Î±+Î´)âˆ’)-optimal. Î´ controls the trade-off between the number of
top valued blocks to bound and the performance guarantee attained for these blocks. The smaller Î´ is,
the more top blocks are bounded, but with a weaker guarantee. We set the input argument Î± = 1/2 (or
Î± = 1 âˆ’ 1/e) as the worst-case performance guarantee for solving SWP so that the above theoretical
analysis follows. However, the worst-case is often achieved only by very contrived submodular
functions. For the ones used in practice, the greedy algorithm often leads to near-optimal solution
([18] and our own observations). Setting Î± as the actual performance guarantee for SWP (often very
close to 1) can improve the empirical bound, and we, in practice, typically set Î± = 1 to good effect.
MM AX: Lastly, we introduce another algorithm for the heterogeneous setting, called minorizationmaximization (MM AX, see Alg. 6). Similar to the one proposed in [14], the idea is to iteratively
maximize tight lower bounds of the submodular functions. Submodular functions have tight modular
lower bounds, which are related to the subdifferential âˆ‚f (Y ) of the submodular set function f at a set
Y âŠ† V [9]. Denote a subgradient at Y by hY âˆˆ âˆ‚f (Y ), the extreme points of âˆ‚f (Y ) may be computed via a greedy algorithm: Let Ïƒ be a permutation of V that assigns the elements in Y to the first
|Y | positions (Ïƒ(i) âˆˆ Y if and only if i â‰¤ |Y |). Each such permutation defines a chain with elements
Ïƒ
Ïƒ
S0Ïƒ = âˆ…, SiÏƒ = {Ïƒ(1), Ïƒ(2), . . . , Ïƒ(i)}, and S|Y
| = Y . An extreme point hY of âˆ‚f (Y ) has each entry
Ïƒ
Ïƒ
Ïƒ
Ïƒ
as hY (Ïƒ(i))
P= f (Si ) âˆ’ f (Siâˆ’1 ). Defined as above, hY forms a lower bound of f , tight at Y â€” i.e.,
hÏƒY (X) = jâˆˆX hÏƒY (j) â‰¤ f (X), âˆ€X âŠ† V and hÏƒY (Y ) = f (Y ). The idea of MM AX is to consider a
modular lower bound tight at the set corresponding to each block of a partition. In other words, at itert
ation t + 1, for each block i, we approximate fi with its modular lower bound tight at AÏ€i and solve a
modular version of Problem 1 (Line 7), which admits efficient approximation algorithms [2]. MM AX
is initialized with a partition Ï€ 0 , which isP
obtained by solving Problem 1, where each fi is replaced
with a simple modular function fi0 (A) = aâˆˆA fi (a). The following worst-case bound holds:
Ï€Ì‚
1+(|AÏ€Ì‚
i |âˆ’1)(1âˆ’Îºfi (Ai ))
âˆš
), where
3m
|AÏ€Ì‚
|
m
log
i
f (v|A\v)
Îºf (A) = 1 âˆ’ minvâˆˆV f (v) âˆˆ

Theorem 2.4. MM AX achieves a worst-case guarantee of O(mini
Ï€Ì‚ = (AÏ€Ì‚1 , Â· Â· Â· , AÏ€Ì‚m ) is the partition obtained by the algorithm, and
[0, 1] is the curvature of a submodular function f at A âŠ† V .
5

2.2

Approximation Algorithms for SLB (Problem 2 with Î» = 0)

p
We next investigate SLB, where existing hardness
p results [25] are o( n/ log n), which is independent
of m and implicitly assumes that m = Î˜( n/ log n). However, applications for SLB are often
dependent on m with m  n. We hence offer hardness analysis in terms of m in the following.
Theorem
p 2.5. For any  > 0, SLB cannot be approximated to a factor of (1 âˆ’ )m for any
m = o( n/ log n) with polynomial number of queries even under the homogeneous setting.
p
For the rest of the paper, we assume m = o( n/ log n) for SLB, unless stated otherwise.
G REED M IN: Theorem 2.5 implies that SLB is hard to approximate better than m. However, an
arbitrary partition Ï€ âˆˆ Î  already achieves the best approximation factor of m that one can hope
P
0
0
for under the homogeneous setting, since maxi f (AÏ€i ) â‰¤ f (V ) â‰¤ i f (AÏ€i ) â‰¤ m maxi f (AÏ€i )
0
for any Ï€ âˆˆ Î . In practice, one can still implement a greedy style heuristic, which we refer to as
G REED M IN (Alg. 4). Very similar to G REED M AX, G REED M IN only differs in Line 5, where the
item with the smallest marginal gain is added. Since the functions are all monotone, any additions
to a block can (if anything) only increase its value, so we choose to add to the minimum valuation
block in Line 4 to attempt to keep the maximum valuation block from growing further.
L OV AÌSZ ROUND: Next we consider the heterogeneous setting, for which we propose a tight
algorithm â€“ L OV AÌSZ ROUND (see Alg. 3). The algorithm proceeds as follows: (1) apply the LovaÌsz
extension of submodular functions to relax SLB to a convex program, which is exactly solved to
a fractional solution (Line 2); (2) map the fractional solution to a partition using the Î¸-rounding
technique as proposed in [13] (Line 3 - 6). The LovaÌsz extension, which naturally connects a
submodular function f with its convex relaxation fËœ, is defined as follows: given any x âˆˆ [0, 1]n ,
we obtain a permutation Ïƒx by ordering its elements in non-increasing order, and thereby a chain of
sets S0Ïƒx âŠ‚, . . . , âŠ‚ SnÏƒx with SjÏƒx = {Ïƒx (1), . . . , Ïƒx (j)} for j = 1, . . . , n. The LovaÌsz extension fËœ
Pn
Ïƒx
for f is the weighted sum of the ordered entries of x: fËœ(x) = j=1 x(Ïƒx (j))(f (SjÏƒx ) âˆ’ f (Sjâˆ’1
)).
Ëœ
Given the convexity of the fi â€™s , SLB is relaxed to the following convex program:
m
X
Ëœ
min
max fi (xi ), s.t
xi (j) â‰¥ 1, for j = 1, . . . , n
(1)
n
x1 ,...,xm âˆˆ[0,1]

i

i=1

Denoting the optimal solution for Eqn 1 as {xâˆ—1 , . . . , xâˆ—m }, the Î¸-rounding step simply maps each
item j âˆˆ V to a block iÌ‚ such that iÌ‚ âˆˆ argmaxi xâˆ—i (j) . The bound for L OV AÌSZ ROUND is as follows:
Theorem 2.6. L OV AÌSZ ROUND achieves a worst-case approximation factor m.
We remark that, to the best of our knowledge, L OV AÌSZ ROUND is the first algorithm that is tight
and that gives an approximation in terms of m for the heterogeneous setting.
MM IN: Similar to MM AX for SFA, we propose Majorization-Minimization (MM IN, see Alg. 5)
for SLB. Here, we iteratively choose modular upper bounds, which are defined via superdifferentials
âˆ‚ f (Y ) of a submodular function [15] at Y . Moreover, there are specific supergradients [14] that
define the following two modular upper bounds (when referring to either one, we use mfX ):
mfX,1 (Y ) , f (X) âˆ’

X

jâˆˆX\Y

f (j|X\j) +

X

f (j|âˆ…), mfX,2 (Y ) , f (X) âˆ’

jâˆˆY \X

X

jâˆˆX\Y

f (j|V \j) +

X

f (j|X).

jâˆˆY \X

Then mfX,1 (Y ) â‰¥ f (Y ) and mfX,2 (Y ) â‰¥ f (Y ), âˆ€Y âŠ† V and mfX,1 (X) = mfX,2 (X) = f (X). At
iteration t + 1, for each block i, MM IN replaces fi with a choice of its modular upper bound mi tight
t
at AÏ€i and solves a modular version of Problem 2 (Line 7), for which there exists an efficient LP relaxation based algorithm [19]. Similar to MM AXP
, the initial partition Ï€ 0 is obtained by solving Problem
2, where each fi is substituted with fi0 (A) = aâˆˆA fi (a). The following worst-case bound holds:
Theorem 2.7. MM IN achieves a worst-case guarantee of (2 maxi
âˆ—

Ï€ =

âˆ—
(AÏ€1 , Â· Â· Â·

âˆ—
, AÏ€m )

denotes the optimal partition.

6

âˆ—
|AÏ€
i |
âˆ—
âˆ—
Ï€
1+(|Ai |âˆ’1)(1âˆ’Îºfi (AÏ€
i ))

), where

5-Partition on 20newsgroup with ADMM

30-Partition on TIMIT

5-Partition on MNIST with Distributed NN

86
50

99

45

83

82

81

Test accuracy (%)

98.9

84

Test accuracy (%)

Test accuracy (%)

85

99.1

98.8
98.7
98.6

40
35
30
25

98.5
20
98.4

80
Submodular partition
Random partition

Submodular partition
Random partition

98.3

Submodular partition
Random partition

15

79
5

10

15

20

25

30

35

5

Number of iterations

10

15

5

20

10

15

99.2

84

25

30

35

40

45

50

55

Number of iterations

Number of iterations

10-Partition on 20newsgroup with ADMM

20

40-Block Partition on TIMIT with Distributed NN

10-Partition on MNIST with Distributed NN
50

99

45

80

78

Test accuracy (%)

98.8

Test accuracy (%)

Test accuracy (%)

82

98.6
98.4
98.2

40
35
30
25

76
98
Submodular partition
Adversarial partition
Random partition

74

20

97.8

15

Submodular partition
Random partition

Submodular partition
Random partition

10
5

10

15

20

25

30

Number of iterations

(a)20Newsgroups

35

5

10

Number of iterations

(b) MNIST

15

20

5

10

15

20

25

30

35

40

45

50

55

Number of iterations

(c) TIMIT

Figure 1: Comparison between submodular and random partitions for distributed ML, including
ADMM (Fig 1a) and distributed neural nets (Fig 1b) and (Fig 1c). For the box plots, the central mark
is the median, the box edges are 25th and 75th percentiles, and the bars denote the best and worst cases.

3

General Submodular Partitioning (Problems 1 and 2 when 0 < Î» < 1)

In this section we study Problem 1 and Problem 2, in the most general case, i.e., 0 < Î» < 1. We first
propose a simple and general â€œextremal combinationâ€ scheme that works both for problem 1 and 2.
It naturally combines an algorithm for solving the worst-case problem (Î» = 0) with an algorithm for
solving the average case (Î» = 1). We use Problem 1 as an example, but the same scheme easily works
for Problem 2. Denote A LG WC as the algorithm for the worst-case problem (i.e. SFA), and A LG AC
as the algorithm for the average case (i.e., SWP). The scheme is to first obtain a partition Ï€Ì‚1 by running
A LG WC on the instance of Problem 1 with Î» = 0 and a second partition Ï€Ì‚2 by running A LG AC with
Î» = 1. Then we output one of Ï€Ì‚1 and Ï€Ì‚2 , with which the higher valuation for Problem 1 is achieved.
We call this scheme C OMB S FA S WP. Suppose A LG WC solves the worst-case problem with a factor
Î± â‰¤ 1 and A LG AC for the average case with Î² â‰¤ 1. When applied to Problem 2 we refer to this
scheme as C OMB S LB S MP (Î± â‰¥ 1 and Î² â‰¥ 1). The following guarantee holds for both schemes:
Î²Î±
Theorem 3.1. For any Î» âˆˆ (0, 1) C OMB S FA S WP solves Problem 1 with a factor max{ Î»Ì„Î²+Î±
, Î»Î²}
Î²Î±
1
in the heterogeneous case, and max{min{Î±, m
}, Î»Ì„Î²+Î±
, Î»Î²} in the homogeneous case. Similarly,
C OMB S LB S MP solves Problem 2 with a factor min{ mmÎ±
, Î²(mÎ»Ì„ + Î»)} in the heterogeneous case,
Î»Ì„+Î»
mÎ±
and min{m, mÎ»Ì„+Î» , Î²(mÎ»Ì„ + Î»)} in the homogeneous case.

The drawback of C OMB S FA S WP and C OMB S LB S MP is that they do not explicitly exploit the tradeoff between the average-case and worst-case objectives in terms of Î». To obtain more practically
interesting algorithms, we also give G ENERAL G REED S AT that generalizes G REED
PmS AT to solve Prob1
Ï€
lem 1. Similar to G REED S AT we define an intermediate objective: FÌ„Î»c (Ï€) = m
i=1 min{Î»Ì„fi (Ai )+
Pm
1
Ï€
Î» m j=1 fj (Aj ), c} in G ENERAL G REED S AT. Following the same algorithmic design as in G REED S AT, G ENERAL G REED S AT only differs from G REED S AT in Line 6, where the submodular welfare
problem is defined on the new objective FÌ„Î»c (Ï€). In [28] we show that G ENERAL G REED S AT gives Î»/2
approximation, while also yielding a bi-criterion guarantee that generalizes Theorem 2.3. In particular
G ENERAL G REED S AT recovers the bicriterion guarantee as shown in Theorem 2.3 when Î» = 0. In
the case of Î» = 1, G ENERAL G REED S AT recovers the 1/2-approximation guarantee of the greedy
algorithm for solving the submodular welfare problem, i.e., the average-case objective. Moreover an
improved guarantee is achieved by G ENERAL G REED S AT as Î» increases. Details are given in [28].
To solve Problem 2 we generalize L OV AÌSZ ROUND leading to G ENERAL L OV AÌSZ ROUND. Similar
to L OV AÌSZ ROUND we relax each submodular objective as its convex relaxation using the LovaÌsz
extension. Almost the same as L OV AÌSZ ROUND, G ENERAL L OV AÌSZ ROUND only differs in Line 2,
where Problem 2 is relaxed as the following convex program: minx1 ,...,xm âˆˆ[0,1]n Î»Ì„ maxi fËœi (xi ) +
7

Pm Ëœ
Pm
1
Î»m
j=1 fj (xj ), s.t
i=1 xi (j) â‰¥ 1, for j = 1, . . . , n. Following the same rounding procedure
as L OV AÌSZ ROUND, G ENERAL L OV AÌSZ ROUND is guaranteed to give an m-approximation for
Problem 2 with general Î». Details are given in [28].

4

Experiments and Conclusions

We conclude in this section by empirically evaluating the algorithms proposed for Problems 1 and 2
on real-world data partitioning applications including distributed ADMM, distributed deep neural
network training, and lastly unsupervised image segmentation tasks.
ADMM: We first consider data partitioning for distributed convex optimization. The evaluation
task is text categorization on the 20 Newsgroup data set, which consists of 18,774 articles divided
almost evenly across 20 classes. We formulate the multi-class classification as an `2 regularized
logistic regression, which is solved by ADMM implemented as [3]. We run 10 instances of random
partitioning on the training data as a baseline. In this case, we use the feature based function (same as
the one used in [27]), in the homogeneous setting of Problem 1 (with Î» = 0). We use G REED M AX
as the partitioning algorithm. In Figure 1a, we observe that the resulting partitioning performs much
better than a random partitioning (and significantly better than an adversarial partitioning, formed
by grouping similar items together). More details are given in [28].
Distributed Deep Neural Network (DNN) Training: Next we evaluate our framework on distributed deep neural network (DNN) training. We test on two tasks: 1) handwritten digit recognition
on the MNIST database, which consists of 60,000 training and 10,000 test samples; 2) phone
classification on the TIMIT data, which has 1,124,823 training and 112,487 test samples. A 4-layer
DNN model is applied to the MNIST experiment, and we train a 5-layer DNN for TIMIT. For both
experiments the submodular partitioning is obtained by solving the homogeneous case of Problem 1
(Î» = 0) using G REED M AX on a form of clustered facility location (as proposed and used in [27]).
We perform distributed training using an averaging stochastic gradient descent scheme, similar to the
one in [24]. We also run 10 instances of random partitioning as a baseline. As shown in Figure 1b
and 1c, the submodular partitioning outperforms the random baseline. An adversarial partitioning,
which is formed by grouping items with the same class, in either case, cannot even be trained.
Unsupervised Image SegF-measure on
Original
mentation: We test the
all of GrabCut
efficacy of Problem 2 on
Ground
1.0
unsupervised image segTruth
mentation over the GrabCut
data set (30 color images
k-means
0.810
and their ground truth foreground/background labels).
k-medoids
0.823
By â€œunsupervisedâ€, we
mean that no labeled data
Spectral
0.854
Clustering
at any time in supervised
or semi-supervised training,
Graph
0.853
Cut
nor any kind of interactive
segmentation, was used in
Submodular
0.870
Partitioning
forming or optimizing the
objective. The submodular
partitioning for each image Figure 2: Unsupervised image segmentation (right: some examples).
is obtained by solving the
homogeneous case of Problem 2 (Î» = 0.8) using a modified variant of G REED M IN on the facility location function. We compare our method against the other unsupervised methods k-means, k-medoids,
spectral clustering, and graph cuts. Given an m-partition of an image and its ground truth labels, we
assign each of the m blocks either to the foreground or background label having the larger intersection.
In Fig. 2 we show example segmentation results after this mapping on several example images as well
as averaged F-measure (relative to ground truth) over the whole data set. More details are given in [28].
Acknowledgments: This material is based upon work supported by the National Science Foundation
under Grant No. IIS-1162606, the National Institutes of Health under award R01GM103544, and by
a Google, a Microsoft, and an Intel research award. R. Iyer acknowledges support from a Microsoft
Research Ph.D Fellowship. This work was supported in part by TerraSwarm, one of six centers of
STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA.
8

References
[1] D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In SODA, 2007.
[2] A. Asadpour and A. Saberi. An approximation algorithm for max-min fair allocation of indivisible goods.
In SICOMP, 2010.
[3] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning
via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 2011.
[4] N. Buchbinder, M. Feldman, J. Naor, and R. Schwartz. A tight linear time (1/2)-approximation for
unconstrained submodular maximization. In FOCS, 2012.
[5] C. Chekuri and A. Ene. Approximation algorithms for submodular multiway partition. In FOCS, 2011.
[6] C. Chekuri and A. Ene. Submodular cost allocation problem and applications. In Automata, Languages
and Programming, pages 354â€“366. Springer, 2011.
[7] A. Ene, J. VondraÌk, and Y. Wu. Local distribution and the symmetry gap: Approximability of multiway
partitioning problems. In SODA, 2013.
[8] M. Fisher, G. Nemhauser, and L. Wolsey. An analysis of approximations for maximizing submodular set
functionsâ€”II. In Polyhedral combinatorics, 1978.
[9] S. Fujishige. Submodular functions and optimization, volume 58. Elsevier, 2005.
[10] L. A. GarcÄ±Ìa-Escudero, A. Gordaliza, C. MatraÌn, and A. Mayo-Iscar. A review of robust clustering methods.
Advances in Data Analysis and Classification, 4(2-3):89â€“109, 2010.
[11] M. Goemans, N. Harvey, S. Iwata, and V. Mirrokni. Approximating submodular functions everywhere. In
SODA, 2009.
[12] D. Golovin. Max-min fair allocation of indivisible goods. Technical Report CMU-CS-05-144, 2005.
[13] R. Iyer, S. Jegelka, and J. Bilmes. Monotone closure of relaxed constraints in submodular optimization:
Connections between minimization and maximization: Extended version.
[14] R. Iyer, S. Jegelka, and J. Bilmes. Fast semidifferential based submodular function optimization. In ICML,
2013.
[15] S. Jegelka and J. Bilmes. Submodularity beyond submodular energies: coupling edges in graph cuts. In
CVPR, 2011.
[16] S. Khot and A. Ponnuswami. Approximation algorithms for the max-min allocation problem. In APPROX,
2007.
[17] V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? In TPAMI, 2004.
[18] A. Krause, B. McMahan, C. Guestrin, and A. Gupta. Robust submodular observation selection. In JMLR,
2008.
[19] J. K. Lenstra, D. B. Shmoys, and EÌ. Tardos. Approximation algorithms for scheduling unrelated parallel
machines. In Mathematical programming, 1990.
[20] M. Li, D. Andersen, and A. Smola. Graph partitioning via parallel submodular approximation to accelerate
distributed machine learning. In arXiv preprint arXiv:1505.04636, 2015.
[21] M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In Optimization
Techniques, 1978.
[22] M. Narasimhan, N. Jojic, and J. A. Bilmes. Q-clustering. In NIPS, 2005.
[23] J. Orlin. A faster strongly polynomial time algorithm for submodular function minimization. Mathematical
Programming, 2009.
[24] D. Povey, X. Zhang, and S. Khudanpur. Parallel training of deep neural networks with natural gradient and
parameter averaging. arXiv preprint arXiv:1410.7455, 2014.
[25] Z. Svitkina and L. Fleischer. Submodular approximation: Sampling-based algorithms and lower bounds.
In FOCS, 2008.
[26] J. VondraÌk. Optimal approximation for the submodular welfare problem in the value oracle model. In
STOC, 2008.
[27] K. Wei, R. Iyer, and J. Bilmes. Submodularity in data subset selection and active learning. In ICML, 2015.
[28] K. Wei, R. Iyer, S. Wang, W. Bai, and J. Bilmes. Mixed robust/average submodular partitioning: Fast
algorithms, guarantees, and applications: NIPS 2015 Extended Supplementary.
[29] L. Zhao, H. Nagamochi, and T. Ibaraki. On generalized greedy splitting algorithms for multiway partition
problems. Discrete applied mathematics, 143(1):130â€“143, 2004.

9

