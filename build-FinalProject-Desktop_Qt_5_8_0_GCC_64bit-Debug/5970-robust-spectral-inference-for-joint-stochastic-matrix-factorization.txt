Robust Spectral Inference for Joint Stochastic Matrix
Factorization
David Mimno
Dept. of Information Science
Cornell University
Ithaca, NY 14850
mimno@cornell.edu

Moontae Lee, David Bindel
Dept. of Computer Science
Cornell University
Ithaca, NY 14850
{moontae,bindel}@cs.cornell.edu

Abstract
Spectral inference provides fast algorithms and provable optimality for latent topic
analysis. But for real data these algorithms require additional ad-hoc heuristics,
and even then often produce unusable results. We explain this poor performance
by casting the problem of topic inference in the framework of Joint Stochastic
Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel
rectification method that learns high quality topics and their interactions even on
small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.

1

Introduction

Summarizing large data sets using pairwise co-occurrence frequencies is a powerful tool for data
mining. Objects can often be better described by their relationships than their inherent characteristics. Communities can be discovered from friendships [1], song genres can be identified
from co-occurrence in playlists [2], and neural word embeddings are factorizations of pairwise cooccurrence information [3, 4]. Recent Anchor Word algorithms [5, 6] perform spectral inference on
co-occurrence statistics for inferring topic models [7, 8]. Co-occurrence statistics can be calculated
using a single parallel pass through a training corpus. While these algorithms are fast, deterministic,
and provably guaranteed, they are sensitive to observation noise and small samples, often producing
effectively useless results on real documents that present no problems for probabilistic algorithms.
We cast this general problem
Area = 0.000313
Area = 0.002602
Area = 0.000660
of learning overlapping latent
clusters as Joint-Stochastic Matrix Factorization (JSMF), a
subset of non-negative matrix
factorization that contains topic
modeling as a special case.
We explore the conditions necessary for inference from co- Figure 1: 2D visualizations show the low-quality convex hull
occurrence statistics and show found by Anchor Words [6] (left) and a better convex hull (middle)
that the Anchor Words algo- found by discovering anchor words on a rectified space (right).
rithms necessarily violate such
conditions. Then we propose a rectified algorithm that matches the performance of probabilistic
inferenceâ€”even on small and noisy datasetsâ€”without losing efficiency and provable guarantees.
Validating on both real and synthetic data, we demonstrate that our rectification not only produces
better clusters, but also, unlike previous work, learns meaningful cluster interactions.
0.05

0.05

0.04

0.04

0.03

0.03

0.02

0.02

0.01

0.01

0.02

0.015

0.01

0

0

-0.01

-0.01

-0.02

-0.02

0.005

0

-0.005

-0.03

-0.04
-0.04

-0.01

-0.03

-0.02

0

0.02

0.04

1

0.06

0.08

-0.04
-0.04

-0.02

0

0.02

0.04

0.06

0.08

-0.015
-0.02

-0.01

0

0.01

0.02

0.03

Let the matrix C represent the co-occurrence of pairs drawn from N objects: Cij is the joint probability p(X1 = i, X2 = j) for a pair of objects i and j. Our goal is to discover K latent clusters by approximately decomposing C â‰ˆ BAB T . B is the object-cluster matrix, in which each
column corresponds to a cluster and Bik = p(X = i|Z = k) is the probability of drawing an
object i conditioned on the object belonging to the cluster k; and A is the cluster-cluster matrix,
in which Akl = p(Z1 = k, Z2 = l) represents the joint probability of pairs of clusters. We
call the matrices C and A joint-stochastic (i.e., C âˆˆ J S N , A âˆˆ J S K ) due to their correspondence to joint distributions; B is column-stochastic. Example applications are shown in Table 1.
Table 1: JSMF applications, with anchor-word equivalents.
Anchor Word algorithms [5,
6] solve JSMF problems usDomain
Object
Cluster
Basis
ing a separability assumption:
Document
Word
Topic
Anchor Word
each topic contains at least
Image
Pixel
Segment
Pure Pixel
one â€œanchorâ€ word that has
Network
User
Community
Representative
non-negligible probability exLegislature Member Party/Group
Partisan
Playlist
Song
Genre
Signature Song
clusively in that topic. The algorithm uses the co-occurrence
patterns of the anchor words as a summary basis for the co-occurrence patterns of all other words.
The initial algorithm [5] is theoretically sound but unable to produce column-stochastic word-topic
matrix B due to unstable matrix inversions. A subsequent algorithm [6] fixes negative entries in B,
but still produces large negative entries in the estimated topic-topic matrix A. As shown in Figure 3,
the proposed algorithm infers valid topic-topic interactions.

2

Requirements for Factorization

In this section we review the probabilistic and statistical structures of JSMF and then define geometric structures of co-occurrence matrices required for successful factorization. C âˆˆ RN Ã—N is a
joint-stochastic matrix constructed from M training examples, each of which contain some subset
of N objects. We wish to find K  N latent clusters by factorizing C into a column-stochastic
matrix B âˆˆ RN Ã—K and a joint-stochastic matrix A âˆˆ RKÃ—K , satisfying C â‰ˆ BAB T .
Probabilistic structure. Figure 2 shows the event
space of our model. The distribution A over pairs of clusters is generated first from a stochastic process with a hyperparameter Î±. If the m-th training example contains
a total of nm objects, our model views the example as
consisting of all possible nm (nm âˆ’ 1) pairs of objects.1
For each of these pairs, cluster assignments are sampled
from the selected distribution ((z1 , z2 ) âˆ¼ A). Then an
actual object pair is drawn with respect to the corresponding cluster assignments (x1 âˆ¼ Bz1 , x2 âˆ¼ Bz2 ). Note that
this process does not explain how each training example
is generated from a model, but shows how our model understands the objects in the training examples.

Î±
Z1

X1

Z2

X2

A

Bk

nm (nm âˆ’ 1)
1â‰¤mâ‰¤M

1â‰¤kâ‰¤K

Figure 2: The JSMF
event space differs
1

from LDAâ€™s. JSMF deals only with pairwise
co-occurrence events and does not generate
observations/documents.

Following [5, 6], our model views B as a set of parameters rather than random variables.2 The
primary learning task is to estimate B; we then estimate A to recover the hyperparameter Î±. Due to
the conditional independence X1 âŠ¥ X2 | (Z1 or Z2 ), the factorization C â‰ˆ BAB T is equivalent to
XX
p(X1 , X2 |A; B) =
p(X1 |Z1 ; B)p(Z1 , Z2 |A)p(X2 |Z2 ; B).
z1

z2

Under the separability assumption, each cluster k has a basis object sk such that p(X = sk |Z =
k) > 0 and p(X = sk |Z 6= k) = 0. In matrix terms, we assume the submatrix of B comprised of
1

Due to the bag-of-words assumption, every object can pair with any other object in that example, except
itself. One implication of our work is better understanding the self-co-occurrences, the diagonal entries in the
co-occurrence matrix.
2
In LDA, each column of B is generated from a known distribution Bk âˆ¼ Dir(Î²).

2

the rows with indices S = {s1 , . . . , sK } is diagonal. As these rows form a non-negative basis for
the row space of B, the assumption implies rank+ (B) = K = rank(B).3 Providing identifiability
to the factorization, this assumption becomes crucial for inference of both B and A. Note that JSMF
factorization is unique up to column permutation, meaning that no specific ordering exists among
the discovered clusters, equivalent to probabilistic topic models (see the Appendix).
Statistical structure. Let f (Î±) be a (known) distribution of distributions from which a cluster
distribution is sampled for each training example. Saying Wm âˆ¼ f (Î±), we have M i.i.d samples
{W1 , . . . , WM } which are not directly observable. Defining the posterior cluster-cluster matrix
PM
1
T
âˆ—
T
4
Aâˆ—M = M
m=1 Wm Wm and the expectation A = E[Wm Wm ], Lemma 2.2 in [5] showed that
Aâˆ—M âˆ’â†’ Aâˆ—

as M âˆ’â†’ âˆž.

(1)
âˆ—
Cm

Denote the posterior co-occurrence for the m-th training example by
and all examples by C âˆ— .
P
M
1
âˆ—
T T
âˆ—
Then Cm
= BWm Wm
B , and C âˆ— = M
m=1 Cm . Thus
!
M
1 X
âˆ—
T
C =B
Wm Wm B T = BAâˆ—M B T .
(2)
M m=1
Denote the noisy observation for the m-th training example by Cm , and all examples by C. Let
W = [W1 |...|WM ] be a matrix of topics. We will construct Cm so that E[C|W ] is an unbiased
estimator of C âˆ— . Thus as M â†’ âˆž
C âˆ’â†’ E[C] = C âˆ— = BAâˆ—M B T âˆ’â†’ BAâˆ— B T .

(3)

Geometric structure. Though the separability assumption allows us to identify B even from the
noisy observation C, we need to throughly investigate the structure of cluster interactions. This is
because it will eventually be related to how much useful information the co-occurrence between
corresponding anchor bases contains, enabling us to best use our training data. Say DN N n is the
set of n Ã— n doubly non-negative matrices: entrywise non-negative and positive semidefinite (PSD).
Claim Aâˆ—M , Aâˆ— âˆˆ DN N K and C âˆ— âˆˆ DN N N
Proof Take any vector y âˆˆ RK . As Aâˆ—M is defined as a sum of outer-products,
y

T

Aâˆ—M y

M
X
1 X T T
1 X T
T
T
y Wm Wm
y=
(Wm y) (Wm
y) =
(non-negative) â‰¥ 0.
=
M m=1
M

(4)

Thus Aâˆ—M âˆˆ PSDK . In addition, (Aâˆ—M )kl = p(Z1 = k, Z2 = l) â‰¥ 0 for all k, l. Proving
Aâˆ— âˆˆ DN N K is analogous by the linearity of expectation. Relying on double non-negativity of
Aâˆ—M , Equation (3) implies not only the low-rank structure of C âˆ— , but also double non-negativity of
C âˆ— by a similar proof (see the Appendix).
The Anchor Word algorithms in [5, 6] consider neither double non-negativity of cluster interactions
nor its implication on co-occurrence statistics. Indeed, the empirical co-occurrence matrices collected from limited data are generally indefinite and full-rank, whereas the posterior co-occurrences
must be positive semidefinite and low-rank. Our new approach will efficiently enforce double nonnegativity and low-rankness of the co-occurrence matrix C based on the geometric property of its
posterior behavior. We will later clarify how this process substantially improves the quality of the
clusters and their interactions by eliminating noises and restoring missing information.

3

Rectified Anchor Words Algorithm

In this section, we describe how to estimate the co-occurrence matrix C from the training data, and
how to rectify C so that it is low-rank and doubly non-negative. We then decompose the rectified
C 0 in a way that preserves the doubly non-negative structure in the cluster interaction matrix.
3
4

rank+ (B) means the non-negative rank of the matrix B, whereas rank(B) means the usual rank.
PM
1
This convergence is not trivial while M
m=1 Wm â†’ E[Wm ] as M â†’ âˆž by the Central Limit Theorem.

3

Generating co-occurrence C. Let Hm be the vector of object counts for the m-th training example, and let pm = BWm where Wm is the documentâ€™s latent topic distribution. Then Hm is assumed
PN
(i)
to be a sample from a multinomial distribution Hm âˆ¼ Multi(nm , pm ) where nm = i=1 Hm , and
recall E[Hm ] = nm pm = nm BWm and Cov(Hm ) = nm diag(pm ) âˆ’ pm pTm . As in [6], we
generate the co-occurrence for the m-th example by
Cm =

T
Hm Hm
âˆ’ diag(Hm )
.
nm (nm âˆ’ 1)

(5)

The diagonal penalty in Eq. 5 cancels out the diagonal matrix term in the variance-covariance matrix,
T
making the estimator unbiased. Putting dm = nm (nm âˆ’ 1), that is E[Cm |Wm ] = d1m E[Hm Hm
]âˆ’
1
1
T
T
T
âˆ—
diag(E[H
])
=
(E[H
]E[H
]
+
Cov(H
)
âˆ’
diag(E[H
]))
=
B(W
W
)B
â‰¡
C
m
m
m
m
m
m
m
m.
dm
dm
âˆ—
Thus E[C|W ] = C by the linearity of expectation.
Rectifying co-occurrence C. While C is an unbiased estimator for C âˆ— in our model, in reality the
two matrices often differ due to a mismatch between our model assumptions and the data5 or due
to error in estimation from limited data. The computed C is generally full-rank with many negative
eigenvalues, causing a large approximation error. As the posterior co-occurrence C âˆ— must be lowrank, doubly non-negative, and joint-stochastic, we propose two rectification methods: Diagonal
Completion (DC) and Alternating Projection (AP). DC modifies only diagonal entries so that C
becomes low-rank, non-negative, and joint-stochastic; while AP enforces modifies every entry and
enforces the same properties as well as positive semi-definiteness. As our empirical results strongly
favor alternating projection, we defer the details of diagonal completion to the Appendix.
Based on the desired property of the posterior co-occurrence C âˆ— , we seek to project our estimator
C onto the set of joint-stochastic, doubly non-negative, low rank matrices. Alternating projection
methods like Dykstraâ€™s algorithm [9] allow us to project onto an intersection of finitely many convex
sets using projections onto each individual set in turn. In our setting, we consider the intersection
of three sets of symmetric N Ã— N matrices: the elementwise non-negative matrices N N N , the
normalized matrices N ORN whose entry sum is equal to 1, and the positive semi-definite matrices
with rank K, PSDN K . We project onto these three sets as follows:
P
1 âˆ’ i,j Cij T
+ T
Î PSDN K (C) = U Î›K U , Î N ORN (C) = C +
11 , Î N N N (C) = max{C, 0}.
N2
where C = U Î›U T is an eigendecomposition and Î›+
K is the matrix Î› modified so that all negative
eigenvalues and any but the K largest positive eigenvalues are set to zero. Truncated eigendecompositions can be computed efficiently, and the other projections are likewise efficient. While N N N
and N ORN are convex, PSDN K is not. However, [10] show that alternating projection with a
non-convex set still works under certain conditions, guaranteeing a local convergence. Thus iterating three projections in turn until the convergence rectifies C to be in the desired space. We will
show how to satisfy such conditions and the convergence behavior in Section 5.
Selecting basis S. The first step of the factorization is to select the subset S of objects that satisfy
the separability assumption. We want the K best rows of the row-normalized co-occurrence matrix
C so that all other rows lie nearly in the convex hull of the selected rows. [6] use the GramSchmidt process to select anchors, which computes pivoted QR decomposition, but did not utilize the
sparsity of C. To scale beyond small vocabularies, they use random projections that approximately
preserve `2 distances between rows of C. For all experiments we use a new pivoted QR algorithm
(see the Appendix) that exploits sparsity instead of using random projections, and thus preserves
deterministic inference.6
Recovering object-cluster B. After finding the set of basis objects S, we can infer each entry of
B by Bayesâ€™ rule as in [6]. Let {p(Z1 = k|X1 = i)}K
k=1 be the coefficients that reconstruct the
i-th row of C in terms of the basis rows corresponding to S. Since Bik = p(X1 = i|Z1 = k),
5

There is no reason to expect real data to be generated from topics, much less exactly K latent topics.
To effectively use random projections, it is necessary to either find proper dimensions based on multiple
trials or perform low-dimensional random projection multiple times [25] and merge the resulting anchors.
6

4

P
we can use the corpus frequencies p(X1 = i) =
j Cij to estimate Bik âˆ p(Z1 = k|X1 =
i)p(X1 = i). Thus the main task for this step is to solve simplex-constrained QPs to infer a
set of such coefficients for each object. We use an exponentiated gradient algorithm to solve the
problem similar to [6]. Note that this step can be efficiently done in parallel for each object.

Recovering cluster-cluster A.
[6] recovered A by minimizing
kC âˆ’ BAB T kF ; but the inferred
A generally has many negative
entries, failing to model the
probabilistic interaction between
topics. While we can further
project A onto the joint-stochastic
matrices, this produces a large Figure 3: The algorithm of [6] (first panel) produces negative cluster
co-occurrence probabilities. A probabilistic reconstruction alone (this
approximation error.
22.842

-7.687

0.629

-2.723

-12.888

45.021

0.000

0.000

0.000

0.000

0.114

0.000

0.002

0.024

0.004

-7.687

43.605

-4.986

-7.788

-22.930

0.000

43.086

0.000

0.000

0.000

0.000

0.115

0.010

0.007

0.017

0.629

-4.986

12.782

-5.269

-2.998

0.000

0.000

52.828

0.000

0.000

0.002

0.010

0.162

0.016

0.012

-2.723

-7.788

-5.269

19.237

-3.267

0.000

0.000

0.000

17.527

0.000

0.024

0.007

0.016

0.072

0.014

-12.888

-22.930

-2.998

-3.267

42.367

0.000

0.000

0.000

0.000

76.153

0.004

0.017

0.012

0.014

0.328

23.46
1.00
0.84
0.67
0.50
0.34
0.17
0.00

âˆ’11.23
âˆ’22.93

paper & [5], second panel) removes negative entries but has no off-

We consider an alternate recovery diagonals and does not sum to one. Trying after rectification (this
method that again leverages the paper, third panel) produces a valid joint stochastic matrix.
separability assumption. Let CSS be the submatrix whose rows and columns correspond to the
selected objects S, and let D be the diagonal submatrix BSâˆ— of rows of B corresponding to S. Then
CSS = DADT = DAD =â‡’ A = Dâˆ’1 CSS Dâˆ’1 .
(6)
This approach efficiently recovers a cluster-cluster matrix A mostly based on the co-occrrurence
information between corresponding anchor basis, and produces no negative entries due to the stability of diagonal matrix inversion. Note that the principle submatrices of a PSD matrix are also
PSD; hence, if C âˆˆ PSDN then CSS , A âˆˆ PSDK . Thus, not only is the recovered A an unbiased
estimator for Aâˆ—M , but also it is now doubly non-negative as Aâˆ—M âˆˆ DN N K after the rectification.7

4

Experimental Results

Our Rectified Anchor Words algorithm with alternating projection fixes many problems in the baseline Anchor Words algorithm [6] while matching the performance of Gibbs sampling [11] and maintaining spectral inferenceâ€™s determinism and independence from corpus size. We evaluate direct
measurement of matrix quality as well as indicators of topic utility. We use two text datasets:
NIPS full papers and New York Times news articles.8 We eliminate a minimal list of 347 English stop words and prune rare words based on tf-idf scores and remove documents with fewer
than five tokens after vocabulary curation. We also prepare two non-textual item-selection datasets:
usersâ€™ movie reviews from the Movielens 10M Dataset,9 and music playlists from the complete
Yes.com dataset.10 We perform similar vocabulary curation and document tailoring, with the exception of frequent stop-object elimination. Playlists often contain the same songs multiple times,
but users are unlikely to review the same movies more than once, so we augment the movie dataset
so that each review contains 2 Ã— (stars) number of movies based on the half-scaled rating information that varies from 0.5 stars to 5 stars. Statistics of our datasets are shown in Table 2.
We run DC 30 times for each experiment, randomly
permuting the order of objects and using the median
Dataset
M
N
Avg. Len
results to minimize the effect of different orderings.
NIPS
1,348
5k
380.5
We also run 150 iterations of AP alternating PSDN K ,
NYTimes 269,325 15k
204.9
N ORN , and N N N in turn. For probabilistic Gibbs
Movies
63,041
10k
142.8
sampling, we use the Mallet with the standard option
Songs
14,653
10k
119.2
doing 1,000 iterations. All metrics are evaluated against
the original C, not against the rectified C 0 , whereas we use B and A inferred from the rectified C 0 .
Table 2: Statistics of four datasets.

7

We later realized that essentially same approach was previously tried in [5], but it was not able to generate
a valid topic-topic matrix as shown in the middle panel of Figure 3.
8
https://archive.ics.uci.edu/ml/datasets/Bag+of+Words
9
http://grouplens.org/datasets/movielens
10
http://www.cs.cornell.edu/Ëœshuochen/lme

5

Qualitative results. Although [6] report comparable results to probabilistic algorithms for LDA,
the algorithm fails under many circumstances. The algorithm prefers rare and unusual anchor words
that form a poor basis, so topic clusters consist of the same high-frequency terms repeatedly, as
shown in the upper third of Table 3. In contrast, our algorithm with AP rectification successfully learns themes similar to the probabilistic algorithm. One can also verify that cluster interactions given in the third panel of Figure 3 explain how the five topics correlate with each other.
Similar to [12], we visualize the Table 3: Each line is a topic from NIPS (K = 5). Previous work
five anchor words in the co- simply repeats the most frequent words in the corpus five times.
occurrence space after 2D PCA
Arora et al. 2013 (Baseline)
of C. Each panel in Figure 1
neuron layer hidden recognition signal cell noise
neuron layer hidden cell signal representation noise
shows a 2D embedding of the
neuron layer cell hidden signal noise dynamic
NIPS vocabulary as blue dots and
neuron layer cell hidden control signal noise
five selected anchor words in red.
neuron layer hidden cell signal recognition noise
The first plot shows standard anThis paper (AP)
chor words and the original coneuron circuit cell synaptic signal layer activity
occurrence space. The second plot
control action dynamic optimal policy controller reinforcement
shows anchor words selected from
recognition layer hidden word speech image net
the rectified space overlaid on the
cell field visual direction image motion object orientation
original co-occurrence space. The
gaussian noise hidden approximation matrix bound examples
third plot shows the same anchor
Probabilistic LDA (Gibbs)
words as the second plot overlaid
neuron cell visual signal response field activity
on the AP-rectified space. The reccontrol action policy optimal reinforcement dynamic robot
recognition image object feature word speech features
tified anchor words provide better
hidden net layer dynamic neuron recurrent noise
coverage on both spaces, explaingaussian approximation matrix bound component variables
ing why we are able to achieve reasonable topics even with K = 5.
Rectification also produces better clusters in the non-textual movie dataset. Each cluster is notably
more genre-coherent and year-coherent than the clusters from the original algorithm. When K = 15,
for example, we verify a cluster of Walt Disney 2D Animations mostly from the 1990s and a cluster
of Fantasy movies represented by Lord of the Rings films, similar to clusters found by probabilistic
Gibbs sampling. The Baseline algorithm [6] repeats Pulp Fiction and Silence of the Lambs 15 times.
Quantitative results. We measure the intrinsic quality of inference and summarization with respect to the JSMF objectives as well as the extrinsic quality of resulting topics. Lines correspond to
four methods: â—¦ Baseline for the algorithm in the previous work [6] without any rectification, 4 DC
for Diagonal Completion,  AP for Alternating Projection, and  Gibbs for Gibbs sampling.
Anchor objects should form a good basis for theremaining objects. We measure Recovery error
PN
PK
1
i kC i âˆ’
k p(Z1 = k|X1 = i)C Sk k2 with respect to the original C matrix, not the
N
rectified matrix. AP reduces error in almost all cases and is more effective than DC. Although
we expect error to decrease as we increase the number of clusters K, reducing recovery error for
a fixed K by choosing better anchors is extremely difficult: no other subset selection algorithm
[13] decreased error by more than 0.001. A good
 matrix factorization should have small elementwise Approximation error kC âˆ’ BAB T kF . DC and AP preserve more of the information in
the original matrix C than the Baseline method, especially when K is small.11 We expect nontrivial interactions between clusters, even when wedo not explicitly model them as in [14]. Greater
PK
1
12
diagonal Dominancy K
k p(Z2 = k|Z1 = k) indicates lower correlation between clusters.
AP and Gibbs results are similar. We do not report held-out probability because we find that relative
results are determined by user-defined smoothing parameters [12, 24].

PK
1
Specificity K
k KL (p(X|Z = k)kp(X)) measures how much each cluster is distinct from
the corpus distribution. When anchors produce a poor basis, the conditional distribution of clus11
In the NYTimes corpus, 10âˆ’2 is a large error: each element is around 10âˆ’9 due to the number of normalized entries.
12
Dominancy in Songs corpus lacks any Baseline results at K > 10 because dominancy is undefined if an
algorithm picks a song that occurs at most once in each playlist as a basis object. In this case, the original
construction of CSS , and hence of A, has a zero diagonal element, making dominancy NaN.

6

Nips
Recovery

Approximation

â—

Dominancy
1.0

â—

â—

â—

â—

Specificity
â—

â— â—

â—

Dissimilarity

â—

3

0.8
0.05

0.10

â—

Coherence
âˆ’160

15

0.15
0.06

â—

â—

â—

10 15

25

Category

AP

â—

5

10 15

1

0.2

0

5
â—

â—

â—

0.00

50 75100

0.4

â—
â—

â—

5

â—

â—

â—

âˆ’240

â—

â—
â—

â—

25

50 75100

5

10 15

25

â—

50 75100

5

10 15

â—

â—

â— â—

â—

â—

0

25

50 75100

â—

â—

â—

â—

â—

â—

â—

5

Baseline
DC

â—
â—

0.03

â—

âˆ’200

2

0.6

0.05

â—

â—

10

â—

0.04

â—

Gibbs

âˆ’280
âˆ’320

10 15

25

50 75100

5

10 15

25

50 75100

NYTimes
Recovery

0.25

Approximation
1.0

â—

Specificity

â—

0.20

0.20
0.15

Dominancy
â—

0.8
â—

â—

â—

â—

0.05

â—

5

â—

10 15 25

â—

â— â—â—

15

5

â—

â— â—

1

10 15 25

â—

â—
â—

0
5

10 15 25

50 75100150

â—

Baseline
DC

â—

â—

â—â—

50 75100150

AP
â—â—

â—
â— â—

5

0.2

50 75100150

Category

â—

â—

â—

â—

âˆ’350

â—

â—

0.00

â—

â—

â—

10

0.4

â—
â—

âˆ’300

2
â—

â—

â— â—

3

â—

0.05

Coherence

â—

â—

â—

0.6

â—

0.10

0.10

â— â—â—

â—

â—

0.15

Dissimilarity

Gibbs

âˆ’400

â—

â—

â—

â— â—â—

â—

5

10 15 25

50 75100150

5

10 15 25

50 75100150

5

10 15 25

50 75100150

Movies
Recovery

Approximation

Dominancy
1.00

â—

â—

â—

â—

â—

â— â—

Specificity
â—

â—

â—

Dissimilarity

Coherence
âˆ’120

4

0.75

10
0.08

â—

â—

â— â—
â—

â—

0.06

5

â—

â—
â—
â—

0.04

â—

10 15

25

â—

50 75100

5

1

â—

â—

â—

0.25
0

10 15

25

50 75100

5

10 15

25

50 75100

5

â— â—

â—

â—

â—

10 15

25

â—

â—

0

50 75100

Gibbs

âˆ’210

â—

0
5

Baseline
DC

â—

â—

â—

âˆ’180

2

0.50

AP

â—

10
â—

Category

â—

âˆ’150

3

â—

5

â—

15

0.10

â—

5

â—

â—

10 15

â—

â— â—

25

â—

â—

âˆ’240
50 75100

5

10 15

25

50 75100

Songs
Recovery
0.150

Approximation

â—

Dominancy

1.0

â—

Specificity

4

0.8

â—

â—

â—
â—
â—

â—

0.075

â—

â—

0.005

0.6

â—
â—

0.050
5

10 15

25

50 75100

5

10 15

25

â—

â—

50 75100

10

âˆ’500

Category

AP

0.4

0
5

10 15

â—

25

50 75100

â—
â—

â—

5

â—

â—

10 15

â— â—

25

â—

â—

â—
â—

â—

DC
Gibbs

âˆ’700

â—

0
50 75100

â—

Baseline

â—
â—

5

1

0.000

âˆ’300

2

â—

â—
â—

Coherence

15

â—

3

0.010

â—

0.100

Dissimilarity

20

5

0.015

0.125

â—

5

â—

â—

10 15

â—

â—

â—
â—

25

50 75100

5

10 15

25

50 75100

Figure 4: Experimental results on real dataset. The x-axis indicates logK where K varies by 5 up to 25 topics
and by 25 up to 100 or 150 topics. Whereas the Baseline algorithm largely fails with small K and does not infer
quality B and A even with large K, Alternating Projection (AP) not only finds better basis vectors (Recovery),
but also shows stable and comparable behaviors to probabilistic inference (Gibbs) in every metric.

ters given objects becomes uniform, making p(X|Z) similar to p(X). Inter-topic Dissimilarity
counts the average number of objects in each cluster that do not occur in any other clusterâ€™s top
20 objects. Our experiments validate that AP and Gibbs yield comparably specific and distinct
topics, while Baseline and DC simply repeat the corpus distribution as in Table 3. Coherence
PK PâˆˆT opk
D2 (x1 ,x2 )+ 
1
penalizes topics that assign high probability (rank > 20) to
k
x1 6=x2 log
K
D1 (x2 )
words that do not occur together frequently. AP produces results close to Gibbs sampling, and
far from the Baseline and DC. While this metric correlates with human evaluation of clusters [15]
â€œworseâ€ coherence can actually be better because the metric does not penalize repetition [12].
In semi-synthetic experiments [6] AP matches Gibbs sampling and outperforms the Baseline, but
the discrepancies in topic quality metrics are smaller than in the real experiments (see Appendix).
We speculate that semi-synthetic data is more â€œwell-behavedâ€ than real data, explaining why issues
were not recognized previously.

5

Analysis of Algorithm

Why does AP work? Before rectification, diagonals of the empirical C matrix may be far from
correct. Bursty objects yield diagonal entries that are too large; extremely rare objects that occur
at most once per document yield zero diagonals. Rare objects are problematic in general: the corresponding rows in the C matrix are sparse and noisy, and these rows are likely to be selected by
the pivoted QR. Because rare objects are likely to be anchors, the matrix CSS is likely to be highly
diagonally dominant, and provides an uninformative picture of topic correlations. These problems
are exacerbated when K is small relative to the effective rank of C, so that an early choice of a poor
anchor precludes a better choice later on; and when the number of documents M is small, in which
case the empirical C is relatively sparse and is strongly affected by noise. To mitigate this issue,
[24] run exhaustive grid search to find document frequency cutoffs to get informative anchors. As
7

model performance is inconsistent for different cutoffs and search requires cross-validation for each
case, it is nearly impossible to find good heuristics for each dataset and number of topics.
Fortunately, a low-rank PSD matrix cannot have too many diagonally-dominant rows, since this violates the low rank property. Nor can it have diagonal entries that are small relative to off-diagonals,
since this violates positive semi-definiteness. Because the anchor word assumption implies that
non-negative rank and ordinary rank are the same, the AP algorithm ideally does not remove the
information we wish to learn; rather, 1) the low-rank projection in AP suppresses the influence of
small numbers of noisy rows associated with rare words which may not be well correlated with the
others, and 2) the PSD projection in AP recovers missing information in diagonals. (As illustrated
in the Dominancy panel of the Songs corpus in Figure 4, AP shows valid dominancies even after
K > 10 in contrast to the Baseline algorithm.)
Why does AP converge? AP enjoys local linear convergence [10] if 1) the initial C is near the
convergence point C 0 , 2) PSDN K is super-regular at C 0 , and 3) strong regularity holds at C 0 . For
the first condition, recall that we rectified C 0 by pushing C toward C âˆ— , which is the ideal convergence
point inside the intersection. Since C â†’ C âˆ— as shown in (5), C is close to C 0 as desired.The proxregular sets13 are subsets of super-regular sets, so prox-regularity of PSDN K at C 0 is sufficient for
the second condition. For permutation invariant M âŠ‚ RN , the spectral set of symmetric matrices
is defined as Î»âˆ’1 (M) = {X âˆˆ SN : (Î»1 (X), . . . , Î»N (X)) âˆˆ M}, and Î»âˆ’1 (M) is prox-regular
if and only if M is prox-regular [16, Th. 2.4]. Let M be {x âˆˆ R+
n : |supp(x)| = K}. Since each
element in M has exactly K positive components and all others are zero, Î»âˆ’1 (M) = PSDN K . By
the definition of M and K < N , PM is locally unique almost everywhere, satisfying the second
condition almost surely. (As the intersection of the convex set PSDN and the smooth manifold of
rank K matrices, PSDN K is a smooth manifold almost everywhere.)
Checking the third condition a priori is challenging, but we expect noise in the empirical C to
prevent an irregular solution, following the argument of Numerical Example 9 in [10]. We expect
AP to converge locally linearly and we can verify local convergence of AP in practice. Empirically,
the ratio of average distances between two iterations are always â‰¤ 0.9794 on the NYTimes dataset
(see the Appendix), and other datasets were similar. Note again that our rectified C 0 is a result of
pushing the empirical C toward the ideal C âˆ— . Because approximation factors of [6] are all computed
based on how far C and its co-occurrence shape could be distant from C âˆ— â€™s, all provable guarantees
of [6] hold better with our rectified C 0 .

6

Related and Future Work

JSMF is a specific structure-preserving Non-negative Matrix Factorization (NMF) performing spectral inference. [17, 18] exploit a similar separable structure for NMF problmes. To tackle hyperspectral unmixing problems, [19, 20] assume pure pixels, a separability-equivalent in computer vision.
In more general NMF without such structures, RESCAL [21] studies tensorial extension of similar
factorization and SymNMF [22] infers BB T rather than BAB T . For topic modeling, [23] performs
spectral inference on third moment tensor assuming topics are uncorrelated.
As the core of our algorithm is to rectify the input co-occurrence matrix, it can be combined with
several recent developments. [24] proposes two regularization methods for recovering better B.
[12] nonlinearly projects co-occurrence to low-dimensional space via t-SNE and achieves better
anchors by finding the exact anchors in that space. [25] performs multiple random projections to
low-dimensional spaces and recovers approximate anchors efficiently by divide-and-conquer strategy. In addition, our work also opens several promising research directions. How exactly do anchors
found in the rectified C 0 form better bases than ones found in the original space C? Since now the
topic-topic matrix A is again doubly non-negative and joint-stochastic, can we learn super-topics in
a multi-layered hierarchical model by recursively applying JSMF to topic-topic co-occurrence A?

Acknowledgments
This research is supported by NSF grant HCC:Large-0910664. We thank Adrian Lewis for valuable
discussions on AP convergence.
13

A set M is prox-regular if PM is locally unique.

8

References
[1] Alan Mislove, Bimal Viswanath, Krishna P. Gummadi, and Peter Druschel. You are who you know: Inferring user profiles in Online Social Networks. In Proceedings of the 3rd ACM International Conference
of Web Search and Data Mining (WSDMâ€™10), New York, NY, February 2010.
[2] Shuo Chen, J. Moore, D. Turnbull, and T. Joachims. Playlist prediction via metric embedding. In ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 714â€“722, 2012.
[3] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. GloVe: Global vectors for word representation. In EMNLP, 2014.
[4] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In NIPS, 2014.
[5] S. Arora, R. Ge, and A. Moitra. Learning topic models â€“ going beyond SVD. In FOCS, 2012.
[6] Sanjeev Arora, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu, and
Michael Zhu. A practical algorithm for topic modeling with provable guarantees. In ICML, 2013.
[7] T. Hofmann. Probabilistic latent semantic analysis. In UAI, pages 289â€“296, 1999.
[8] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, pages
993â€“1022, 2003. Preliminary version in NIPS 2001.
[9] JamesP. Boyle and RichardL. Dykstra. A method for finding projections onto the intersection of convex
sets in Hilbert spaces. In Advances in Order Restricted Statistical Inference, volume 37 of Lecture Notes
in Statistics, pages 28â€“47. Springer New York, 1986.
[10] Adrian S. Lewis, D. R. Luke, and Jrme Malick. Local linear convergence for alternating and averaged
nonconvex projections. Foundations of Computational Mathematics, 9:485â€“513, 2009.
[11] T. L. Griffiths and M. Steyvers. Finding scientific topics. Proceedings of the National Academy of
Sciences, 101:5228â€“5235, 2004.
[12] Moontae Lee and David Mimno. Low-dimensional embeddings for interpretable anchor-based topic
inference. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pages 1319â€“1328. Association for Computational Linguistics, 2014.
[13] Mary E Broadbent, Martin Brown, Kevin Penner, I Ipsen, and R Rehman. Subset selection algorithms:
Randomized vs. deterministic. SIAM Undergraduate Research Online, 3:50â€“71, 2010.
[14] D. Blei and J. Lafferty. A correlated topic model of science. Annals of Applied Statistics, pages 17â€“35,
2007.
[15] David Mimno, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. Optimizing
semantic coherence in topic models. In EMNLP, 2011.
[16] A. Daniilidis, A. S. Lewis, J. Malick, and H. Sendov. Prox-regularity of spectral functions and spectral
sets. Journal of Convex Analysis, 15(3):547â€“560, 2008.
[17] Christian Thurau, Kristian Kersting, and Christian Bauckhage. Yes we can: simplex volume maximization
for descriptive web-scale matrix factorization. In CIKMâ€™10, pages 1785â€“1788, 2010.
[18] Abhishek Kumar, Vikas Sindhwani, and Prabhanjan Kambadur. Fast conical hull algorithms for nearseparable non-negative matrix factorization. CoRR, pages â€“1â€“1, 2012.
[19] Jos M. P. Nascimento, Student Member, and Jos M. Bioucas Dias. Vertex component analysis: A fast
algorithm to unmix hyperspectral data. IEEE Transactions on Geoscience and Remote Sensing, pages
898â€“910, 2005.
[20] CeÌcile Gomez, H. Le Borgne, Pascal Allemand, Christophe Delacourt, and Patrick Ledru. N-FindR
method versus independent component analysis for lithological identification in hyperspectral imagery.
International Journal of Remote Sensing, 28(23):5315â€“5338, 2007.
[21] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning on
multi-relational data. In Proceedings of the 28th International Conference on Machine Learning (ICML11), ICML, pages 809â€“816. ACM, 2011.
[22] Da Kuang, Haesun Park, and Chris H. Q. Ding. Symmetric nonnegative matrix factorization for graph
clustering. In SDM. SIAM / Omnipress, 2012.
[23] Anima Anandkumar, Dean P. Foster, Daniel Hsu, Sham Kakade, and Yi-Kai Liu. A spectral algorithm
for latent Dirichlet allocation. In Advances in Neural Information Processing Systems 25: 26th Annual
Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December
3-6, 2012, Lake Tahoe, Nevada, United States., pages 926â€“934, 2012.
[24] Thang Nguyen, Yuening Hu, and Jordan Boyd-Graber. Anchors regularized: Adding robustness and
extensibility to scalable topic-modeling algorithms. In Association for Computational Linguistics, 2014.
[25] Tianyi Zhou, Jeff A Bilmes, and Carlos Guestrin. Divide-and-conquer learning by anchoring a conical
hull. In Advances in Neural Information Processing Systems 27, pages 1242â€“1250. 2014.

9

