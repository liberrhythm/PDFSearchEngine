b-bit Marginal Regression
Ping Li
Department of Statistics and Biostatistics
Department of Computer Science
Rutgers University
pingli@stat.rutgers.edu

Martin Slawski
Department of Statistics and Biostatistics
Department of Computer Science
Rutgers University
martin.slawski@rutgers.edu

Abstract
We consider the problem of sparse signal recovery from m linear measurements
quantized to b bits. b-bit Marginal Regression is proposed as recovery algorithm.
We study the question of choosing b in the setting of a given budget of bits B =
m Â· b and derive a single easy-to-compute expression characterizing the trade-off
between m and b. The choice b = 1 turns out to be optimal for estimating the unit
vector corresponding to the signal for any level of additive Gaussian noise before
quantization as well as for adversarial noise. For b â‰¥ 2, we show that Lloyd-Max
quantization constitutes an optimal quantization scheme and that the norm of the
signal can be estimated consistently by maximum likelihood by extending [15].

1 Introduction
Consider the common compressed sensing (CS) model
yi = hai , xâˆ— i + ÏƒÎµi , i = 1, . . . , m, or equivalently

m,n
m
m
n
y = Axâˆ— + ÏƒÎµ, y = (yi )m
i=1 , A = (Aij )i,j=1 , {ai = (Aij )j=1 }i=1 , Îµ = (Îµi )i=1 ,

(1)

where the {Aij } and the {Îµi } are i.i.d. N (0, 1) (i.e. standard Gaussian) random variables, the latter
of which will be referred to by the term â€œadditive noiseâ€ and accordingly Ïƒ > 0 as â€œnoise levelâ€, and
xâˆ— âˆˆ Rn is the signal of interest to be recovered given (A, y). Let s = kxâˆ— k0 := |S(xâˆ— )|, where
S(xâˆ— ) = {j : |xâˆ—j | > 0}, be the â„“0 -norm of xâˆ— (i.e. the cardinality of its support S(xâˆ— )). One of the
celebrated results in CS is that accurate recovery of xâˆ— is possible as long as m & s log n, and can
be carried out by several computationally tractable algorithms e.g. [3, 5, 21, 26, 29].
Subsequently, the concept of signal recovery from an incomplete set (m < n) of linear measurements was developed further to settings in which only coarsely quantized versions of such linear
measurements are available, with the extreme case of single-bit measurements [2, 8, 11, 22, 23, 28,
16]. More generally, one can think of b-bit measurements, b âˆˆ {1, 2, . . .}. Assuming that one is free
to choose b given a fixed budget of bits B = m Â· b gives rise to a trade-off between m and b. An
optimal balance of these two quantities minimizes the error in recovering the signal. Such optimal
trade-off depends on the quantization scheme, the noise level, and the recovery algorithm. This
trade-off has been considered in previous CS literature [13]. However, the analysis therein concerns
an oracle-assisted recovery algorithm equipped with knowledge of S(xâˆ— ) which is not fully realistic.
In [9] a specific variant of Iterative Hard Thresholding [1] for b-bit measurements is considered. It is
shown via numerical experiments that choosing b â‰¥ 2 can in fact achieve improvements over b = 1
at the level of the total number of bits required for approximate signal recovery. On the other hand,
there is no analysis supporting this observation. Moreover, the experiments in [9] only concern a
noiseless setting. Another approach is to treat quantization as additive error and to perform signal
recovery by means of variations of recovery algorithms for infinite-precision CS [10, 14, 18]. In this
line of research, b is assumed to be fixed and a discussion of the aforementioned trade-off is missing.
In the present paper we provide an analysis of compressed sensing from b-bit measurements using a
specific approach to signal recovery which we term b-bit Marginal Regression. This approach builds
on a method for one-bit compressed sensing proposed in an influential paper by Plan and Vershynin
[23] which has subsequently been refined in several recent works [4, 24, 28]. As indicated by the
name, b-bit Marginal Regression can be seen as a quantized version of Marginal Regression, a simple
1

yet surprisingly effective approach to support recovery that stands out due to its low computational
cost, requiring only a single matrix-vector multiplication and a sorting operation [7]. Our analysis
yields a precise characterization of the above trade-off involving m and b in various settings. It
turns out that the choice b = 1 is optimal for recovering the normalized signal xâˆ—u = xâˆ— /kxâˆ— k2 ,
under additive Gaussian noise as well as under adversarial noise. It is shown that the choice b =
2 additionally enables one to estimate kxâˆ— k2 , while being optimal for recovering xâˆ—u for b â‰¥ 2.
Hence for the specific recovery algorithm under consideration, it does not pay off to take b > 2.
Furthermore, once the noise level is significantly high, b-bit Marginal Regression is empirically
shown to perform roughly as good as several alternative recovery algorithms, a finding suggesting
that in high-noise settings taking b > 2 does not pay off in general. As an intermediate step in our
analysis, we prove that Lloyd-Max quantization [19, 20] constitutes an optimal b-bit quantization
scheme in the sense that it leads to a minimization of an upper bound on the reconstruction error.
Notation: We use [d] = {1, . . . , d} and S(x) for the support of x âˆˆ Rn . x âŠ™ xâ€² = (xj Â· xâ€²j )nj=1 .
I(P ) is the indicator function of expression P . The symbol âˆ means â€œup to a positive universal
constantâ€. Supplement: Proofs and additional experiments can be found in the supplement.

2 From Marginal Regression to b-bit Marginal Regression
Some background on Marginal Regression. It is common to perform sparse signal recovery by
solving an optimization problem of the form
1
Î³
min
ky âˆ’ Axk22 + P (x), Î³ â‰¥ 0,
(2)
x 2m
2
where P is a penalty term encouraging sparse solutions. Standard choices for P are P (x) = kxk0 ,
which is computationally not feasible in general, its convex relaxation P (x) = kxk1 or non-convex
penalty terms like SCAD or MCP that are more amenable to optimization than the â„“0 -norm [27].
Alternatively P can as well be used to enforce a constraint by setting P (x) = Î¹C (x), where Î¹C (x) =
0 if x âˆˆ C and +âˆž otherwise, with C = {x âˆˆ Rn : kxk0 â‰¤ s} or C = {x âˆˆ Rn : kxk1 â‰¤ r} being
standard choices. Note that (2) is equivalent to the optimization problem
Î³
AâŠ¤ y
1 AâŠ¤ A
x + P (x), where Î· =
.
min âˆ’ hÎ·, xi + xâŠ¤
x
2
m
2
m
Replacing AâŠ¤ A/m by E[AâŠ¤ A/m] = I (recall that the entries of A are i.i.d. N (0, 1)), we obtain
1
Î³
AâŠ¤ y
min âˆ’ hÎ·, xi + kxk22 + P (x), Î· =
,
(3)
x
2
2
m
which tends to be much simpler to solve than (2) as the first two terms are separable in the components of x. For the choices of P mentioned above, we obtain closed form solutions:
P (x) = kxk0 : x
bj = Î·j I(|Î·j | â‰¥ Î³ 1/2 )
P (x) = kxk1 : x
bj = (|Î·j | âˆ’ Î³)+ sign(Î·j ),
P (x) = Î¹x:kxk0 â‰¤s : x
bj = Î·j I(|Î·j | â‰¥ |Î·(s) |) P (x) = Î¹x:kxk1 â‰¤r : x
bj = (|Î·j | âˆ’ Î³ âˆ— )+ sign(Î·j ) (4)

for j âˆˆ [n], where + denotes the positive part and |Î·(s) | is the sth largest entry in Î· in absolute
Pn
magnitude and Î³ âˆ— = min{Î³ â‰¥ 0 : j=1 (|Î·j | âˆ’ Î³)+ â‰¤ r}. In other words, the estimators are hardrespectively soft-thresholded versions of Î·j = AâŠ¤
j y/m which are essentially equal to the univariate
2
âˆ’1
(or marginal) regression coefficients Î¸j = AâŠ¤
y/kA
)),
j k2 in the sense that Î·j = Î¸j (1 + OP (m
j
j âˆˆ [n], hence the term â€œmarginal regressionâ€. In the literature, it is the estimator in the left half of
(4) that is popular [7], albeit as a means to infer the support of xâˆ— rather than xâˆ— itself. Under (2) the
performance with respect to signal recovery can still be reasonable in view of the statement below.
Proposition 1. Consider model (1) with xâˆ— 6= 0 and the Marginal Regression estimator x
b defined
component-wise by x
bj = Î·j I(|Î·j | â‰¥ |Î·(s) |), j âˆˆ [n], where Î· = AâŠ¤ y/m. Then there exists positive
constants c, C > 0 such that with probability at least 1 âˆ’ cnâˆ’1
r
kxâˆ— k2 + Ïƒ s log n
kb
x âˆ’ xâˆ— k2
â‰¤C
.
(5)
kxâˆ— k2
kxâˆ— k2
m
In comparison,pthe relative â„“2 -error of more sophisticated methods like the lasso scales as
O({Ïƒ/kxâˆ— k2 } s log(n)/m) which is comparable to (5) once Ïƒ is of the same order of magnitude as kxâˆ— k2 . Marginal Regression can also be interpreted as a single projected gradient iteration
2

from 0 for problem (2) with P = Î¹x:kxk0 â‰¤s . Taking more than one projected gradient iteration gives
rise to a popular recovery algorithm known as Iterative Hard Thresholding (IHT, [1]).
Compressed sensing with non-linear observations and the method of Plan & Vershynin. As a
generalization of (1) one can consider measurements of the form
yi = Q(hai , xâˆ— i + ÏƒÎµi ), i âˆˆ [m]
(6)

for some map Q. Without loss generality, one may assume that kxâˆ— k2 = 1 as long as xâˆ— 6= 0 (which
is assumed in the sequel) by defining Q accordingly. Plan and Vershynin [23] consider the following
optimization problem for recovering xâˆ— , and develop a framework for analysis that covers even more
general measurement models than (6). The proposed estimator minimizes
min
âˆš âˆ’ hÎ·, xi ,
x:kxk2 â‰¤1,kxk1 â‰¤ s

Î· = AâŠ¤ y/m.

(7)

âˆš
Note that the constraint set {x : kxk2 â‰¤ 1, kxk1 â‰¤ s} contains {x : kxk2 â‰¤ 1, kxk0 â‰¤ s}. The
authors prefer the former because it is suited for approximately sparse signals as well and second
because it is convex. However, the optimization problem with sparsity constraint is easy to solve:
min

x:kxk2 â‰¤1,kxk0 â‰¤s

âˆ’ hÎ·, xi ,

Î· = AâŠ¤ y/m.

(8)

Lemma 1. The solution of problem (8) is given by x
b=x
e/ke
xk2 , x
ej = Î·j I(|Î·j | â‰¥ |Î·(s) |), j âˆˆ [n].

While this is elementary we state it as a separate lemma as there has been some confusion in the existing literature. In [4] the same solution is obtained after (unnecessarily) convexifying the constraint
set, which yields the unit ball of the so-called s-support norm. In [24] a family of concave penalty
terms including the SCAD and MCP is proposed in place of the cardinality constraint. However, in
light of Lemma 1, the use of such penalty terms lacks motivation.

The minimization problem (8) is essentially that of Marginal Regression (3) with P = Î¹x:kxk0 â‰¤s , the
only difference being that the norm of the solution is fixed to one. Note that the Marginal Regression
estimator is equi-variant w.r.t. re-scaling of y, i.e. for a Â· y with a > 0, x
b changes to ab
x. In addition,
let Î±, Î² > 0 and define x
b(Î±) and x
b[Î²] as the minimizers of the optimization problems
Î±
min âˆ’ hÎ·, xi + kxk22 ,
min
âˆ’ hÎ·, xi .
(9)
x:kxk0 â‰¤s
2
x:kxk2 â‰¤Î²,kxk0 â‰¤s
It is not hard to verify that x
b(Î±)/kb
x(Î±)k2 = x
b[Î²]/kb
x[Î²]k2 = x
b[1]. In summary, for estimating the
direction xâˆ—u = xâˆ— /kxâˆ— k2 it does not matter if a quadratic term in the objective or an â„“2 -norm constraint is used. Moreover, estimation of the â€™scaleâ€™ Ïˆ âˆ— = kxâˆ— k2 and the direction can be separated.
Adopting the framework in [23], we provide a straightforward bound on the â„“2 -error of x
b minimizing
(8). To this end we define two quantities which will be of central interest in subsequent analysis.
Î» = E[g Î¸(g)], g âˆ¼ N (0, 1), where Î¸ is defined by E[y1 |a1 ] = Î¸(ha1 , xâˆ— i)
p
(10)
Î¨ = inf{C > 0 : P{max1â‰¤jâ‰¤n |Î·j âˆ’ E[Î·j ]| â‰¤ C log(n)/m} â‰¥ 1 âˆ’ 1/n.}.

The quantity Î» concerns the deterministic part of the analysis as it quantifies the distortion of the
linear measurements under the map Q, while Î¨ is used to deal with the stochastic part. The definition
of Î¨ is based on the usual tail bound for the maximum of centered sub-Gaussian random variables.
In fact, as long as Q has bounded range, Gaussianity of the {Aij } implies that the {Î·j âˆ’ E[Î·j ]}nj=1
are zero-mean sub-Gaussian. Accordingly, the constant Î¨ is proportional to the sub-Gaussian norm
of the {Î·j âˆ’ E[Î·j ]}nj=1 , cf. [25].

Proposition 2. Consider model (6) s.t. kxâˆ— k2 = 1 and (10). Suppose that Î» > 0 and denote by x
b
the minimizer of (8). Then with probability at least 1 âˆ’ 1/n, it holds that
r
âˆš Î¨ s log n
âˆ—
kx âˆ’ x
bk2 â‰¤ 2 2
.
(11)
Î»
m
So far s has been assumed to be known. If that is not the case, s can be estimated as follows.
p
b as
Proposition 3. In the setting of Proposition 2, consider sb = |{j : |Î·j | > Î¨ log(n)/m}| and x
the minimizer of (8) with s replaced by sb. Then with probability at least 1 âˆ’ 1/n, S(b
x) âŠ† S(xâˆ— )
(i.e. no false positive selection). Moreover, if
p
minâˆ— |xâˆ—j | > (2Î¨/Î») log(n)/m, one has S(b
x) = S(xâˆ— ).
(12)
jâˆˆS(x )

3

b-bit Marginal Regression. b-bit quantized measurements directly fit into the non-linear observation model (6). Here the map Q represents a quantizer that partitions R+ into K = 2bâˆ’1 bins
âŠ¤
{Rk }K
(in increasing order) and t0 = 0,
k=1 given by distinct thresholds t = (t1 , . . . , tKâˆ’1 )
tK = +âˆž such that R1 = [t0 , t1 ), . . . , RK = [tKâˆ’1 , tK ). Each bin is assigned a distinct representative from M = {Âµ1 , . . . , ÂµK } (in increasing order) so that Q : R â†’ âˆ’M âˆª M is defined by
PK
z 7â†’ Q(z) = sign(z) k=1 Âµk I(|z| âˆˆ Rk ). Expanding model (6) accordingly, we obtain
PK
yi = sign(hai , xâˆ— i + ÏƒÎµi ) k=1 Âµk I( |(hai , xâˆ— i + ÏƒÎµi )| âˆˆ Rk )
P
âˆ—
âˆ—
= sign(hai , xâˆ—u i + Ï„ Îµi ) K
k=1 Âµk I( |(hai , xu i + Ï„ Îµi )| âˆˆ Rk /Ïˆ ), i âˆˆ [m],

where Ïˆ âˆ— = kxâˆ— k2 , xâˆ—u = xâˆ— /Ïˆ âˆ— and Ï„ = Ïƒ/Ïˆ âˆ— . Thus the scale Ïˆ âˆ— of the signal can be absorbed
into the definition of the bins respectively thresholds which should be proportional to Ïˆ âˆ— . We may
thus again fix Ïˆ âˆ— = 1 and in turn xâˆ— = xâˆ—u , Ïƒ = Ï„ w.l.o.g. for the analysis below. Estimation of Ïˆ âˆ—
separately from xâˆ—u will be discussed in an extra section.

3 Analysis
In this section we study in detail the central question of the introduction. Suppose we have a fixed
budget B of bits available and are free to choose the number of measurements m and the number
of bits per measurement b subject to B = m Â· b such that the â„“2 -error kb
x âˆ’ xâˆ— k2 of b-bit Marginal
Regression is as small as possible. What is the optimal choice of (m, b)? In order to answer this
question, let us go back to the error bound (11). That bound applies to b-bit Marginal Regression for
any choice of b and varies with Î» = Î»b and Î¨ = Î¨b , both of which additionally depend on Ïƒ, the
choice of the thresholds t and the representatives Âµ. It can be shown that the dependence of (11) on
the ratio Î¨/Î» is tight asymptotically as m â†’ âˆž. Hence it makes sense to compare two different
â€²
choices b and
âˆšb in terms of the ratio of â„¦b = Î¨b /Î»b and â„¦bâ€² = Î¨bâ€² /Î»bâ€² . Since the bound (11)
decays with m, for bâ€² -bit measurements, bâ€² > b, to improve
p over b-bit measurements with respect
â€²
to the total #bits used, it is then required that â„¦b /â„¦b > bâ€² /b. The route to be taken is thus as
follows: we first derive expressions for Î»b and Î¨b and then minimize the resulting expression for â„¦b
w.r.t. the free parameters t and Âµ. We are then in position to compare â„¦b /â„¦bâ€² for b 6= bâ€² .
Evaluating Î»b = Î»b (t, Âµ). Below, âŠ™ denotes the entry-wise multiplication between vectors.
Lemma 2. We have Î»b (t, Âµ) = hÎ±(t), E(t) âŠ™ Âµi /(1 + Ïƒ 2 ), where
âŠ¤

Î±(t) = (Î±1 (t), . . . , Î±K (t)) , Î±k (t) = P {|e
g | âˆˆ Rk (t)} , e
g âˆ¼ N (0, 1 + Ïƒ 2 ), k âˆˆ [K],

E(t) = (E1 (t), . . . , EK (t))âŠ¤ , Ek (t) = E[e
g|e
g âˆˆ Rk (t)], e
g âˆ¼ N (0, 1 + Ïƒ 2 ), k âˆˆ [K].

Evaluating Î¨b = Î¨b (t, Âµ). Exact evaluation proves to be difficult. We hence resort to an analytically more tractable approximation which is still sufficiently accurate as confirmed by experiments.
p
Lemma 3. As |xâˆ—j | â†’ 0, j = 1, . . . , n, and as m â†’ âˆž, we have Î¨b (t, Âµ) âˆ hÎ±(t), Âµ âŠ™ Âµi.

Note that the proportionality constant (not depending on b) in front of the given expression does not
need to be known as it cancels out when computing ratios â„¦b /â„¦bâ€² . The asymptotics |xâˆ—j | â†’ 0, j âˆˆ
[n], is limiting but still makes sense for s growing with n (recall that we fix kxâˆ— k2 = 1 w.l.o.g.).

Optimal choice of t and Âµ. It turns that the optimal choice of (t, Âµ) minimizing Î¨b /Î»b coincides
with the solution of an instance of the classical Lloyd-Max quantization problem [19, 20] stated
below. Let h be a random variable with finite variance and Q the quantization map from above.
PK
(13)
min E[{h âˆ’ Q(h; t, Âµ)}2 ] = min E[{h âˆ’ sign(h) k=1 Âµk I(|h| âˆˆ Rk (t) )}2 ].
t,Âµ

t,Âµ

Problem (13) can be seen as a one-dimensional k-means problem at the population level, and it is
solved in practice by an alternating scheme similar to that used for k-means. For h from a logconcave distribution (e.g. Gaussian) that scheme can be shown to deliver the global optimum [12].
Theorem 1. Consider the minimization problem mint,Âµ Î¨b (t, Âµ)/Î»b (t, Âµ). Its minimizer (tâˆ— , Âµâˆ— )
equals that of the Lloyd-Max problem (13) for h âˆ¼ N (0, 1 + Ïƒ 2 ). Moreover,
p
â„¦b (tâˆ— , Âµâˆ— ) = Î¨b (tâˆ— , Âµâˆ— )/Î»b (tâˆ— , Âµâˆ— ) âˆ (Ïƒ 2 + 1)/Î»b,0 (tâˆ—0 , Âµâˆ—0 ),
where Î»b,0 (tâˆ—0 , Âµâˆ—0 ) denotes the value of Î»b for Ïƒ = 0 evaluated at (tâˆ—0 , Âµâˆ—0 ), the choice of (t, Âµ)
minimizing â„¦b for Ïƒ = 0.
4

Regarding the choice of (t, Âµ) the result of Theorem 1 may not come as a suprise as the entries of y
are i.i.d. N (0, 1 + Ïƒ 2 ). It is less immediate though that this specific choice can also be motivated
as the one leading to the minimization of the error bound (11). Furthermore, Theorem 1 implies
that the relative performance of b- and bâ€² -bit measurements does not depend on Ïƒ as long as the
respective optimal choice of (t, Âµ) is used, which requires Ïƒ to be known. Theorem 1 provides
an explicit expression for â„¦b that is straightforward to compute. The following table lists ratios
â„¦b /â„¦bâ€² for selected values of b and bâ€² .
â„¦b /â„¦bâ€² :
required for bâ€² â‰« b:

b = 1, bâ€² = 2
1.178
âˆš
2 â‰ˆ 1.414

b = 2, bâ€² = 3
1.046
p
3/2 â‰ˆ 1.225

b = 3, bâ€² = 4
1.013
p
4/3 â‰ˆ 1.155

These figures suggests that the smaller b, the better the performance for a given budget of bits B.
Beyond additive noise. Additive Gaussian noise is perhaps the most studied form of perturbation,
but one can of course think of numerous other mechanisms whose effect can be analyzed on the
basis of the same scheme used for additive noise as long as it is feasible to obtain the corresponding
expressions for Î» and Î¨. We here do so for the following mechanisms acting after quantization.
(I) Random bin flip. For i âˆˆ [m]: with probability 1 âˆ’ p, yi remains unchanged. With probability p,
yi is changed to an element from (âˆ’M âˆª M) \ {yi } uniformly at random.
(II) Adversarial bin flip. For i âˆˆ [m]: Write yi = qÂµk for q âˆˆ {âˆ’1, 1} and Âµk âˆˆ M. With
probability 1 âˆ’ p, yi remains unchanged. With probability p, yi is changed to âˆ’qÂµK .

Note that for b = 1, (I) and (II) coincide (sign flip with probability p). Depending on the magnitude
of p, the corresponding value Î» = Î»b,p may even be negative, which is unlike the case of additive
noise. Recall that the error bound (11) requires Î» > 0. Borrowing terminology from robust statistics,
we consider pÌ„b = min{p : Î»b,p â‰¤ 0} as the breakdown point, i.e. the (expected) proportion of
contaminated observations that can still be tolerated so that (11) continues to hold. Mechanism (II)
produces a natural counterpart of gross corruptions in the standard setting (1). It can be shown
that among all maps âˆ’M âˆª M â†’ âˆ’M âˆª M applied randomly to the observations with a fixed
probability, (II) maximizes the ratio Î¨/Î», hence the attribute â€œadversarialâ€. In Figure 1 we display
Î¨b,p /Î»b,p for b âˆˆ {1, 2, 3, 4} for both (I) and (II). The table below lists the corresponding breakdown
points. For simplicity, (t, Âµ) are not optimized but set to the optimal (in the sense of Lloyd-Max)
choice (tâˆ—0 , Âµâˆ—0 ) in the noiseless case. The underlying derivations can be found in the supplement.
(I)
pÌ„b

b=1
1/2

b=2
3/4

b=3
7/8

b=4
15/16

b=1
1/2

(II)
pÌ„b

b=2
0.42

b=3
0.36

b=4
0.31

Figure 1 and the table provide one more argument in favour of one-bit measurements as they offer
better robustness vis-aÌ€-vis adversarial corruptions. In fact, once the fraction of such corruptions
reaches 0.2, b = 1 performs best âˆ’ on the measurement scale. For the milder corruption scheme (I),
b = 2 turns out to the best choice for significant but moderate p.
1.8

2.5

1.6
2

1.4

b=4
b=3

log10 (Î¨/Î»)

log10 (Î¨/Î»)

1.2
1.5

1

0.8

b=1

0.6

1

b=1

b=2

0.4

0.5

0.2
0
0

b=2

b = 3 / 4 (~overlap)
0.1

0.2

0.3

fraction of bin flips

0.4

0
0

0.5

0.1

0.2

0.3

0.4

fraction of gross corruptions

0.5

Figure 1: Î¨b,p /Î»b,p (log10 -scale), b âˆˆ {1, 2, 3, 4}, p âˆˆ [0, 0.5] for mechanisms (I, L) and (II, R).

4 Scale estimation
In Section 2, we have decomposed xâˆ— = xâˆ—u Ïˆ âˆ— into a product of a unit vector xâˆ—u and a scale
parameter Ïˆ âˆ— > 0. We have pointed out that xâˆ—u can be estimated by b-bit Marginal Regression
5

separately from Ïˆ âˆ— since the latter can be absorbed into the definition of the bins {Rk }. Accordingly,
bu and Ïˆb estimating xâˆ—u and Ïˆ âˆ— , respectively. We here consider
we may estimate xâˆ— as x
b=x
bu Ïˆb with x
the maximum likelihood estimator (MLE) for Ïˆ âˆ— , by following [15] which studied the estimation of
the scale parameter for the entire Î±-stable family of distributions. The work of [15] was motivated
by a different line of one scan 1-bit CS algorithm [16] based on Î±-stable designs [17].
First, we consider the case Ïƒ = 0, so that the {yi } are i.i.d. N (0, (Ïˆ âˆ— )2 ). The likelihood function is
L(Ïˆ) =

m X
K
Y

i=1 k=1

I(yi âˆˆ Rk ) P(|yi | âˆˆ Rk ) =

K
Y

k=1

{2(Î¦(tk /Ïˆ) âˆ’ Î¦(tkâˆ’1 /Ïˆ))}mk ,

(14)

where mk = |{i : |yi | âˆˆ Rk }|, k âˆˆ [K], and Î¦ denotes the standard Gaussian cdf. Note that for
K = 1, L(Ïˆ) is constant (i.e. does not depend on Ïˆ) which confirms that for b = 1, it is impossible
to recover Ïˆ âˆ— . For K = 2 (i.e. b = 2), the MLE has a simple a closed form expression given by
Ïˆb = t1 /Î¦âˆ’1 (0.5(1 + m1 /m)). The following tail bound establishes fast convergence of Ïˆb to Ïˆ âˆ— .
Proposition 4. Let Îµ âˆˆ (0, 1) and c = 2{Ï†â€² (t1 /Ïˆ âˆ— )}2 , where Ï†â€² denotes the derivative of the
b âˆ— âˆ’ 1| â‰¤ Îµ.
standard Gaussian pdf. With probability at least 1 âˆ’ 2 exp(âˆ’cmÎµ2 ), we have |Ïˆ/Ïˆ

The exponent c is maximized for t1 = Ïˆ âˆ— and becomes smaller as t1 /Ïˆ âˆ— moves away from 1.
While scale estimation from 2-bit measurements is possible, convergence can be slow if t1 is not
well chosen. For b â‰¥ 3, convergence can be faster but the MLE is not available in closed form [15].

We now turn to the case Ïƒ > 0. The MLE based on (14) is no longer consistent. If xâˆ—u is known then
the joint likelihood of for (Ïˆ âˆ— , Ïƒ) is given by



m  
Y
ui âˆ’ Ïˆ hai , xâˆ—u i
li âˆ’ Ïˆ hai , xâˆ—u i
Î¦
L(Ïˆ, Ïƒ
e) =
âˆ’Î¦
,
(15)
Ïƒ
e
Ïƒ
e
i=1
where [li , ui ] denotes the interval the i-th observation is contained in before quantization, i âˆˆ [m]. It
is not clear to us whether the likelihood is log-concave, which would ensure that the global optimum
can be obtained by convex programming. Empirically, we have not encountered any issue with
spurious local minima when using Ïˆ = 0 and Ïƒ
e as the MLE from the noiseless case as starting
point. The only issue with (15) we are aware of concerns the case in which there exists Ïˆ so that
Ïˆ hai , xâˆ—u i âˆˆ [li , ui ], i âˆˆ [m]. In this situation, the MLE for Ïƒ equals zero and the MLE for Ïˆ may
not be unique. However, this is a rather unlikely scenario as long as there is a noticeable noise level.
As xâˆ—u is typically unknown, we may follow the plug-in principle, replacing xâˆ—u by an estimator x
bu .

5 Experiments

We here provide numerical results supporting/illustrating some of the key points made in the previous sections. We also compare b-bit Marginal Regression to alternative recovery algorithms.
Setup. Our simulations follow model (1) with n = 500, s âˆˆ {10, 20, . . . , 50}, Ïƒ âˆˆ {0, 1, 2}
and b âˆˆ {1, 2}. Regarding xâˆ— , the support and its signs are selected uniformly at random, while
the absolute magnitude of the entries corresponding
p to the support are drawn from the uniform
distribution on [Î², 2Î²], where Î² = f Â· (1/Î»1,Ïƒ ) log(n)/m and m = f 2 (1/Î»1,Ïƒ )2 s log n with
f âˆˆ {1.5, 3, 4.5, . . . , 12} controlling the signal strength. The resultingâˆšsignal is then normalized
to unit 2-norm. Before normalization, the norm of the signal lies in [1, 2] by construction which
ensures that as f increases the signal strength condition (12) is satisfied with increasing probability. For b = 2, we use Lloyd-Max quantization for a N (0, 1)-random variable which is optimal for
Ïƒ = 0, but not for Ïƒ > 0. Each possible configuration for s, f and Ïƒ is replicated 20 times. Due to
space limits, a representative subset of the results is shown; the rest can be found in the supplement.
Empirical verification of the analysis in Section 3. The experiments reveal that what is predicted
by the analysis of the comparison of the relative performance of 1-bit and 2-bit measurements for
estimating xâˆ— closely agrees with what is observed empirically, as can be seen in Figure 2.
Estimation of the scale and the noise level. Figure 3 suggests that the plug-in MLE for (Ïˆ âˆ— =
kxâˆ— k2 , Ïƒ) is a suitable approach, at least as long as Ïˆ âˆ— /Ïƒ is not too small. For Ïƒ = 2, the plug-in
MLE for Ïˆ âˆ— appears to have a noticeable bias as it tends to 0.92 instead of 1 for increasing f (and
thus increasing m). Observe that for Ïƒ = 0, convergence to the true value 1 is smaller as for Ïƒ = 1,
6

âˆ’1

b=1
b=2
required improvement
predicted improvement

âˆ’1.5

âˆ’2
âˆ’2.5

Ïƒ =0, s = 10

âˆ’2.5

log2(error)

log2(error)

âˆ’2

âˆ’3
âˆ’3.5

Ïƒ =0, s = 50

âˆ’3
âˆ’3.5
âˆ’4

âˆ’4

âˆ’4.5

âˆ’4.5

âˆ’5

âˆ’5
0.5

1

1.5

2

f

2.5

3

3.5

4

0.5

b=1
b=2
required improvement
predicted improvement

âˆ’1.5
âˆ’2
âˆ’2.5

1

1.5

2

f

âˆ’2
âˆ’2.5

Ïƒ =1, s = 50

âˆ’3
âˆ’3.5

3

3.5

4

Ïƒ =2, s = 50

âˆ’3
âˆ’3.5

âˆ’4

âˆ’4

âˆ’4.5

âˆ’4.5

âˆ’5

2.5

b=1
b=2
required improvement
predicted improvement

âˆ’1.5

log2(error)

log2(error)

b=1
b=2
required improvement
predicted improvement

âˆ’1.5

âˆ’5

0.5

1

1.5

2

f

2.5

3

3.5

4

0.5

1

1.5

2

f

2.5

3

3.5

4

Figure 2: Average â„“2 -estimation errors kxâˆ— âˆ’ x
bk2 for b = 1 and b = 2 on the log2 -scale in dependence of the signal strength f . The curve â€™predicted improvementâ€™ (of b = 2 vs. b = 1) is obtained
by scaling the â„“2 -estimation error by the factor predicted by the theory of Section
3. Likewise the
âˆš
curve â€™required improvementâ€™ results by scaling the error of b = 1 by 1/ 2 and indicates what
would be required by b = 2 to improve over b = 1 at the level of total #bits.
1.02

1.8

0.98
0.96
0.94

Ïƒ=2
0.92
0.9

Ïƒ=0

s = 50

0.88

1.4
1.2

Ïƒ=1
1
0.8
0.6

s = 50
0.4

0.86
0.5

Ïƒ=2

1.6

estimated noise level

estimated norm of x*

1 Ïƒ=1

0.2
1

1.5

2

2.5

3

3.5

0.5

4

Ïƒ=0
1

1.5

2

2.5

3

3.5

4

f

f

Figure 3: Estimation of Ïˆ = kxâˆ— k2 (here 1) and Ïƒ. The curves depict the average of the plug-in
MLE discussed in Section 4 while the bars indicate Â±1 standard deviation.
while Ïƒ is over-estimated (about 0.2) for small f . The above two issues are presumably a plug-in
effect, i.e. a consequence of using x
bu in place of xâˆ—u .

b-bit Marginal Regression and alternative recovery algorithms. We compare the â„“2 -estimation
error of b-bit Marginal Regression to several common recovery algorithms. Compared to apparently
more principled methods which try to enforce agreement of Q(y) and Q(Ab
x) w.r.t. the Hamming
distance (or a surrogate thereof), b-bit Marginal Regression can be seen as a crude approach as it is
based on maximizing the inner product between y and Ax. One may thus expect that its performance
is inferior. In summary, our experiments confirm that this is true in low-noise settings, but not so if
the noise level is substantial. Below we briefly present the alternatives that we consider.
Plan-Vershynin: The approach in [23] based on (7) which only differs in that the constraint set
results from a relaxation. As shown in Figure 4 the performance is similar though slightly inferior.
IHT-quadratic: Standard Iterative Hard Thresholding based on quadratic loss [1]. As pointed out
above, b-bit Marginal Regression can be seen as one-step version of Iterative Hard Thresholding.
7

IHT-hinge (b = 1): The variant of Iterative Hard Threshold for binary observations using a hinge
loss-type loss function as proposed in [11].
SVM (b = 1): Linear SVM with squared hinge
loss and an â„“1 -penalty, implemented in LIBLINEAR
âˆš
[6]. The cost parameter is chosen from 1/ m log m.{2âˆ’3 , 2âˆ’2 , . . . , 23 } by 5-fold cross-validation.
IHT-Jacques (b = 2): A variant of Iterative Hard Threshold for quantized observations based on a
specific piecewiese linear loss function [9].

SVM-type (b = 2):P
This approach is based on solving the following convex optimization problem:
m
minx,{Î¾i } Î³kxk1 + i=1 Î¾i subject to li âˆ’ Î¾i â‰¤ hai , xi â‰¤ ui + Î¾i , Î¾i â‰¥ 0, i âˆˆ [m], where [li , ui ]
is the bin observation i is assigned to. The essential idea is to enforce consistency of the observed
and predicted bin assignments up to slacks
âˆš {Î¾i } while promoting sparsity of the solution via an â„“1 penalty. The parameter Î³ is chosen from m log mÂ·{2âˆ’10 , 2âˆ’9 , . . . , 23 } by 5-fold cross-validation.
Turning to the results as depicted by Figure 4, the difference between a noiseless (Ïƒ = 0) and
heavily noisy setting (Ïƒ = 2) is perhaps most striking.
Ïƒ = 0: both IHT variants significantly outperform b-bit Marginal Regression. By comparing errors
for IHT, b = 2 can be seen to improve over b = 1 at the level of the total # bits.
Ïƒ = 2: b-bit Marginal Regression is on par with the best performing methods. IHT-quadratic for
b = 2 only achieves a moderate reduction in error over b = 1, while IHT-hinge is supposedly
affected by convergence issues. Overall, the results suggest that a setting with substantial noise
favours a crude approach (low-bit measurements and conceptually simple recovery algorithms).
Marginal
Planâˆ’Vershynin
IHTâˆ’quadratic
IHTâˆ’hinge
SVM

âˆ’2
âˆ’3

0

âˆ’1
âˆ’1.5

log2(error)

log2(error)

âˆ’4

b=1

âˆ’5
âˆ’6
âˆ’7

âˆ’2
âˆ’2.5
âˆ’3
âˆ’3.5
âˆ’4

âˆ’8 Ïƒ =0, s = 50

0.5

1

1.5

2

f

2.5

âˆ’3
âˆ’4

3.5

4

0.5

âˆ’5
âˆ’6
âˆ’7
âˆ’8

1

1.5

2

f

2.5

3

3.5

4

Marginal
Planâˆ’Vershynin
IHTâˆ’quadratic
IHTâˆ’Jacques
SVMâˆ’type

âˆ’1.5
âˆ’2
âˆ’2.5

log2(error)

log2(error)

3

Marginal
Planâˆ’Vershynin
IHTâˆ’quadratic
IHTâˆ’Jacques
SVMâˆ’type

âˆ’2

âˆ’3
âˆ’3.5
âˆ’4

âˆ’9
âˆ’10

Ïƒ =2, s = 50

âˆ’4.5

âˆ’9

b=2

Marginal
Planâˆ’Vershynin
IHTâˆ’quadratic
IHTâˆ’hinge
SVM

âˆ’0.5

âˆ’4.5

Ïƒ =0, s = 50

0.5

1

1.5

âˆ’5 Ïƒ =2, s = 50

2

f

2.5

3

3.5

4

0.5

1

1.5

2

f

2.5

3

3.5

4

Figure 4: Average â„“2 -estimation errors for several recovery algorithms on the log2 -scale in dependence of the signal strength f . We contrast Ïƒ = 0 (L) vs. Ïƒ = 2 (R), b = 1 (T) vs. b = 2 (B).

6 Conclusion
Bridging Marginal Regression and a popular approach to 1-bit CS due to Plan & Vershynin, we
have considered signal recovery from b-bit quantized measurements. The main finding is that for
b-bit Marginal Regression it is not beneficial to increase b beyond 2. A compelling argument for
b = 2 is the fact that the norm of the signal can be estimated unlike the case b = 1. Compared to
high-precision measurements, 2-bit measurements also exhibit strong robustness properties. It is of
interest if and under what circumstances the conclusion may differ for other recovery algorithms.
Acknowledgement. This work is partially supported by NSF-Bigdata-1419210, NSF-III-1360971,
ONR-N00014-13-1-0764, and AFOSR-FA9550-13-1-0137.
8

References
[1] T. Blumensath and M. Davies. Iterative hard thresholding for compressed sensing. Applied and Computational Harmonic Analysis, 27:265â€“274, 2009.
[2] P. Boufounos and R. Baraniuk. 1-bit compressive sensing. In Information Science and Systems, 2008.
[3] E. Candes and T. Tao. The Dantzig selector: statistical estimation when p is much larger than n. The
Annals of Statistics, 35:2313â€“2351, 2007.
[4] S. Chen and A. Banerjee. One-bit Compressed Sensing with the k-Support Norm. In AISTATS, 2015.
[5] D. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52:1289â€“1306, 2006.
[6] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear
classification. Journal of Machine Learning Research, 9:1871â€“1874, 2008.
[7] C. Genovese, J. Jin, L. Wasserman, and Z. Yao. A Comparison of the Lasso and Marginal Regression.
Journal of Machine Learning Research, 13:2107â€“2143, 2012.
[8] S. Gopi, P. Netrapalli, P. Jain, and A. Nori. One-bit Compressed Sensing: Provable Support and Vector
Recovery. In ICML, 2013.
[9] L. Jacques, K. Degraux, and C. De Vleeschouwer. Quantized iterative hard thresholding: Bridging 1-bit
and high-resolution quantized compressed sensing. arXiv:1305.1786, 2013.
[10] L. Jacques, D. Hammond, and M. Fadili. Dequantizing compressed sensing: When oversampling and
non-gaussian constraints combine. IEEE Transactions on Information Theory, 57:559â€“571, 2011.
[11] L. Jacques, J. Laska, P. Boufounos, and R. Baraniuk. Robust 1-bit Compressive Sensing via Binary Stable
Embeddings of Sparse Vectors. IEEE Transactions on Information Theory, 59:2082â€“2102, 2013.
[12] J. Kieffer. Uniqueness of locally optimal quantizer for log-concave density and convex error weighting
function. IEEE Transactions on Information Theory, 29:42â€“47, 1983.
[13] J. Laska and R. Baraniuk. Regime change: Bit-depth versus measurement-rate in compressive sensing.
arXiv:1110.3450, 2011.
[14] J. Laska, P. Boufounos, M. Davenport, and R. Baraniuk. Democracy in action: Quantization, saturation,
and compressive sensing. Applied and Computational Harmonic Analysis, 31:429â€“443, 2011.
[15] P. Li. Binary and Multi-Bit Coding for Stable Random Projections. arXiv:1503.06876, 2015.
[16] P. Li. One scan 1-bit compressed sensing. Technical report, arXiv:1503.02346, 2015.
[17] P. Li, C.-H. Zhang, and T. Zhang. Compressed counting meets compressed sensing. In COLT, 2014.
[18] J. Liu and S. Wright. Robust dequantized compressive sensing. Applied and Computational Harmonic
Analysis, 37:325â€“346, 2014.
[19] S. Lloyd. Least Squares Quantization in PCM. IEEE Transactions on Information Theory, 28:129â€“137,
1982.
[20] J. Max. Quantizing for Minimum Distortion. IRE Transactions on Information Theory, 6:7â€“12, 1960.
[21] D. Needell and J. Tropp. CoSaMP: Iterative signal recovery from incomplete and inaccurate samples.
Applied and Computational Harmonic Analysis, 26:301â€“321, 2008.
[22] Y. Plan and R. Vershynin. One-bit compressed sensing by linear programming. Communications on Pure
and Applied Mathematics, 66:1275â€“1297, 2013.
[23] Y. Plan and R. Vershynin. Robust 1-bit compressed sensing and sparse logistic regression: a convex
programming approach. IEEE Transactions on Information Theory, 59:482â€“494, 2013.
[24] R. Zhu and Q. Gu. Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing. In
ICML, 2015.
[25] R. Vershynin. In: Compressed Sensing: Theory and Applications, chapter â€™Introduction to the nonasymptotic analysis of random matricesâ€™. Cambridge University Press, 2012.
[26] M. Wainwright. Sharp thresholds for noisy and high-dimensional recovery of sparsity using â„“1 constrained quadratic programming (Lasso). IEEE Transactions on Information Theory, 55:2183â€“2202,
2009.
[27] C.-H. Zhang and T. Zhang. A general theory of concave regularization for high-dimensional sparse
estimation problems. Statistical Science, 27:576â€“593, 2013.
[28] L. Zhang, J. Yi, and R. Jin. Efficient algorithms for robust one-bit compressive sensing. In ICML, 2014.
[29] T. Zhang. Adaptive Forward-Backward Greedy Algorithm for Learning Sparse Representations. IEEE
Transactions on Information Theory, 57:4689â€“4708, 2011.

9

