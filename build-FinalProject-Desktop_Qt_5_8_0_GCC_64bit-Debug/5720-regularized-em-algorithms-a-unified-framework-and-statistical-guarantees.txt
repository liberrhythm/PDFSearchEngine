Regularized EM Algorithms: A Unified Framework
and Statistical Guarantees
Constantine Caramanis
Dept. of Electrical and Computer Engineering
The University of Texas at Austin
constantine@utexas.edu

Xinyang Yi
Dept. of Electrical and Computer Engineering
The University of Texas at Austin
yixy@utexas.edu

Abstract
Latent models are a fundamental modeling tool in machine learning applications,
but they present significant computational and analytical challenges. The popular
EM algorithm and its variants, is a much used algorithmic tool; yet our rigorous
understanding of its performance is highly incomplete. Recently, work in [1] has
demonstrated that for an important class of problems, EM exhibits linear local
convergence. In the high-dimensional setting, however, the M -step may not be
well defined. We address precisely this setting through a unified treatment using
regularization. While regularization for high-dimensional problems is by now
well understood, the iterative EM algorithm requires a careful balancing of making
progress towards the solution while identifying the right structure (e.g., sparsity or
low-rank). In particular, regularizing the M -step using the state-of-the-art highdimensional prescriptions (e.g., à la [19]) is not guaranteed to provide this balance.
Our algorithm and analysis are linked in a way that reveals the balance between
optimization and statistical errors. We specialize our general framework to sparse
gaussian mixture models, high-dimensional mixed regression, and regression with
missing variables, obtaining statistical guarantees for each of these examples.

1

Introduction

We give general conditions for the convergence of the EM method for high-dimensional estimation.
We specialize these conditions to several problems of interest, including high-dimensional sparse
and low-rank mixed regression, sparse gaussian mixture models, and regression with missing covariates. As we explain below, the key problem in the high-dimensional setting is the M -step. A natural
idea is to modify this step via appropriate regularization, yet choosing the appropriate sequence of
regularizers is a critical problem. As we know from the theory of regularized M-estimators (e.g.,
[19]) the regularizer should be chosen proportional to the target estimation error. For EM, however,
the target estimation error changes at each step.
The main contribution of our work is technical: we show how to perform this iterative regularization.
We show that the regularization sequence must be chosen so that it converges to a quantity controlled
by the ultimate estimation error. In existing work, the estimation error is given by the relationship
between the population and empirical M -step operators, but this too is not well defined in the highdimensional setting. Thus a key step, related both to our algorithm and its convergence analysis, is
obtaining a different characterization of statistical error for the high-dimensional setting.
Background and Related Work
EM (e.g., [8, 12]) is a general algorithmic approach for handling latent variable models (including
mixtures), popular largely because it is typically computationally highly scalable, and easy to implement. On the flip side, despite a fairly long history of studying EM in theory (e.g., [12, 17, 21]),
1

very little has been understood about general statistical guarantees until recently. Very recent work
in [1] establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to
obtain near-optimal rates for several specific low-dimensional problems – low-dimensional in the
sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in [1]) to the high-dimensional regime is
the M -step. On the algorithm side, the M -step will not be stable (or even well-defined in some
cases) in the high-dimensional setting. To make matters worse, any analysis that relies on showing
that the finite-sample M -step is somehow “close” to the M -step performed with infinite data (the
population-level M -step) simply cannot apply in the high-dimensional regime. Recent work in [20]
treats high-dimensional EM using a truncated M -step. This works in some settings, but also requires
specialized treatment for every different setting, precisely because of the difficulty with the M -step.
In contrast to work in [20], we pursue a high-dimensional extension via regularization. The central
challenge, as mentioned above, is in picking the sequence of regularization coefficients, as this
must control the optimization error (related to the special structure of β ∗ ), as well as the statistical
error. Finally, we note that for finite mixture regression, Städler et al.[16] consider an `1 regularized
EM algorithm for which they develop some asymptotic analysis and oracle inequality. However,
this work doesn’t establish the theoretical properties of local optima arising from regularized EM.
Our work addresses this issue from a local convergence perspective by using a novel choice of
regularization.

2

Classical EM and Challenges in High Dimensions

The EM algorithm is an iterative algorithm designed to combat the non-convexity of max likelihood
due to latent variables. For space concerns we omit the standard derivation, and only give the
definitions we need in the sequel. Let Y , Z be random variables taking values in Y,Z, with joint
distribution fβ (y, z) depending on model parameter β ⊆ Ω ⊆ Rp . We observe samples of Y but not
of the latent variable Z. EM seeks to maximize a lower bound on the maximum likelihood function
for β. Letting κβ (z|y) denote the conditional distribution of Z given Y = y, letting yβ∗ (y) denote
the marginal distribution of Y , and defining the function
n Z
1X
0
Qn (β |β) :=
κβ (z|yi ) log fβ0 (yi , z)dz,
(2.1)
n i=1 Z
one iteration of the EM algorithm, mapping β (t) to β (t+1) , consists of the following two steps:
• E-step: Compute function Qn (β|β (t) ) given β (t) .
• M-step: β (t+1) ← Mn (β) := arg maxβ0 ∈Ω Qn (β 0 |β (t) ).
We can define the population (infinite sample) versions of Qn and Mn in a natural manner:
Z
Z
0
∗
Q(β |β) :=
yβ (y)
κβ (z|y) log fβ0 (y, z)dzdy
Y

M(β)

=

Z
0

arg max
Q(β |β).
0
β ∈Ω

(2.2)
(2.3)

This paper is about the high-dimensional setting where the number of samples n may be far less
than the dimensionality p of the parameter β, but where β exhibits some special structure, e.g., it
may be a sparse vector or a low-rank matrix. In such a setting, the M -step of the EM algorithm may
be highly problematic. In many settings, for example sparse mixed regression, the M -step may not
even be well defined. More generally, when n  p, Mn (β) may be far from the population version,
M(β), and in particular, the minimum estimation error kMn (β ∗ ) − M(β ∗ )k can be much larger
than the signal strength kβ ∗ k. This quantity is used in [1] as well as in follow-up work in [20], as a
measure of statistical error. In the high dimensional setting, something else is needed.

3

Algorithm

The basis of our algorithm is the by-now well understood concept of regularized high dimensional
estimators, where the regularization is tuned to the underlying structure of β ∗ , thus defining a regu2

larized M -step via

Mrn (β) := arg max
Qn (β 0 |β) − λn R(β 0 ),
0

(3.1)

β ∈Ω

where R(·) denotes an appropriate regularizer chosen to match the structure of β ∗ . The key chal(t)
lenge is how to choose the sequence of regularizers {λn } in the iterative process, so as to control
optimization and statistical error. As detailed in Algorithm 1, our sequence of regularizers attempts
to match the target estimation error at each step of the EM iteration. For an intuition of what this
might look like, consider the estimation error at step t: kMrn (β (t) ) − β ∗ k2 . By the triangle inequality, we can bound this by a sum of two terms: the optimization error and the final estimation
error:
kMrn (β (t) ) − β ∗ k2 ≤ kMrn (β (t) ) − Mrn (β ∗ )k2 + kMrn (β ∗ ) − β ∗ k2 .
(3.2)
(t)

Since we expect (and show) linear convergence of the optimization, it is natural to update λn via a
(t)
(t−1)
recursion of the form λn = κλn
+∆ as in (3.3), where the first term represents the optimization
error, and ∆ represents the final statistical error, i.e., the last term above in (3.2). A key part of our
analysis shows that this error (and hence ∆) is controlled by k∇Qn (β ∗ |β) − ∇Q(β ∗ |β)kR∗ , which
in turn can be bounded uniformly for a variety of important applications of EM, including the three
discussed in this paper (see Section 5). While a technical point, it is this key insight that enables
the right choice of algorithm and its analysis. In the cases we consider, we obtain min-max optimal
rates of convergence, demonstrating that no algorithm, let alone another variant of EM, can perform
better.
Algorithm 1 Regularized EM Algorithm
Input Samples {yi }ni=1 , regularizer R, number of iterations T , initial parameter β (0) , initial regu(0)
larization parameter λn , estimated statistical error ∆, contractive factor κ < 1.
1: For t = 1, 2, . . . , T do
2:
Regularization parameter update:
(t−1)
λ(t)
+ ∆.
n ← κλn

3:
4:

E-step: Compute function Qn (·|β
Regularized M-step:

(t−1)

(3.3)

) according to (2.1).

β (t) ← Mrn (β (t−1) ) := arg max Qn (β|β (t−1) ) − λ(t)
n · R(β).
β∈Ω

5: End For
Output β (T ) .

4

Statistical Guarantees

We now turn to the theoretical analysis of regularized EM algorithm. We first set up a general
analytical framework for regularized EM where the key ingredients are decomposable regularizer
and several technical conditions on the population based Q(·|·) and the sample based Qn (·|·). In
Section 4.3, we provide our main result (Theorem 1) that characterizes both computational and
statistical performance of the proposed variant of regularized EM algorithm.
4.1

Decomposable Regularizers

Decomposable regularizers (e.g., [3, 6, 14, 19]), have been shown to be useful both empirically and
theoretically for high dimensional structural estimation, and they also play an important role in our
analytical framework. Recall that for R : Rp → R+ a norm, and a pair of subspaces (S, S) in Rp
such that S ⊆ S, we have the following definition:
Definition 1 (Decomposability). Regularizer R : Rp → R+ is decomposable with respect to (S, S)
if
⊥
R(u + v) = R(u) + R(v), for any u ∈ S, v ∈ S .
Typically, the structure of model parameter β ∗ can be characterized by specifying a subspace S such
that β ∗ ∈ S. The common use of a regularizer is thus to penalize the compositions of solution that
3

live outside S. We are interested in bounding the estimation error in some norm k · k. The following
quantity is critical in connecting R to k · k.
Definition 2 (Subspace Compatibility Constant). For any subspace S ⊆ Rp , a given regularizer R
and some norm k · k, the subspace compatibility constant of S with respect to R, k · k is given by
R(u)
Ψ(S) := sup
.
u∈S\{0} kuk



As is standard, the dual norm of R is defined as R∗ (v) := supR(u)≤1 u, v . To simplify notation,
we let kukR := R(u) and kukR∗ := R∗ (u).
4.2

Conditions on Q(·|·) and Qn (·|·)

Next, we review three technical conditions, originally proposed by [1], on the population level Q(·|·)
function, and then we give two important conditions that the empirial function Qn (·|·) must satisfy,
including one that characterizes the statistical error.
It is well known that performance of EM algorithm is sensitive to initialization. Following the lowdimensional development
in [1], our results

	 are local, and apply to an r-neighborhood region around
β ∗ : B(r; β ∗ ) := u ∈ Ω, ku − β ∗ k ≤ r .
We first require that Q(·|β ∗ ) is self consistent as stated below. This is satisfied, in particular, when
β ∗ maximizes the population log likelihood function, as happens in most settings of interest [12].
Condition 1 (Self Consistency). Function Q(·|β ∗ ) is self consistent, namely
β ∗ = arg max Q(β|β ∗ ).
β∈Ω

We also require that the function Q(·|·) satisfies a certain strong concavity condition and is smooth
over Ω.
Condition 2 (Strong Concavity and Smoothness (γ, µ, r)). Q(·|β ∗ ) is γ-strongly concave over Ω,
i.e.,



γ
(4.1)
Q(β2 |β ∗ ) − Q(β1 |β ∗ ) − ∇Q(β1 |β ∗ ), β2 − β1 ≤ − kβ2 − β1 k2 , ∀ β1 , β2 ∈ Ω.
2
∗
For any β ∈ B(r; β ), Q(·|β) is µ-smooth over Ω, i.e.,



µ
Q(β2 |β) − Q(β1 |β) − ∇Q(β1 |β), β2 − β1 ≥ − kβ2 − β1 k2 , ∀ β1 , β2 ∈ Ω.
(4.2)
2
The next condition is key in guaranteeing the curvature of Q(·|β) is similar to that of Q(·|β ∗ ) when
β is close to β ∗ . It has also been called First Order Stability in [1].
Condition 3 (Gradient Stability (τ, r)). For any β ∈ B(r; β ∗ ), we have


∇Q(M(β)|β) − ∇Q(M(β)|β ∗ ) ≤ τ kβ − β ∗ k.
The above condition only requires that the gradient be stable at one point M(β). This is sufficient
for our analysis.
In fact, for many concrete
examples, one can verify a stronger version of Condition


3 that is ∇Q(β 0 |β) − ∇Q(β 0 |β ∗ ) ≤ τ kβ − β ∗ k, ∀ β 0 ∈ B(r; β ∗ ).
Next we require two conditions on the empirical function Qn (·|·), which is computed from finite
number of samples according to (2.1). Our first condition, parallel to Condition 2, imposes a curvature constraint on Qn (·|·). In order to guarantee that the estimation error kβ (t) − β ∗ k in step t
of the EM algorithm is well controlled, we would like Qn (·|β (t−1) ) to be strongly concave at β ∗ .
However, in the setting where n  p, there might exist directions along which Qn (·|β (t−1) ) is flat,
e.g., as in mixed linear regression and missing covariate regression. In contrast with Condition 2, we
only require Qn (·|·) to be strongly concave over a particular set C(S, S; R) that is defined in terms
of the subspace pair (S, S) and regularizer R. This set is defined as follows:






 
p 





C(S, S; R) := u ∈ R : ΠS ⊥ (u) R ≤ 2 · ΠS (u) R + 2 · Ψ(S) · u ,
(4.3)
where the projection operator ΠS : Rp → Rp is defined as ΠS (u) := arg minv∈S kv − uk. The
restricted strong concavity (RSC) condition is as follows.
4

Condition 4 (RSC (γn , S, S, r,Tδ)). For any fixed β ∈ B(r; β ∗ ), with probability at least 1 − δ, we
have that for all β 0 − β ∗ ∈ Ω C(S, S; R),



γn
Qn (β 0 |β) − Qn (β ∗ |β) − ∇Qn (β ∗ |β), β 0 − β ∗ ≤ − kβ 0 − β ∗ k2 .
2
The above condition states that Qn (·|β) is strongly concave in directions β 0 − β ∗ that belong to
C(S, S; R). It is instructive to compare Condition 4 with a related condition proposed by [14] for
analyzing high dimensional M-estimators. They require the loss function to be strongly convex over
the cone {u ∈ Rp : kΠS ⊥ (u)kR . kΠS (u)kR }. Therefore our restrictive set (4.3) is similar to the
cone but has the additional term 2Ψ(S)kuk. The main purpose of the term 2Ψ(S)kuk is to allow
the regularization parameter λn to jointly control optimization and statistical error. We note that
while Condition 4 is stronger than the usual
 RSCcondition in M-estimator, in typical settings
  the
difference is immaterial. This is because ΠS (u)R is within a constant factor of Ψ(S) · u, and
hence checking RSC over C amounts to checking it over kΠS ⊥ (u)kR . Ψ(S)kuk, which is indeed
what is typically also done in the M-estimator setting.
Finally, we establish the condition that characterizes the achievable statistical error.
Condition 5 (Statistical Error (∆n , r, δ)). For any fixed β ∈ B(r; β ∗ ), with probability at least
1 − δ, we have


∇Qn (β ∗ |β) − ∇Q(β ∗ |β) ∗ ≤ ∆n .
(4.4)
R
This quantity replaces the term kMn (β)−M(β)k which appears in [1] and [20], and which presents
problems in the high dimensional regime.
4.3

Main Results

In this section, we provide the theoretical guarantees for a resampled version of our regularized EM
algorithm: we split the whole dataset into T pieces and use a fresh piece of data in each iteration of
regularized EM. As in [1], resampling makes it possible to check that Conditions 4-5 are satisfied
without requiring them to hold uniformly for all β ∈ B(r; β ∗ ) with high probability. Our empirical
results indicate that it is not in fact required and is an artifact of the analysis. We refer to this
resampled version as Algorithm 2. In the sequel, we let m := n/T to denote the sample complexity
in each iteration. We let α := supu∈Rp \{0} kuk∗ /kuk, where k · k∗ is the dual norm of k · k.
For Algorithm 2, our main result is as follows. The proof is deferred to the Supplemental Material.
Theorem 1. Assume the model parameter β ∗ ∈ S and regularizer R is decomposable with respect
to (S, S) where S ⊆ S ⊆ Rp . Assume r > 0 is such that B(r; β ∗ ) ⊆ Ω. Further, assume function
Q(·|·), defined in (2.2), is self consistent and satisfies Conditions 2-3 with parameters (γ, µ, r) and
(τ, r). Given n samples and T iterations, let m := n/T . Assume Qm (·|·), computed from any
m i.i.d. samples according to (2.1), satisfies Conditions 4-5 with parameters (γm , S, S, r, 0.5δ/T )
αµτ
and (∆m , r, 0.5δ/T ). Let κ∗ := 5 γγ
, and assume 0 < τ < γ and 0 < κ∗ ≤ 3/4. Define
m
∆ := rγm /[60Ψ(S)] and assume ∆m is such that ∆m ≤ ∆.
Consider Algorithm 2 with initialization β (0) ∈ B(r; β ∗ ) and with regularization parameters given
by
1 − κt
t γm
λ(t)
kβ (0) − β ∗ k +
∆, t = 1, 2, . . . , T
(4.5)
m =κ
1−κ
5Ψ(S)
for any ∆ ∈ [3∆m , 3∆], κ ∈ [κ∗ , 3/4]. Then with probability at least 1 − δ, we have that for any
t ∈ [T ],
5 1 − κt
Ψ(S)∆.
(4.6)
kβ (t) − β ∗ k ≤ κt kβ (0) − β ∗ k +
γm 1 − κ
The estimation error is bounded by a term decaying linearly with number of iterations t, which we
can think of as the optimization error and a second term that characterizes the ultimate estimation
error of our algorithm. With T = O(log n) and suitable choice of ∆ such that ∆ = O(∆n/T ), we
bound the ultimate estimation error as
1
kβ (T ) − β ∗ k .
Ψ(S)∆n/T .
(4.7)
(1 − κ)γn/T
5

We note that overestimating the initial error, kβ (0) −β ∗ k is not important, as it may slightly increase
the overall number of iterations, but will not impact the ultimate estimation error.
The constraint ∆m . rγm /Ψ(S) ensures that β (t) is contained in B(r; β ∗ ) for all t ∈ [T ]. This
constraint is quite mild in the sense that if ∆m = Ω(rγm /Ψ(S)), β (0) is a decent estimator with
estimation error O(Ψ(S)∆m /γm ) that already matches our expectation.

5

Examples: Applying the Theory

Now we introduce three well known latent variable models. For each model, we first review the
standard EM algorithm formulations, and discuss the extensions to the high dimensional setting.
Then we apply Theorem 1 to obtain the statistical guarantee of the regularized EM with data splitting
(Algorithm 2). The key ingredient underlying these results is to check the technical conditions in
Section 4 hold for each model. We postpone these tedious details to the Supplemental Material.
5.1

Gaussian Mixture Model

We consider the balanced isotropic Gaussian mixture model (GMM) with two components where
the distribution of random variables (Y, Z) ∈ Rp × {−1, 1} is characterized as
Pr (Y = y|Z = z) = φ(y; z · β ∗ , σ 2 Ip ), Pr(Z = 1) = Pr(Z = −1) = 1/2.
Here we use φ(·|µ, Σ) to denote the probability density function of N (µ, Σ). In this example, Z
is the latent variable that indicates the cluster id of each sample. Given n i.i.d. samples {yi }ni=1 ,
function Qn (·|·) defined in (2.1) corresponds to
n

M
QGM
(β 0 |β) = −
n


1 X
w(yi ; β)kyi − β 0 k22 + (1 − w(yi ; β))kyi + β 0 k22 ,
2n i=1
ky−βk2

ky−βk2

(5.1)

ky+βk2

where w(y; β) := exp (− 2σ2 2 )[exp (− 2σ2 2 ) + exp (− 2σ2 2 )]−1 . We assume β ∗ ∈
B0 (s; p) := {u ∈ Rp : | supp(u)| ≤ s}. Naturally, we choose the regularizer R(·) to be the `1
norm. We define the signal-to-noise ratio SNR := kβ ∗ k2 /σ.
Corollary 1 (Sparse Recovery in GMM). There exist constants ρ, C such that if SNR ≥ ρ, n/T ≥
2
[80C(kβ ∗ k∞ + σ)/kβ ∗ k2 ] s log p, β (0) ∈ B(kβ ∗ k2 /4; β ∗ ); then with probability at least 1 − T /p
p
√
(0)
Algorithm 2 with parameters ∆ = C(kβ ∗ k∞ + σ) T log p/n, λn/T = 0.2kβ (0) − β ∗ k2 / s, any
κ ∈ [1/2, 3/4] and `1 regularization generates β (t) that has estimation error
r
5C(kβ ∗ k∞ + σ) s log p
(t)
∗
t
(0)
∗
kβ − β k2 ≤ κ kβ − β k2 +
T , for all t ∈ [T ].
1−κ
n

(5.2)

Note that by p
setting T  log(n/ log p), the order of final estimation error turns out to be
(kβ ∗ k∞ + δ) (s log p)/n)
p log (n/ log p). The minimax rate for estimating s-sparse vector in a
single Gaussian cluster is s log p/n, thereby the rate is optimal on (n, p, s) up to a log factor.
5.2

Mixed Linear Regression

Mixed linear regression (MLR), as considered in some recent work [5, 7, 22], is the problem of
recovering two or more linear vectors from mixed linear measurements. In the case of mixed linear
regression with two symmetric and balanced components, the response-covariate pair (Y, X) ∈
R × Rp is linked through
Y = hX, Z · β ∗ i + W,
where W is the noise term and Z is the latent variable that has Rademacher distribution over {−1, 1}.
We assume X ∼ N (0, Ip ), W ∼ N (0, σ 2 ). In this setting, with n i.i.d. samples {yi , xi }ni=1 of pair
(Y, X), function Qn (·|·) then corresponds to
n

LR
QM
(β 0 |β) = −
n


1 X
w(yi , xi ; β)(yi − hxi , β 0 i)2 + (1 − w(yi , xi ; β))(yi + hxi , β 0 i)2 ,
2n i=1
(5.3)
6

2

2

2

where w(y, x; β) := exp (− (y−hx,βi)
)[exp (− (y−hx,βi)
) + exp (− (y+hx,βi)
)]−1 .
2σ 2
2σ 2
2σ 2
We consider two kinds of structure on β ∗ :
Sparse Recovery. Assume β ∗ ∈ B0 (s; p). Then let R be the `1 norm, as in the previous section.
We define SNR := kβ ∗ k2 /σ.
Corollary 2 (Sparse recovery in MLR). There exist constant ρ, C, C 0 such that if SNR ≥ ρ, n/T ≥
2
C 0 [(kβ ∗ k2 + δ)/kβ ∗ k2 ] s log p, β (0) ∈ B(kβ ∗ k2 /240, β ∗ ); then with probability at least 1 − T /p
p
√
(0)
Algorithm 2 with parameters ∆ = C(kβ ∗ k2 + δ) T log p/n, λn/T = kβ (0) − β ∗ k2 /(15 s), any
κ ∈ [1/2, 3/4] and `1 regularization generates β (t) that has estimation error
r
15C(kβ ∗ k2 + δ) s log p
(t)
∗
t
(0)
∗
kβ − β k2 ≤ κ kβ − β k2 +
T , for all t ∈ [T ].
1−κ
n
Performing
T

log(n/(s log p)) iterations gives us estimation rate (kβ ∗ k2 +
p
δ) (s log p/n) log (n/(s log p)) which is near-optimal on (s, p, n). The dependence on kβ ∗ k2 ,
which also appears in the analysis of EM in the classical (low dimensional) setting [1], arises from
fundamental limits of EM. Removing such dependence for MLR is possible by convex relaxation
[7]. It is interesting to study how to remove it in the high dimensional setting.
Low Rank Recovery. Second we consider the setting where the model parameter is a matrix Γ∗ ∈
Rp1 ×p2 with rank(Γ∗ ) = θ  min(p1 , p2 ). We further assume X ∈ Rp1 ×p2 is an i.i.d. Gaussian
matrix, i.e., entries of X are independent random variables with distribution
1). We apply
PpN1 ,p(0,
2
nuclear norm regularization to serve the low rank structure, i.e, R(Γ) =
i=1 |si (Γ)|, where
si (Γ) is the ith singular value of Γ. Similarly, we let SNR := kΓ∗ kF /σ.
Corollary 3 (Low rank recovery in MLR). There exist constant ρ, C, C 0 such that if SNR ≥ ρ,
2
n/T ≥ C 0 [(kΓ∗ kF + σ)/kΓ∗ kF ] θ(p1 + p2 ), Γ(0) ∈ B(kΓ∗ kF /1600, Γ∗ ); thenpwith probability
at least 1 − T exp(−p1 − p2 ) Algorithm 2 with parameters ∆ = C(kΓ∗ kF + σ) T (p1 + p2 )/n,
√
(0)
λn/T = 0.01kΓ(0) − Γ∗ kF / 2θ, any κ ∈ [1/2, 3/4] and nuclear norm regularization generates
Γ(t) that has estimation error
(t)

kΓ

∗

t

(0)

− Γ kF ≤ κ kΓ

100C 0 (kΓ∗ kF + σ)
− Γ kF +
1−κ
∗

r

2θ(p1 + p2 )
T , for all t ∈ [T ].
n

The standard low rank matrix recovery with a single component, including other sensing matrix
designs beyond the Gaussianity, has been studied extensively (e.g., [2, 4, 13, 15]). To the best of our
knowledge, the theoretical study of the mixed low rank matrix recovery has not been considered.
5.3

Missing Covariate Regression

As our last example, we consider the missing covariate regression (MCR) problem. To parallel
standard linear regression, {yi , xi }ni=1 are samples of (Y, X) linked through Y = hX, β ∗ i + W .
However, we assume each entry of xi is missing independently with probability  ∈ (0, 1). Thereei takes the form
fore, the observed covariate vector x

xi,j with probability 1 − 
x
ei,j =
.
∗
otherwise
We assume the model is under Gaussian design X ∼ N (0, Ip ), W ∼ N (0, σ 2 ). We refer the
reader to our Supplementary Material for the specific Qn (·|·) function. In high dimensional case,
we assume β ∗ ∈ B0 (s; p). We define ρ := kβ ∗ k2 /σ to be the SNR and ω := r/kβ ∗ k2 to be the
relative contractivity radius. In particular, let ζ := (1 + ω)ρ.
Corollary 4 (Sparse Recovery in MCR). There exist constants C, C 0 , C0 , C1 such that if (1+ω)ρ ≤
C0 < 1,  < C1 , n/T ≥ C 0 max{σ 2 (ωρ)−1 , 1}s log p, β (0) ∈ B(ωkβ ∗ k2 , β ∗ ); then with probp
(0)
ability at least 1 − T /p Algorithm 2 with parameters ∆ = Cσ T log p/n, λn/T = kβ (0) −
√
β ∗ k2 /(45 s), any κ ∈ [1/2, 3/4] and `1 regularization generates β (t) that has estimation error
r
45Cσ s log p
(t)
∗
t
(0)
∗
kβ − β k2 ≤ κ kβ − β k2 +
T , for all t ∈ [T ],
1−κ
n
7

Unlike the previous two models, we require an upper bound on the signal to noise ratio. This unusual
constraint
p is in fact unavoidable [10]. By optimizing T , the order of final estimation error turns out
to be σ s log p/n log(n/(s log p)).

6

Simulations

We now provide some simulation results to back up our theory. Note that while Theorem 1 requires
resampling, we believe in practice this is unnecessary. This is validated by our results, where we
apply Algorithm 1 to the four latent variable models discussed in Section 5.
Convergence Rate. We first evaluate the convergence of Algorithm 1 assuming only that the initialization is a bounded distance from β ∗ . For a given error ωkβ ∗ k2 , the initial parameter β (0) is picked
randomly from the sphere centered around β ∗ with radius ωkβ ∗ k2 . We use Algorithm 1 with T = 7,
(0)
κ = 0.7, λn in Theorem 1. The choice of the critical parameter ∆ is given in the Supplementary
Material. For every single trial, we report estimation error kβ (t) − β ∗ k2 and optimization error
kβ (t) − β (T ) k2 in every iteration. We plot the log of errors over iteration t in Figure 1.
2
Est error
Opt error

Log error

-1

Log error

1
Est error
Opt error

0

-2
-3

3
Est error
Opt error

0

-2

Log error

0

-4

-1
-2

-6

-4
-5

-8

-6

-10

0

1

2

3

4

5

6

7

Est error
Opt error

2

Log error

1

1
0
-1

-3

0

1

2

Number of iterations

3

4

5

6

-4

7

-2
0

1

2

Number of iterations

(a) GMM

3

4

5

6

-3

7

0

1

2

Number of iterations

(b) MLR(sparse)

3

4

5

6

7

Number of iterations

(c) MLR(low rank)

(d) MCR

Figure 1: Convergence of regularized EM algorithm. In each panel, one curve is plotted from single
independent trial. Settings: (a,b,d) (n, p, s) = (500, 800, 5); (d) (n, p, θ) = (600, 30, 3); (a-c)
SNR = 5; (d) (SNR, ) = (0.5, 0.2); (a-d) ω = 0.5.
Statistical Rate. We now evaluate the statistical rate. We set T = 7 and compute estimation error
on βb := β (T ) . In Figure 2, we plot kβb − β ∗ k2 over normalized sample complexity, i.e., n/(s log p)
for s-sparse parameter and n/(θp) for rank θ p-by-p parameter. We refer the reader to Figure 1 for
other settings. We observe that the same normalized sample complexity leads to almost identical
estimation error in practice, which thus supports the corresponding statistical rate established in
Section 5.
0.4
p = 200
p = 400
p = 800

1.4
p = 200
p = 400
p = 800

0.18
0.16
0.14

0.3
0.25

1

0.12

0.6

0.1

0.15

0.4

10

15

20

25

30

n/(s log p)

(a) GMM

5

10

15

20

25

30

1.4

1
4

5

6

7

n/(θp)

(b) MLR(sparse)

1.6

1.2

3

n/(s log p)

p = 200
p = 400
p = 800

1.8

0.8

0.2

5

2
p = 25
p = 30
p = 35

1.2

kΓ̂ − Γ∗ kF

0.35

kβ̂ − β ∗ k2

kβ̂ − β ∗ k2

0.2

kβ̂ − β ∗ k2

0.22

(c) MLR(low rank)

8

5

10

15

20

25

30

n/(s log p)

(d) MCR

Figure 2: Statistical rates. Each point is an average of 20 independent trials. Settings: (a,b,d) s = 5;
(c) θ = 3.

Acknowledgments
The authors would like to acknowledge NSF grants 1056028, 1302435 and 1116955. This research
was also partially supported by the U.S. Department of Transportation through the Data-Supported
Transportation Operations and Planning (D-STOP) Tier 1 University Transportation Center.

8

References
[1] Sivaraman Balakrishnan, Martin J Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm:
From population to sample-based analysis. arXiv preprint arXiv:1408.2156, 2014.
[2] T Tony Cai and Anru Zhang. Rop: Matrix recovery via rank-one projections. The Annals of Statistics,
43(1):102–138, 2015.
[3] Emmanuel Candes and Terence Tao. The Dantzig selector: statistical estimation when p is much larger
than n. The Annals of Statistics, pages 2313–2351, 2007.
[4] Emmanuel J Candès and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements. Information Theory, IEEE Transactions on, 57(4):2342–
2359, 2011.
[5] Arun Tejasvi Chaganty and Percy Liang. Spectral experts for estimating mixtures of linear regressions.
arXiv preprint arXiv:1306.3729, 2013.
[6] Yudong Chen, Sujay Sanghavi, and Huan Xu. Improved graph clustering. Information Theory, IEEE
Transactions on, 60(10):6440–6455, Oct 2014.
[7] Yudong Chen, Xinyang Yi, and Constantine Caramanis. A convex formulation for mixed regression with
two components: Minimax optimal rates. In Conf. on Learning Theory, 2014.
[8] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via
the em algorithm. Journal of the royal statistical society. Series B (methodological), pages 1–38, 1977.
[9] Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. In Advances in Neural Information Processing Systems, pages 2726–
2734, 2011.
[10] Po-Ling Loh and Martin J Wainwright. Corrupted and missing predictors: Minimax bounds for highdimensional linear regression. In Information Theory Proceedings (ISIT), 2012 IEEE International Symposium on, pages 2601–2605. IEEE, 2012.
[11] Jinwen Ma and Lei Xu. Asymptotic convergence properties of the em algorithm with respect to the
overlap in the mixture. Neurocomputing, 68:105–129, 2005.
[12] Geoffrey McLachlan and Thriyambakam Krishnan. The EM algorithm and extensions, volume 382. John
Wiley & Sons, 2007.
[13] Sahand Negahban, Martin J Wainwright, et al. Estimation of (near) low-rank matrices with noise and
high-dimensional scaling. The Annals of Statistics, 39(2):1069–1097, 2011.
[14] Sahand Negahban, Bin Yu, Martin J Wainwright, and Pradeep K Ravikumar. A unified framework for
high-dimensional analysis of m-estimators with decomposable regularizers. In Advances in Neural Information Processing Systems, pages 1348–1356, 2009.
[15] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization. SIAM review, 52(3):471–501, 2010.
[16] Nicolas Städler, Peter Bühlmann, and Sara Van De Geer. L1-penalization for mixture regression models.
Test, 19(2):209–256, 2010.
[17] Paul Tseng. An analysis of the em algorithm and entropy-like proximal point methods. Mathematics of
Operations Research, 29(1):27–44, 2004.
[18] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
[19] Martin J Wainwright. Structured regularizers for high-dimensional problems: Statistical and computational issues. Annual Review of Statistics and Its Application, 1:233–253, 2014.
[20] Zhaoran Wang, Quanquan Gu, Yang Ning, and Han Liu. High dimensional expectation-maximization
algorithm: Statistical optimization and asymptotic normality. arXiv preprint arXiv:1412.8729, 2014.
[21] C.F.Jeff Wu. On the convergence properties of the em algorithm. The Annals of statistics, pages 95–103,
1983.
[22] Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi. Alternating minimization for mixed linear
regression. arXiv preprint arXiv:1310.3745, 2013.

9

