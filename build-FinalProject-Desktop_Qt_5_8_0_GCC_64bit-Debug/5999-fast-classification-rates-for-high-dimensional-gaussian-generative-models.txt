Fast Classification Rates for High-dimensional
Gaussian Generative Models
Tianyang Li

Adarsh Prasad
Department of Computer Science, UT Austin
{lty,adarsh,pradeepr}@cs.utexas.edu

Pradeep Ravikumar

Abstract
We consider the problem of binary classification when the covariates conditioned
on the each of the response values follow multivariate Gaussian distributions. We
focus on the setting where the covariance matrices for the two conditional distributions are the same. The corresponding generative model classifier, derived
via the Bayes rule, also called Linear Discriminant Analysis, has been shown to
behave poorly in high-dimensional settings. We present a novel analysis of the
classification error of any linear discriminant approach given conditional Gaussian
models. This allows us to compare the generative model classifier, other recently
proposed discriminative approaches that directly learn the discriminant function,
and then finally logistic regression which is another classical discriminative model
classifier. As we show, under a natural sparsity assumption, and letting s denote
the sparsity of the Bayes classifier, p the number of covariates, and n the number of samples, the simple (`1 -regularized)
 logistic
 regression classifier achieves
p
the fast misclassification error rates of O s log
, which is much better than the
n
other approaches, which are either
under high-dimensional settings,
q inconsistent

s log p
or achieve a slower rate of O
.
n

1

Introduction

We consider the problem of classification of a binary response given p covariates. A popular class of
approaches are statistical decision-theoretic: given a classification evaluation metric, they then optimize a surrogate evaluation metric that is computationally tractable, and yet have strong guarantees
on sample complexity, namely, number of observations required for some bound on the expected
classification evaluation metric. These guarantees and methods have been developed largely for the
zero-one evaluation metric, and extending these to general evaluation metrics is an area of active
research. Another class of classification methods are relatively evaluation metric agnostic, which
is an important desideratum in modern settings, where the evaluation metric for an application is
typically less clear: these are based on learning statistical models over the response and covariates,
and can be categorized into two classes. The first are so-called generative models, where we specify
conditional distributions of the covariates conditioned on the response, and then use the Bayes rule
to derive the conditional distribution of the response given the covariates. The second are the socalled discriminative models, where we directly specify the conditional distribution of the response
given the covariates.
In the classical fixed p setting, we have now have a good understanding of the performance of
the classification approaches above. For generative and discriminative modeling based approaches,
consider the specific case of Naive Bayes generative models and logistic regression discriminative
models (which form a so-called generative-discriminative pair1 ), Ng and Jordan [27] provided qual1
In such a so-called generative-discriminative pair, the discriminative model has the same form as that of
the conditional distribution of the response given the covariates specified by the Bayes rule given the generative
model

1

itative consistency analyses, and showed that under small sample settings, the generative model
classifiers converge at a faster rate to their population error rate compared to the discriminative
model classifiers, though the population error rate of the discriminative model classifiers could be
potentially lower than that of the generative model classifiers due to weaker model assumptions.
But if the generative model assumption holds, then generative model classifiers seem preferable to
discriminative model classifiers.
In this paper, we investigate whether this conventional wisdom holds even under high-dimensional
settings. We focus on the simple generative model where the response is binary, and the covariates
conditioned on each of the response values, follows a conditional multivariate Gaussian distribution.
We also assume that the two covariance matrices of the two conditional Gaussian distributions are
the same. The corresponding generative model classifier, derived via the Bayes rule, is known in
the statistics literature as the Linear Discriminant Analysis (LDA) classifier [21]. Under classical
settings where p  n, the misclassification error rate of this classifier has been shown to converge to
that of the Bayes classifier. However, in a high-dimensional setting, where the number of covariates
p could scale with the number of samples n, this performance of the LDA classifier breaks down. In
particular, Bickel and Levina [3] show that when p/n â†’ âˆ, then the LDA classifier could converge
to an error rate of 0.5, that of random chance. What should one then do, when we are even allowed
this generative model assumption, and when p > n?
Bickel and Levina [3] suggest the use of a Naive Bayes or conditional independence assumption,
which in the conditional Gaussian context, assumes the covariance matrices to be diagonal. As they
showed, the corresponding Naive Bayes LDA classifier does have misclassification error rate that is
better than chance, but it is asymptotically biased: it converges to an error rate that is strictly larger
than that of the Bayes classifier when the Naive Bayes conditional independence assumption does
not hold. Bickel and Levina [3] also considered a weakening of the Naive Bayes rule, by assuming
that the covariance matrix is weakly sparse, and an ellipsoidal constraint on the means, showed
that an estimator that leverages these structural constraints converges to the Bayes risk at a rate of
O(log(n)/nÎ³ ), where 0 < Î³ < 1 depends on the mean and covariance structural assumptions. A
caveat is that these covariance sparsity assumptions might not hold in practice. Similar caveats apply
to the related works on feature annealed independence rules [14], nearest shrunken centroids [29,
30], as well as those . Moreover, even when the assumptions hold, they do not yield the â€œfastâ€ rates
of O(1/n).
An alternative approach is to directly impose sparsity on the linear discriminant [28, 7], which is
weaker than the covariance sparsity assumptions (though [28] impose these in addition). [28, 7]
then proposed new estimators that leveraged these assumptions, but while they
were able
q
 to show
s log p
convergence to the Bayes risk, they were only able to show a slower rate of O
.
n
It is instructive at this juncture to look at recent results on classification error rates from the machine
learning community. A key notion of importance here is whether the two classes are separable:
which can be understood as requiring that
âˆš the classification error of the Bayes classifier is 0. Classical learning theory gives a rate of O(1/ n) for any classifier when two classes are non-separable,
and it shown that this is also minimax [12], with the note that this is relatively distribution agnostic,
since it assumes very little on the underlying distributions. When the two classes are non-separable,
only rates slower than â„¦(1/n) are known. Another key notion is a â€œlow-noise
conditionâ€ [25], under
âˆš
which certain classifiers can be shown to attain a rate faster than o(1/ n), albeit not at the O(1/n)
rate unless the two classes are separable. Specifically, let Î± denote a constant such that
P (|P(Y = 1|X) âˆ’ 1/2| â‰¤ t) â‰¤ O (tÎ± ) ,

(1)

holds when t â†’ 0. This is said to be a low-noise assumption, since as Î± â†’ +âˆ, the two classes
start becoming separable, that is, the Bayes
riskapproaches zero. Under this low-noise assumption,

1+Î±
known rates for excess 0-1 risk is O ( n1 ) 2+Î± [23]. Note that this is always slower than O( n1 )
when Î± < +âˆ.
There has been a surge of recent results on high-dimensional statistical statistical analyses of M estimators [26, 9, 1]. These however are largely focused on parameter error bounds, empirical and
population log-likelihood, and sparsistency. In this paper however, we are interested in analyzing
the zero-one classification error under high-dimensional sampling regimes. One could stitch these
recent results to obtain some error bounds: use bounds on the excess log-likelihood, and use trans2

forms from [2], to convert excess log-likelihood bounds to get bounds on 0-1 classification error,
however, the resulting bounds are very loose, and in particular, do not yield the fast rates that we
seek.
In this paper, we leverage the closed form expression for the zero-one classification error for our
generative model, and directly analyse it to give faster rates for any linear discriminant method.
Our analyses show that, assuming a sparse linear discriminant in addition,

the simple `1 -regularized
s log p
logistic regression classifier achieves near optimal fast rates of O
, even without requiring
n
that the two classes be separable.

2

Problem Setup

We consider the problem of high dimensional binary classification under the following generative
model. Let Y âˆˆ {0, 1} denote a binary response variable, and let X = (X1 , . . . , Xp ) âˆˆ Rp denote
a set of p covariates. For technical simplicity, we assume Pr[Y = 1] = Pr[Y = 0] = 12 , however
our analysis easily extends to the more general case when Pr[Y = 1], Pr[Y = 0] âˆˆ [Î´0 , 1 âˆ’ Î´0 ], for
some constant 0 < Î´0 < 21 . We assume that X|Y âˆ¼ N (ÂµY , Î£Y ), i.e. conditioned on a response, the
covariate follows a multivariate Gaussian distribution. We assume we are given n training samples
{(X (1) , Y (1) ), (X (2) , Y (2) ), . . . , (X (n) , Y (n) )} drawn i.i.d. from the conditional Gaussian model
above.
For any classifier, C : Rp â†’ {1, 0}, the 0-1 risk or simply the classification error is given by
R0âˆ’1 (C) = EX,Y [`0âˆ’1 (C(X), Y )], where `0âˆ’1 (C(x), y) = 1(C(x) 6= y) is the 0-1 loss. It can
also be simply written as R(C) = Pr[C(X) 6= Y ]. The classifier attaining the lowest classification
error is known as the Bayes classifier, which we will denote by C âˆ— . Under the generative model
=1|X]
assumption above, the Bayes classifier can be derived simply as C âˆ— (X) = 1(log Pr[Y
Pr[Y =0|X] > 0),
so that given sample X, it would be classified as 1 if
the error of the Bayes classifier Râˆ— = R(C âˆ— ).

Pr[Y =1|X]
Pr[Y =0|X]

> 1, and as 0 otherwise. We denote

When Î£1 = Î£0 = Î£,
log

1
Pr[Y = 1|X]
= (Âµ1 âˆ’ Âµ0 )T Î£ âˆ’1 X + (âˆ’ÂµT1 Î£ âˆ’1 Âµ1 + ÂµT0 Î£ âˆ’1 Âµ0 )
Pr[Y = 0|X]
2

(2)

and we denote this quantity as wâˆ— T X + bâˆ— where
wâˆ— = Î£ âˆ’1 (Âµ1 âˆ’ Âµ0 ), bâˆ— =

âˆ’ÂµT1 Î£ âˆ’1 Âµ1 + ÂµT0 Î£ âˆ’1 Âµ0
,
2

so that the Bayes classifier can be written as: C âˆ— (x) = 1(wâˆ— T x + bâˆ— > 0).
For any trained classifier CÌ‚ we are interested in bounding the excess risk defined as R(CÌ‚) âˆ’ Râˆ— .
The generative approach to training a classifier is to estimate estimate Î£ âˆ’1 and Î´ from data, and
then plug the estimates into Equation 2 to construct the classifier. This classifier is known as the
linear discriminant analysis (LDA) classifier, whose theoretical properties have been well-studied
=1|X]
in classical fixed p setting. The discriminative approach to training is to estimate Pr[Y
Pr[Y =0|X] directly
from samples.
2.1 Assumptions.
We assume that mean is bounded i.e. Âµ1 , Âµ0 âˆˆ {Âµ âˆˆ Rp : kÂµk2 â‰¤ BÂµ }, where BÂµ is a constant
which doesnâ€™t scale with p. We assume that the covariance matrix Î£ is non-degenerate i.e. all
eigenvalues of Î£ are in [BÎ»min , BÎ»max ]. Additionally we assume (Âµ1 âˆ’ Âµ0 )T Î£ âˆ’1 (Âµ1 âˆ’ Âµ0 ) â‰¥ Bs ,
which gives a lower bound on the Bayes classifierâ€™s classification error Râˆ— â‰¥ 1 âˆ’ Î¦( 21 Bs ) > 0.
Note that this assumption is different from the definition of separable classes in [11] and the low
noise condition in [25], and the two classes are still not separable because Râˆ— > 0.
2.1.1

Sparsity Assumption.

Motivated by [7], we assume that Î£ âˆ’1 (Âµ1 âˆ’ Âµ0 ) is sparse, and there at most s non-zero entries.
Cai and Liu [7] extensively discuss and show that such a sparsity assumption, is much weaker than
assuming either Î£ âˆ’1 and (Âµ1 âˆ’ Âµ0 ) to be individually sparse. We refer the reader to [7] for an
elaborate discussion.
3

2.2 Generative Classifiers
Generative techniques work by estimating Î£ âˆ’1 and (Âµ1 âˆ’ Âµ0 ) from data and plugging them into
Equation 2. In high-dimensions, simple estimation techniques do not perform well when p  n,
the sample estimate for the covariance matrix Î£Ì‚ is singular; using the generalized inverse of the
sample covariance matrix makes the estimator highly biased and unstable. Numerous alternative
approaches have been proposed by imposing structural conditions on Î£ or Î£ âˆ’1 and Î´ to ensure that
they can be estimated consistently. Some early work based on nearest shrunken centroids [29, 30],
feature annealed independence rules [14], and Naive Bayes [4] imposed independence assumptions
on Î£, which are often violated in real-world applications. [4, 13] impose more complex structural
assumptions on the covariance matrix and suggest more complicated thresholding techniques. Most
commonly, Î£ âˆ’1 and Î´ are assumed to be sparse and then some thresholding techniques are used to
estimate them consistently [17, 28].
2.3 Discriminative Classifiers.
Recently, more direct techniques have been proposed to solve the sparse LDA problem. Let Î£Ì‚
0
and ÂµË†d be consistent estimators of Î£ and Âµ = Âµ1 +Âµ
. Fan et al. [15] proposed the Regularized
2
Optimal Affine Discriminant (ROAD) approach which minimizes wT Î£w with wT Âµ restricted to be
a constant value and an `1 -penalty of w.
wROAD = argmin wT Î£Ì‚w

(3)

wT ÂµÌ‚=1
||w||1 â‰¤c

Kolar and Liu [22] provided theoretical insights into the ROAD estimator by analysing its consistency for variable selection. Cai and Liu [7] proposed another variant called linear programming
discriminant (LPD) which tries to make w close to the Bayes rules linear term Î£ âˆ’1 (Âµ1 âˆ’ Âµ0 ) in the
`âˆ norm. This can be cast as a linear programming problem related to the Dantzig selector.[8].
wLPD = argmin ||w||1
(4)
w

s.t.||Î£Ì‚w âˆ’ ÂµÌ‚||âˆ â‰¤ Î»n
Mai et al. [24]proposed another version of the sparse linear discriminant analysis based on an equivalent least square formulation of the LDA, where they solve an `1 -regularized least squares problem
to produce a consistent classifier.
All the techniques above
 do not have finite sample convergence rates, or the 0-1 risk converged
q either
s log p
.
at a slow rate of O
n
In this paper, we first provide an analysis of classification error rates for any classifier with a linear
discriminant function, and then follow this analysis by investigating the performance of generative
and discriminative classifiers for conditional Gaussian model.

3

Classifiers with Sparse Linear Discriminants

We first analyze any classifier with a linear discriminant function, of the form: C(x) = 1(wT x+b >
0). We first note that the 0-1 classification error of any such classifier is available in closed-form as
 T



1
w Âµ1 + b
1
wT Âµ0 + b
R(w, b) = 1 âˆ’ Î¦ âˆš
âˆ’ Î¦ âˆ’âˆš
,
(5)
2
2
wT Î£w
wT Î£w
which can be shown by noting that wT X + b is a univariate normal random variable when conditioned on the label Y .
Next, we relate the 0-1 classifiction error above to that of the Bayes classifier. Recall the earlier
notation of the Bayes classifier as C âˆ— (x) = 1(xT wâˆ— + bâˆ— > 0). The following theorem is a key
result of the paper that shows that for any linear discriminant classifier whose linear discriminant
parameters are close to that of the Bayes classifier, the excess 0-1 risk is bounded only by second
order terms of the difference. Note that this theorem will enable fast classification rates if we obtain
fast rates for the parameter error.
Theorem 1. Let w = wâˆ— + âˆ†, b = bâˆ— + â„¦, and âˆ† â†’ 0, â„¦ â†’ 0, then we have
R(w = wâˆ— + âˆ†, b = bâˆ— + â„¦) âˆ’ R(wâˆ— , bâˆ— ) = O(kâˆ†k22 + â„¦ 2 ).
4

(6)

Proof. Denote the quantity S âˆ— =
âˆ’ÂµT
wâˆ— âˆ’bâˆ—
âˆš 1
âˆ—
w T Î£wâˆ—

=

1 âˆ—
2S .

p

(Âµ1 âˆ’ Âµ0 )T Î£ âˆ’1 (Âµ1 âˆ’ Âµ0 ), then we have

Using (5) and the Taylor series expansion of Î¦(Â·) around

1 âˆ—
2S ,

ÂµT wâˆ— +bâˆ—
âˆš1
wâˆ— T Î£wâˆ—

=

we have

1
ÂµT w + b
1
1
âˆ’ÂµT w âˆ’ b
|(Î¦( âˆš1
) âˆ’ Î¦( S âˆ— )) + (Î¦( âˆš 0
) âˆ’ Î¦( S âˆ— ))| (7)
T
T
2
2
2
w Î£w
w Î£w
T
T
T
Âµ w+b
1
(Âµ1 âˆ’ Âµ0 ) w
âˆ’Âµ w âˆ’ b 1 âˆ— 2
âˆ’ S âˆ— | + K2 ( âˆš1
âˆ’ S âˆ— )2 + K3 ( âˆš 0
âˆ’ S )
â‰¤K1 | âˆš
T
T
2
w Î£w
w Î£w 2
wT Î£w
where K1 , K2 , K3 > 0 are constants because the first and second order derivatives of Î¦(Â·) are
bounded.
âˆš
âˆš
First note that | wT Î£w âˆ’ wâˆ— T Î£wâˆ— | = O(kâˆ†k2 ), because kwâˆ— k2 is bounded.
|R(w, b) âˆ’ R(wâˆ— , bâˆ— )| =

1

1

âˆ—

1

1

Denote w00 = Î£ 2 w, âˆ†00 = Î£ 2 âˆ†, w00 = Î£ 2 wâˆ— a00 = Î£ âˆ’ 2 (Âµ1 âˆ’ Âµ0 ), we have (by the binomial
Taylor series expansion)
T
p
(Âµ1 âˆ’ Âµ0 )T w
a00 w00
âˆš
âˆ’ Sâˆ— = âˆš T
âˆ’ a00 T a00
(8)
00 w 00
wT Î£w
w
q
00T
00
00 T
00T
00
00
âˆ’
1 + 2 aa00 Tâˆ†
+ âˆ†a00T aâˆ†00
1 + aa00 Tâˆ†
00
kâˆ†00 k22
a
a00
âˆš
âˆš
=
=
O(
)
w00T w00
00 T a00
a
00T
00
a a
T

0) w
âˆ’
Note that w00 â†’ a00 , âˆ†00 â†’ 0, kâˆ†k2 = Î˜(kâˆ†00 k2 ), and S âˆ— is lower bouned, we have | (Âµâˆš1 âˆ’Âµ
wT Î£w
S âˆ— | = O(kâˆ†k22 ).

ÂµT w+b
w Î£w

Next we bound | âˆš 1 T

âˆ’ 12 S âˆ— |:

âˆš
âˆš
âˆš
ÂµT1 w + b
1 âˆ—
(ÂµT1 wâˆ— + bâˆ— )( wâˆ— T Î£wâˆ— âˆ’ wT Î£w) + wâˆ— T Î£wâˆ— (ÂµT1 âˆ† + â„¦)
âˆš
|âˆš
âˆ’ S |=|
|
âˆš
wT Î£w 2
wT Î£w wâˆ— T Î£wâˆ—
q
= O( kâˆ†k22 + â„¦ 2 )

(9)

where we use the fact that |ÂµT1 wâˆ— + bâˆ— | and S âˆ— are bounded.
p
âˆ’ÂµT wâˆ’b
Similarly | âˆš 0T
âˆ’ 12 S âˆ— | = O( kâˆ†k22 + â„¦ 2 ).
w Î£w

Combing the above bounds we get the desired result.

4

Logistic Regression Classifier

In this section, we show that the simple `1 regularized logistic regression classifier attains fast classification error rates.
Specifically, we are interested in the M -estimator [21] below:
 X

1
(i)
T
(i)
T
(i)
(wÌ‚, bÌ‚) = arg min
(Y (w X + b) + log(1 + exp(w X + b))) + Î»(kwk1 + |b|) ,
w,b
n
(10)
which maximizes the penalized log-likelihood of the logistic regression model, which also corresponds to the conditional probability of the response given the covariates P(Y |X) for the conditional
Gaussian model.
Note that here we penalize the intercept term b as well. Although the intercept term usually is not
penalized (e.g. [19]), some packages (e.g. [16]) penalize the intercept term. Our analysis show that
penalizing the intercept term does not degrade the performance of the classifier.
In [2, 31] it is shown that minimizing the expected risk of the logistic loss also minimizes the
classification error for the corresponding linear classifier. `1 regularized logistic regression is a
popular classification method in many settings [18, 5]. Several commonly used packages ([19, 16])
have been developed for `1 regularized logistic regression. And recent works ([20, 10]) have been
on scaling regularized logistic regression to ultra-high dimensions and large number of samples.
5

4.1

Analysis

We first show that `1 regularized logistic regression estimator above converges to the Bayes classifier
parameters using techniques. Next we use the theorem from the previous section to argue that since
estimated parameter wÌ‚, bÌ‚ is close to the Bayes classifierâ€™s parameter wâˆ— , bâˆ— , the excess risk of the
classifier using estimated parameter is tightly bounded as well.
For the first step, we first show a restricted eigenvalue condition for X 0 = (X, 1) where X are our
covariates, that comes from a mixture of two Gaussian distributions 21 N (Âµ1 , Î£)+ 12 N (Âµ0 , Î£). Note
that X 0 is not zero centered, which is different from existing scenarios ([26], [6], etc.) that assume
âˆ—
covariates are zero centered. And we denote w0 = (w, b), S 0 = {i : w0 i 6= 0}, and s0 = |S 0 | â‰¤ s+1.
Lemma 1. With probability 1 âˆ’ Î´, âˆ€v 0 âˆˆ A0 âŠ† {v 0 âˆˆ Rp+1 : kv 0 k2 = 1}, for some constants
Îº1 , Îº2 , Îº3 > 0 we have
r
âˆš
1
1
0
0 0
kX v k2 â‰¥ Îº1 n âˆ’ Îº2 w(A ) âˆ’ Îº3 log
(11)
n
Î´
T

where w(A0 ) = Eg0 âˆ¼N (0,Ip+1 ) [supa0 âˆˆA0 g 0 a0 ] is the Gaussian width of A0 .

âˆš
In the special case when A0 = {v 0 : kvS0Â¯0 k1 â‰¤ 3kvS0 0 k1 , kv 0 k2 = 1}, we have w(A0 ) = O( s log p).
Proof. First note that X 0 is sub-Gaussian with bounded parameter and


1 0T 0
Î£ + 21 (Âµ1 ÂµT1 + Âµ0 ÂµT0 ) 12 (Âµ1 + Âµ0 )
0
Î£ = E[ X X ] =
(12)
1
T
T
1
n
2 (Âµ1 + Âµ0 )




Î£ + 14 (Âµ1 âˆ’ Âµ0 )T (Âµ1 âˆ’ Âµ0 ) 0
I âˆ’ 12 (Âµ1 + Âµ0 )
where A = p
, and
Note that AÎ£ 0 AT =
0
1
0
1






1

I 0
âˆ’ 12 (Âµ1 + Âµ0 )  1
I
(Âµ + Âµ0 )
. Notice that AAT = p
+
âˆ’ 2 (Âµ1 + Âµ0 )T 1
Aâˆ’1 = p 2 1
0 0
0
1
1

 1



I
0
(Âµ
+
Âµ
)
p
0
1
T
and Aâˆ’1 Aâˆ’T =
+ 2 1
1 , and we can see that the singular
2 (Âµ1 + Âµ0 )
0 0
1
q
values of A and Aâˆ’1 are lower bounded by âˆš 1 2 and upper bounded by 2 + BÂµ2 . Let Î»1
2+BÂµ

be the minimum eigenvalue of Î£ 0 , and u01 (ku01 k2 = 1) the corresponding eigenvector. From the
expression AÎ£AT Aâˆ’T u01 = Î»1 Au01 , so we know that the minimum eigenvalue of Î£ 0 is lower
bounded. Similarly the largest eigenvalue of Î£ 0 is upper bounded. Then the desired result follows
the proof of Theorem 10 in [1]. Although the proof of Theorem 10 in [1] is for zero-centered random
variables, the proof remains valid for non zero-centered random variables.
âˆš
When A0 = {v 0 : kvS0Â¯0 k1 â‰¤ 3kvS0 0 k1 , kv 0 k2 = 1}, [9] gives w(A0 ) = O( s log p).
Having established a restricted eigenvalue result in Lemma 1, next we use the result in [26] for parameter recovery in generalized linear models (GLMs) to show that `1 regularized logistic regression
can recover the Bayes classifier parameters.
q
Lemma 2. When the number of samples n  s0 log p, and choose Î» = c0 logn p for some constant
c0 , then we have
s0 log p
kwâˆ— âˆ’ wÌ‚k22 + (bâˆ— âˆ’ bÌ‚)2 = O(
)
(13)
n
with probability at least 1 âˆ’ O( p1c1 + n1c2 ), where c1 , c2 > 0 are constants.
Proof. Following the proof of Lemma 1, we see that the conditions (GLM1) and (GLM2) in [26]
are satisfied. Following the proof of Proposition 2 and Corollary 5 in [26], we have the desired
result. Although the proof of Proposition 2 and Corollary 5 in [26] is for zero-centered random
variables, the proof remains valid for non zero-centered random variables.
Combining Lemma 2 and Theorem 1 we have the following theorem which gives a fast rate for the
excess 0-1 risk of a classifier trained using `1 regularized logistic regression.
6

Theorem 2. With probability at least 1 âˆ’ O( p1c1 + n1c2 ) where c1 , c2 > 0 are constants, when we
q
set Î» = c0 logn p for some constant c0 the Lasso estimate wÌ‚, bÌ‚ in (10) satisfies
R(wÌ‚, bÌ‚) âˆ’ R(wâˆ— , bâˆ— ) = O(

s log p
)
n

(14)

Proof. This follows from Lemma 2 and Theorem 1.

5

Other Linear Discriminant Classifiers

In this section, we provide convergence results for the 0-1 risk for other linear discriminant classifiers
discussed in Section 2.3.
Naive Bayes We compare the discriminative approach using `1 logistic regression to the generative approach
 For
 using naive Bayes.
 illustration purposes we conside the case where Î£ = Ip , Âµ1 =
1
1
s
s
M
M
âˆš1
and Âµ0 = âˆ’ âˆšs0
. where 0 < B1 â‰¤ M1 , M0 â‰¤ B2 are unknown but bounded
s 0pâˆ’s
0pâˆ’s


1s
M1âˆš
+M0
âˆ—
and bâˆ— = 12 (âˆ’M12 + M02 ). Using naive Bayes we esconstants. In this case w =
s
0pâˆ’s
P
P
timate wÌ‚ = ÂµÂ¯1 âˆ’ ÂµÂ¯0 , where ÂµÂ¯1 = P 1(Y1 (i) =1 ) Y (i) =1 X (i) and ÂµÂ¯0 = P 1(Y1 (i) =0 ) Y (i) =0 X (i) .
Thus with high probability, we have kwÌ‚ âˆ’ wâˆ— k22 = O( np ), using Theorem 1 we get a slower rate
than the bound given in Theorem 2 for discriminative classification using `1 regularized logistic
regression.
LPD [7]

LPD uses a linear programming similar to the Dantzig selector.
q
p
Lemma 3 (Cai and Liu [7], Theorem 4). Let Î»n = C s log
with C being a sufficiently large
n
constant. Let n > log p, let âˆ† = (Âµ1 âˆ’ Âµ0 )T Î£ âˆ’1 (Âµ1 âˆ’ Âµ0 ) > c1 for some constant c1 > 0, and
âˆ’1
let wLPD be obtained
q as in Equation 4, then with probability greater than 1 âˆ’ O(p ), we have
R(wLPD)
Râˆ—

âˆ’ 1 = O(

s log p
n ).

SLDA [28] SLDA uses thresholded estimate for Î£ and Âµ1 âˆ’ Âµ0 . We state a simpler version.
R(w

SLDA)
âˆ’1 =
Lemma 4 ([28], Theorem 3). Assume that Î£ and Âµ1 âˆ’ Âµ0 are sparse, then we have
Râˆ—
S log p Î±2
s log p Î±1
O(max(( n ) , ( n ) ) with high probability, where s = kÂµ1 âˆ’ Âµ0 k0 , S is the number of
non-zero entries in Î£, and Î±1 , Î±2 âˆˆ (0, 12 ) are constants.

ROAD [15] ROAD minimizes wT Î£w with wT Âµ restricted to be a constant value and an `1 -penalty
of w.
q
Lemma 5 (Fan et al. [15], Theorem 1). Assume that with high probability, ||Î£Ì‚ âˆ’Î£||âˆ = O( logn p )
q
and ||ÂµÌ‚âˆ’Âµ||âˆ = O( logn p ), and let wROAD be obtained as in Equation 3, then with high probability,
q
p
we have R(wROAD ) âˆ’ Râˆ— = O( s log
n ).

6

Experiments

In this section we describe experiments which illustrate the rates for excess 0-1 risk given in Theorem
2. In our experiments we use Glmnet [19] where we set the option to penalize the intercept term
along with all other parameters. Glmnet is popular package for `1 regularized logistic regression
using coordinate descent methods.


1s
1
âˆš
For illustration purposes in all simulations we use Î£ = Ip , Âµ1 = 1p + s
, Âµ0 = 1p âˆ’
0pâˆ’s


1s
âˆš1
To illustrate our bound in Theorem 2, we consider three different scenarios. In Figure 1a
s 0pâˆ’s
7

0.5

classification error

0.4
0.35
0.3
0.25
0.2
0

0.25
s=5
s=10
s=15
s=20

0.45
classification error

p=100
p=400
p=1600

0.4
0.35
0.3
0.25

0.2
excess 0âˆ’1 risk

0.5
0.45

0.15
0.1
0.05

0.2
100

200
300
n/log(p)

(a) Only varying p.

400

0

100

200
n/s

300

(b) Only varying s.

400

0
0

0.005

0.01
1/n

0.015

(c) Dependence of excess 0-1 risk
on n.

Figure 1: Simulations for different Gaussian classification problems showing the dependence of
classification error on different quantities. All experiments
plotted the average of 20 trials. In all
q
experiments we set the regularization parameter Î» =

log p
n .

we vary p while keeping s, (Âµ1 âˆ’ Âµ0 )T Î£ âˆ’1 (Âµ1 âˆ’ Âµ0 ) constant. Figure 1a shows for different p how
the classification error changes with increasing n. In Figure 1a we show the relationship between
the classification error and the quantity logn p . This figure agrees with our result on excess 0-1 riskâ€™s
dependence on p. In Figure 1b we vary s while keeping p, (Âµ1 âˆ’ Âµ0 )T Î£ âˆ’1 (Âµ1 âˆ’ Âµ0 ) constant.
Figure 1b shows for different s how the classification error changes with increasing n. In Figure 1a
we show the relationship between the classification error and the quantity ns . This figure agrees with
our result on excess 0-1 riskâ€™s dependence on s. In Figure 1c we show how R(wÌ‚, bÌ‚) âˆ’ R(wâˆ— , bâˆ— )
changes with respect to n1 in one instance Gaussian classification. We can see that the excess 0-1
risk achieves the fast rate and agrees with our bound.

Acknowledgements
We acknowledge the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS1320894, IIS-1447574, and DMS-1264033, and NIH via R01 GM117594-01 as part of the Joint
DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical
Sciences.

References
[1] Arindam Banerjee, Sheng Chen, Farideh Fazayeli, and Vidyashankar Sivakumar. Estimation with norm
regularization. In Advances in Neural Information Processing Systems, pages 1556â€“1564, 2014.
[2] Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds.
Journal of the American Statistical Association, 101(473):138â€“156, 2006.
[3] Peter J Bickel and Elizaveta Levina. Some theory for Fisherâ€™s linear discriminant function, â€˜naive Bayesâ€™,
and some alternatives when there are many more variables than observations. Bernoulli, pages 989â€“1010,
2004.
[4] Peter J Bickel and Elizaveta Levina. Covariance regularization by thresholding. The Annals of Statistics,
pages 2577â€“2604, 2008.
[5] C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer,
2006. ISBN 9780387310732.
[6] Peter BuÌˆhlmann and Sara Van De Geer. Statistics for high-dimensional data: methods, theory and applications. Springer Science & Business Media, 2011.
[7] Tony Cai and Weidong Liu. A direct estimation approach to sparse linear discriminant analysis. Journal
of the American Statistical Association, 106(496), 2011.
[8] Emmanuel Candes and Terence Tao. The dantzig selector: statistical estimation when p is much larger
than n. The Annals of Statistics, pages 2313â€“2351, 2007.
[9] Venkat Chandrasekaran, Benjamin Recht, Pablo A Parrilo, and Alan S Willsky. The convex geometry of
linear inverse problems. Foundations of Computational Mathematics, 12(6):805â€“849, 2012.

8

[10] Weizhu Chen, Zhenghao Wang, and Jingren Zhou. Large-scale L-BFGS using MapReduce. In Advances
in Neural Information Processing Systems, pages 1332â€“1340, 2014.
[11] L. Devroye, L. GyoÌˆrfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer New York,
1996.
[12] Luc Devroye. A probabilistic theory of pattern recognition, volume 31. Springer Science & Business
Media, 1996.
[13] David Donoho and Jiashun Jin. Higher criticism thresholding: Optimal feature selection when useful
features are rare and weak. Proceedings of the National Academy of Sciences, 105(39):14790â€“14795,
2008.
[14] Jianqing Fan and Yingying Fan. High dimensional classification using features annealed independence
rules. Annals of statistics, 36(6):2605, 2008.
[15] Jianqing Fan, Yang Feng, and Xin Tong. A road to classification in high dimensional space: the regularized optimal affine discriminant. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(4):745â€“771, 2012.
[16] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. LIBLINEAR: A
library for large linear classification. The Journal of Machine Learning Research, 9:1871â€“1874, 2008.
[17] Yingying Fan, Jiashun Jin, Zhigang Yao, et al. Optimal classification in sparse gaussian graphic model.
The Annals of Statistics, 41(5):2537â€“2571, 2013.
[18] Manuel FernaÌndez-Delgado, Eva Cernadas, SeneÌn Barro, and Dinani Amorim. Do we need hundreds of
classifiers to solve real world classification problems? The Journal of Machine Learning Research, 15
(1):3133â€“3181, 2014.
[19] Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for generalized linear models
via coordinate descent. Journal of statistical software, 33(1):1, 2010.
[20] Siddharth Gopal and Yiming Yang. Distributed training of Large-scale Logistic models. In Proceedings
of the 30th International Conference on Machine Learning (ICML-13), pages 289â€“297, 2013.
[21] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data Mining, Inference,
and Prediction. Springer, 2009.
[22] Mladen Kolar and Han Liu. Feature selection in high-dimensional classification. In Proceedings of the
30th International Conference on Machine Learning (ICML-13), pages 329â€“337, 2013.
[23] Vladimir Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems: Ecole dEteÌ de ProbabiliteÌs de Saint-Flour XXXVIII-2008, volume 2033. Springer Science & Business Media, 2011.
[24] Qing Mai, Hui Zou, and Ming Yuan. A direct approach to sparse discriminant analysis in ultra-high
dimensions. Biometrika, page asr066, 2012.
[25] Enno Mammen, Alexandre B Tsybakov, et al. Smooth discrimination analysis. The Annals of Statistics,
27(6):1808â€“1829, 1999.
[26] Sahand Negahban, Bin Yu, Martin J Wainwright, and Pradeep K Ravikumar. A unified framework for
high-dimensional analysis of M -estimators with decomposable regularizers. In Advances in Neural Information Processing Systems, pages 1348â€“1356, 2009.
[27] Andrew Y. Ng and Michael I. Jordan. On discriminative vs. generative classifiers: A comparison of
logistic regression and naive bayes. In Advances in Neural Information Processing Systems 14 (NIPS
2001), 2001.
[28] Jun Shao, Yazhen Wang, Xinwei Deng, Sijian Wang, et al. Sparse linear discriminant analysis by thresholding for high dimensional data. The Annals of Statistics, 39(2):1241â€“1265, 2011.
[29] Robert Tibshirani, Trevor Hastie, Balasubramanian Narasimhan, and Gilbert Chu. Diagnosis of multiple
cancer types by shrunken centroids of gene expression. Proceedings of the National Academy of Sciences,
99(10):6567â€“6572, 2002.
[30] Sijian Wang and Ji Zhu. Improved centroids estimation for the nearest shrunken centroid classifier. Bioinformatics, 23(8):972â€“979, 2007.
[31] Tong Zhang. Statistical behavior and consistency of classification methods based on convex risk minimization. Annals of Statistics, pages 56â€“85, 2004.

9

