Adaptive Primal-Dual Splitting Methods for
Statistical Learning and Image Processing
Thomas Goldstein⇤
Department of Computer Science
University of Maryland
College Park, MD

Min Li†
School of Economics and Management
Southeast University
Nanjing, China

Xiaoming Yuan‡
Department of Mathematics
Hong Kong Baptist University
Kowloon Tong, Hong Kong

Abstract
The alternating direction method of multipliers (ADMM) is an important tool for
solving complex optimization problems, but it involves minimization sub-steps
that are often difficult to solve efficiently. The Primal-Dual Hybrid Gradient
(PDHG) method is a powerful alternative that often has simpler sub-steps than
ADMM, thus producing lower complexity solvers. Despite the flexibility of this
method, PDHG is often impractical because it requires the careful choice of multiple stepsize parameters. There is often no intuitive way to choose these parameters
to maximize efficiency, or even achieve convergence. We propose self-adaptive
stepsize rules that automatically tune PDHG parameters for optimal convergence.
We rigorously analyze our methods, and identify convergence rates. Numerical
experiments show that adaptive PDHG has strong advantages over non-adaptive
methods in terms of both efficiency and simplicity for the user.

1

Introduction

Splitting methods such as ADMM [1, 2, 3] have recently become popular for solving problems
in distributed computing, statistical regression, and image processing. ADMM allows complex
problems to be broken down into sequences of simpler sub-steps, usually involving large-scale least
squares minimizations. However, in many cases these least squares minimizations are difficult to
directly compute. In such situations, the Primal-Dual Hybrid Gradient method (PDHG) [4, 5],
also called the linearized ADMM [4, 6], enables the solution of complex problems with a simpler
sequence of sub-steps that can often be computed in closed form. This flexibility comes at a cost
– the PDHG method requires the user to choose multiple stepsize parameters that jointly determine
the convergence of the method. Without having extensive analytical knowledge about the problem
being solved (such as eigenvalues of linear operators), there is no intuitive way to select stepsize
parameters to obtain fast convergence, or even guarantee convergence at all.
In this article we introduce and analyze self-adaptive variants of PDHG – variants that automatically
tune stepsize parameters to attain (and guarantee) fast convergence without user input. Applying
adaptivity to splitting methods is a difficult problem. It is known that naive adaptive variants of
⇤

tomg@cs.umd.edu
limin@seu.edu.cn
‡
xmyuan@hkbu.edu.hk
†

1

ADMM are non-convergent, however recent results prove convergence when specific mathematical
requirements are enforced on the stepsizes [7]. Despite this progress, the requirements for convergence of adaptive PDHG have been unexplored. This is surprising, given that stepsize selection is a
much bigger issue for PDHG than for ADMM because it requires multiple stepsize parameters.
The contributions of this paper are as follows. First, we describe applications of PDHG and its
advantages over ADMM. We then introduce a new adaptive variant of PDHG. The new algorithm not
only tunes parameters for fast convergence, but contains a line search that guarantees convergence
when stepsize restrictions are unknown to the user. We analyze the convergence of adaptive PDHG,
and rigorously prove convergence rate guarantees. Finally, we use numerical experiments to show
the advantages of adaptivity on both convergence speed and ease of use.

2

The Primal-Dual Hybrid Gradient Method

The PDHG scheme has its roots in the Arrow-Hurwicz method, which was studied by Popov [8].
Research in this direction was reinvigorated by the introduction of PDHG, which converges rapidly
for a wider range of stepsizes than Arrow-Hurwicz. PDHG was first presented in [9] and analyzed
for convergence in [4, 5]. It was later studied extensively for image segmentation [10]. An extensive
technical study of the method and its variants is given by He and Yuan [11]. Several extensions
of PDHG, including simplified iterations for the case that f or g is differentiable, are presented by
Condat [12]. Several authors have also derived PDHG as a preconditioned form of ADMM [4, 6].
PDHG solves saddle-point problems of the form
min max f (x) + y T Ax

x2X y2Y

g(y).

(1)

for convex f and g. We will see later that an incredibly wide range of problems can be cast as (1).
The steps of PDHG are given by
8 k+1
x̂
= x k ⌧ k AT y k
>
>
>
>
1
>
k+1
>
= arg min f (x) +
kx x̂k+1 k2
>
<x
2⌧
k
x2X
k+1
k
k+1
>
ŷ
=
y
+
A(2x
xk )
k
>
>
>
>
1
>
>
: y k+1 = arg min g(y) +
ky ŷ k+1 k2
2 k
y2Y

(2)
(3)
(4)
(5)

where {⌧k } and { k } are stepsize parameters. Steps (2) and (3) of the method update x, decreasing
the energy (1) by first taking a gradient descent step with respect to the inner product term in (1)
and then taking a “backward” or proximal step involving f . In steps (4) and (5), the energy (1) is
increased by first marching up the gradient of the inner product term with respect to y, and then a
backward step is taken with respect to g.
PDHG has been analyzed in the case of constant stepsizes, ⌧k = ⌧ and k = . In particular,
it is known to converge as long as ⌧ < 1/⇢(AT A) [4, 5, 11]. However, PDHG typically does
not converge when non-constant stepsizes are used, even in the case that k ⌧k < 1/⇢(AT A) [13].
Furthermore, it is unclear how to select stepsizes when the spectral properties of A are unknown. In
this article, we identify the specific stepsize conditions that guarantee convergence in the presence
of adaptivity, and propose a backtracking scheme that can be used when the spectral radius of A is
unknown.

3

Applications

Linear Inverse Problems Many inverse problems and statistical regressions have the form
minimize

h(Sx) + f (Ax

b)

(6)

where f (the data term) is some convex function, h is a (convex) regularizer (such as the `1 -norm),
A and S are linear operators, and b is a vector of data. Recently, the alternating direction method
2

of multipliers (ADMM) has become a popular method for solving such problems. The ADMM
relies on the change of variables y
Sx, and generates the following sequence of iterates for some
stepsize ⌧
8 k+1
= arg minx f (Ax b) + (Sx y k )T k + ⌧2 kSx y k k2
<x
k+1
(7)
y
= arg miny h(y) + (Sxk+1 y)T k + ⌧2 kSxk+1 yk2
: k+1
= k + ⌧ (Sxk+1 y k+1 ).

The x-update in (7) requires the solution of a (potentially large) least-square problem involving both
A and S. Common formulations such as the consensus ADMM [14] solve these large sub-problems
with direct matrix factorizations, however this is often impractical when either the data matrices are
extremely large or fast transforms (such as FFT, DCT, or Hadamard) cannot be used.
The problem (6) can be put into the form (1) using the Fenchel conjugate of the convex function h,
denoted h⇤ , which satisfies the important identity
h(z) = max y T z
y

h⇤ (y)

for all z in the domain of h. Replacing h in (6) with this expression involving its conjugate yields
min max f (Ax
x

y

b) + y T Sx

h⇤ (y)

which is of the form (1). The forward (gradient) steps of PDHG handle the matrix A explicitly,
allowing linear inverse problems to be solved without any difficult least-squares sub-steps. We will
see several examples of this below.
Scaled Lasso The square-root lasso [15] or scaled lasso [16] is a variable selection regression that
obtains sparse solutions to systems of linear equations. Scaled lasso has several advantages over
classical lasso – it is more robust to noise and it enables setting penalty parameters without cross
validation [15, 16]. Given a data matrix D and a vector b, the scaled lasso finds a sparse solution to
the system Dx = b by solving
min µkxk1 + kDx bk2
(8)
x

for some scaling parameter µ. Note the `2 term in (8) is not squared as in classical lasso. If we write
µkxk1 =

max y1T x,

ky1 k1 µ

and

kDx

bk2 = max y2T (Dx
ky2 k2 1

b)

we can put (8) in the form (1)
min
x

max

ky1 k1 µ,ky2 k2 1

y1T x + y2T (Dx

b).

(9)

Unlike ADMM, PDHG does not require the solution of least-squares problems involving D.
Total-Variation Minimization
form

Total variation [17] is commonly used to solve problems of the

1
min µkrxk1 + kAx f k22
(10)
x
2
where x is a 2D array (image), r is the discrete gradient operator, A is a linear operator, and f
contains data. If we add a dual variable y and write µkrxk1 = maxkyk1 µ y T rx, we obtain
max min

kyk1 µ

x

1
kAx
2

f k2 + y T rx

(11)

which is clearly of the form (1).
The PDHG solver using formulation (11) avoids the inversion of the gradient operator that is required
by ADMM. This is useful in many applications. For example, in compressive sensing the matrix A
may be a sub-sampled orthogonal Hadamard [18], wavelet, or Fourier transform [19, 20]. In this
case, the proximal sub-steps of PDHG are solvable in closed form using fast transforms because they
do not involve the gradient operator r. The sub-steps of ADMM involve both the gradient operator
and the matrix A simultaneously, and thus require inner loops with expensive iterative solvers.
3

4

Adaptive Formulation

The convergence of PDHG can be measured by the size of the residuals, or gradients of (1) with
respect to the primal and dual variables x and y. These primal and dual gradients are simply
pk+1 = @f (xk+1 ) + AT y k+1 ,

and

dk+1 = @g(y k+1 ) + Axk+1

(12)

where @f and @g denote the sub-differential of f and g. The sub-differential can be directly evaluated from the sequence of PDHG iterates using the optimality condition for (3): 0 2 @f (xk+1 ) +
1
k+1
x̂k+1 ). Rearranging this yields ⌧1k (x̂k+1 xk+1 ) 2 @f (xk+1 ). The same method can be
⌧k (x
applied to (5) to obtain @g(y k+1 ). Applying these results to (12) yields the closed form residuals
pk+1 =

1 k
(x
⌧k

xk+1 )

AT (y k

y k+1 ),

dk+1 =

1

(y k

y k+1 )

A(xk

xk+1 ).

(13)

k

When choosing the stepsize for PDHG, there is a tradeoff between the primal and dual residuals.
Choosing a large ⌧k and a small k drives down the primal residuals at the cost of large dual residuals. Choosing a small ⌧k and large k results in small dual residuals but large primal errors. One
would like to choose stepsizes so that the larger of pk+1 and dk+1 is as small as possible. If we assume the residuals on step k+1 change monotonically with ⌧k , then max{pk+1 , dk+1 } is minimized
when pk+1 = dk+1 . This suggests that we tune ⌧k to “balance” the primal and dual residuals.
To achieve residual balancing, we first select a parameter ↵0 < 1 that controls the aggressiveness of
adaptivity. On each iteration, we check whether the primal residual is at least twice the dual. If so,
we increase the primal stepsize to ⌧k+1 = ⌧k /(1 ↵k ) and decrease the dual to k+1 = k (1 ↵k ).
If the dual residual is at least twice the primal, we do the opposite. When we modify the stepsize, we
shrink the adaptivity level to ↵k+1 = ⌘↵k , for ⌘ 2 (0, 1). We will see in Section 5 that this adaptivity
level decay is necessary to guarantee convergence. In our implementation we use ↵0 = ⌘ = .95.
In addition to residual balancing, we check the following backtracking condition after each iteration
c
kxk+1
2⌧k

xk k2

2(y k+1

y k )T A(xk+1

xk ) +

c
2

k

ky k+1

y k k2 > 0

(14)

where c 2 (0, 1) is a constant (we use c = 0.9) is our experiments. If condition (14) fails, then we
shrink ⌧k and k before the next iteration. We will see in Section 5 that the backtracking condition
(14) is sufficient to guarantee convergence. The complete scheme is listed in Algorithm 1.
Algorithm 1 Adaptive PDHG
1: Choose x0 , y 0 , large ⌧0 and 0 , and set ↵0 = ⌘ = 0.95.
2: while kpk k, kdk k > tolerance do
3:
Compute (xk+1 , y k+1 ) from (xk , y k ) using the PDHG updates (2-5)
4:
Check the backtracking condition (14) and if it fails set ⌧k
⌧k /2, k
k /2
5:
Compute the residuals (13), and use them for the following two adaptive updates
6:
If 2kpk+1 k < kdk+1 k, then set ⌧k+1 = ⌧k (1 ↵k ), k+1 = k /(1 ↵k ), and ↵k+1 = ↵k ⌘
7:
If kpk+1 k > 2kdk+1 k, then set ⌧k+1 = ⌧k /(1 ↵k ), k+1 = k (1 ↵k ), and ↵k+1 = ↵k ⌘
8:
If no adaptive updates were triggered, then ⌧k+1 = ⌧k , k+1 = k , and ↵k+1 = ↵k
9: end while

5

Convergence Theory

In this section, we analyze Algorithm 1 and its rate of convergence. In our analysis, we consider
adaptive variants of PDHG that satisfy the following assumptions. We will see later that these
assumptions guarantee convergence of PDHG with rate O(1/k).
Algorithm 1 trivially satisfies Assumption A. The sequence { k } measures the adaptive aggressiveness on iteration k, and serves the same role as ↵k in Algorithm 1. The geometric decay of ↵k
ensures that Assumption B holds. The backtracking rule explicitly guarantees Assumption C.
4

Assumptions for Adaptive PDHG
A The sequences {⌧k } and {

are positive and bounded.
n
B The sequence { k } is summable, where k = max ⌧k ⌧⌧kk+1 ,
k}

k

k+1
k

o
,0 .

C Either X or Y is bounded, and there is a constant c 2 (0, 1) such that for all k > 0
c
c
kxk+1 xk k2 2(y k+1 y k )T A(xk+1 xk ) +
ky k+1 y k k2 > 0.
2⌧k
2 k

5.1

Variational Inequality Formulation

For notational simplicity, we define the composite vector uk = (xk , y k ) and the matrices
✓ 1
◆
✓ 1
◆
✓
◆
⌧ I
AT
⌧k I
0
AT y
Mk = k
,
H
=
,
and
Q(u)
=
.
k
1
1
Ax
A
0
k I
k I

(15)

This notation allows us to formulate the optimality conditions for (1) as a variational inequality (VI).
If u? = (x? , y ? ) is a solution to (1), then x? is a minimizer of (1). More formally,
f (x? ) + (x

f (x)

x ? ) T AT y ?

0

8 x 2 X.

(16)

y ? )T Ax?  0

8 y 2 Y.

(17)

8u 2 ⌦,

(18)

Likewise, (1) is maximized by y , and so
?

g(y) + g(y ? ) + (y

Subtracting (17) from (16) and letting h(u) = f (x) + g(y) yields the VI formulation
h(u)

h(u? ) + (u

u? )T Q(u? )

0

where ⌦ = X ⇥ Y. We say ũ is an approximate solution to (1) with VI accuracy ✏ if
h(u)

h(ũ) + (u

ũ)T Q(ũ)

✏

8u 2 B1 (ũ) \ ⌦,

(19)

where B1 (ũ) is a unit ball centered at ũ. In Theorem 1, we prove O(1/k) ergodic convergence of
adaptive PDHG using the VI notion of convergence.
5.2

Preliminary Results

We now prove several results about the PDHG iterates that are needed to obtain a convergence rate.
Lemma 1. The iterates generated by PDHG (2-5) satisfy
kuk

u? k2Mk

kuk+1

uk k2Mk + kuk+1

u? k2Mk .

The proof of this lemma follows standard techniques, and is presented in the supplementary material.
This next lemma bounds iterates generated by PDHG.
Lemma 2. Suppose the stepsizes for PDHG satisfy Assumptions A, B and C. Then
kuk

for some upper bound CU > 0.

u? k2Hk  CU

The proof of this lemma is given in the supplementary material.
Lemma 3. Under Assumptions A, B, and C, we have
n ⇣
X

kuk

k=0

k

k=1

where C =

P1

uk2Mk

kuk

uk2Mk

1

⌘

 2C CU + 2C CH ku

and CH is a constant such that ku
5

u? k2Hk  CH ku

u ? k2
u? k2 .

Proof. Using the definition of Mk we obtain
n ⇣
X

k=1

kuk

uk2Mk

n 
X
1
=
(
⌧k


=

k=1
n
X
k=1
n
X

k=1
n
X

2
2

k=1
n
X

k 1

kuk

uk2Mk

1
)kxk
⌧k 1
✓

1 k
kx
⌧k

k 1 ku

k

1

1

k

xk2 +

1
k

ky k

)ky k

yk2

k 1

yk2

◆
(20)

uk2Hk

kuk

k 1

CU + CH ku

u? k2Hk + ku

 2C CU + 2C CH ku
where we have used the bound kuk

⌘

xk2 + (

k 1

k=1

1

u? k2Hk

u? k2

u ? k2 ,

u? k2Hk  CU from Lemma 2 and C =

P1

k=0

k.

This final lemma provides a VI interpretation of the PDHG iteration.
Lemma 4. The iterates uk = (xk , y k ) generated by PDHG satisfy
h(u)

h(uk+1 ) + (u

uk+1 )T [Quk+1 + Mk (uk+1

uk )]

0

8u 2 ⌦.

(21)

Proof. Let uk = (xk , y k ) be a pair of PDHG iterates. The minimizers in (3) and (5) of PDHG
satisfy the following for all x 2 X
f (x)

f (xk+1 ) + (x

xk+1 )T [AT y k+1

AT (y k+1

y k+1 )T [ Axk+1

A(xk+1

yk ) +

1 k+1
(x
⌧k

xk )]

1

y k )]

0,

(22)

and also for all y 2 Y
g(y)

g(y k+1 ) + (y

xk ) +

(y k+1

0.

(23)

k

Adding these two inequalities and using the notation (15) yields the result.
5.3

Convergence Rate

We now combine the above lemmas into our final convergence result.
Theorem 1. Suppose that the stepsizes in PDHG satisfy Assumptions A, B, and C. Consider the
sequence defined by
t
1X k
ũt =
u .
t
k=1

This sequence satisfies the convergence bound
h(u)

h(ũt ) + (u

ũt )T Q(ũt )

ku

ũt k2Mt

ku

u0 k2M0

2C CU
2t

Thus ũt converges to a solution of (1) with rate O(1/k) in the VI sense (19).
6

2C CH ku

u? k2

.

Proof. We begin with the following identity (a special case of the polar identity for vector spaces):
1
1
(ku uk+1 k2Mk
ku uk k2Mk ) + kuk
2
2
We apply this to the VI formulation of the PDHG iteration (18) to get
(u

uk+1 )T Mk (uk

h(uk+1 ) + (u

h(u)

uk+1 ) =

uk+1 )T Q(uk+1 )
1
ku uk+1 k2Mk
2

1
uk k2Mk + kuk
2

ku

uk+1 k2Mk .

uk+1 k2Mk .

(24)

xk+1 ) = 0,

(25)

Note that
(u

uk+1 )T Q(u

uk+1 ) = (x

y k+1 )

y k+1 )A(x

(y

uk+1 )T Q(u) = (u uk+1 )T Q(uk+1 ). Also, Assumption C guarantees that kuk
0. These observations reduce (24) to

and so (u
uk+1 k2Mk

h(u)

1
ku uk+1 k2Mk
2
1, and invoke Lemma 3,

h(uk+1 ) + (u

uk+1 )T Q(u)

We now sum (26) for k = 0 to t
2

xk+1 )AT (y

t 1
X

[h(u)

h(uk+1 ) + (u

uk k2Mk .

ku

(26)

uk+1 )T Q(u)]

k=0

Because h is convex,

ku

ut k2Mt

ku

u0 k2M0 +

ku

ut k2Mt

ku

u0 k2M0

t 1
X

h(u

k+1

)=

k=0

t
X

t h
X

k=1

uk k2Mk

ku

2C CU

h(u )

th

k=1

1X k
u
t
k=1

!

uk k2Mk

u? k2 .

2C CH ku

t

k

1

ku

i

(27)

= th(ũt ).

The left side of (27) therefore satisfies
2t h(u)

h(ũt ) + (u

ũt )T Q(u)

2

t 1
X
⇥

h(u)

h(uk+1 ) + (u

k=0

Combining (27) and (28) yields the tasty bound
h(u)

h(ũt ) + (u

ũt )T Q(u)

ku

ut k2Mt

ku

u0 k2M0

2C CU
2t

⇤
uk+1 )T Q(u) .
2C CH ku

(28)

u? k2

.

Applying (19) proves the theorem.

6

Numerical Results

We apply the original and adaptive PDHG to the test problems described in Section 3. We terminate
the algorithms when both the primal and dual residual norms (i.e. kpk k and kdk k) are smaller
than 0.05. We consider four variants of PDHG. The method “Adapt:Backtrack” denotes adaptive
PDHG with backtracking. The method “Adapt: ⌧ = L” refers to the adaptive method without
1
backtracking with ⌧0 = 0 = 0.95⇢(AT A) 2 .
We alsop
consider the non-adaptive PDHG with two different stepsize choices. The method “Const:
p
⌧, = L” refers to the constant-stepsize method with both stepsize parameters equal to L =
1
⇢(AT A) 2 . The method “Const: ⌧ -final” refers to the constant-stepsize method, where the stepsizes
are chosen to be the final values of the stepsizes used by “Adapt: ⌧ = L.” This final method is
meant to demonstrate the performance of PDHG with a stepsize that is customized to the problem
at hand, but still non-adaptive. The specifics of each test problem are described below:
7

Primal Stepsize (τ k )

ROF Convergence Curves, µ = 0.05

7

10

12

6

A d a p t: Ba cktra ck
A d a p t: τ σ = L
√
Co n st: τ = L
Co n st: τ -fi n al

10

5

A d a p t: Ba cktra ck
A d a p t: τ σ = L

8
4

10

τk

Energy Gap

10

10

6

3

10

4

2

10

2

1

10

0

10

0
0

50

100

150
200
Iteration

250

300

0

50

100

150
200
Iteration

250

300

Figure 1: (left) Convergence curves for the TV denoising experiment with µ = 0.05. The y-axis
displays the difference between the objective (10) at the kth iterate and the optimal objective value.
(right) Stepsize sequences, {⌧k }, for both adaptive schemes.
Table 1: Iteration counts for each problem with runtime (sec) in parenthesis.

Problem
Scaled Lasso (50%)
Scaled Lasso (20%)
Scaled Lasso (10%)
TV, µ = .25
TV, µ = .05
TV, µ = .01
Compressive (20%)
Compressive (10%)
Compressive (5%)

Adapt:Backtrack
212 (0.33)
349 (0.22)
360 (0.21)
16 (0.0475)
50 (0.122)
109 (0.262)
163 (4.08)
244 (5.63)
382 (9.54)

Adapt: ⌧ = L
240 (0.38)
330 (0.21)
322 (0.18)
16 (0.041)
51 (0.122)
122 (0.288)
168 (4.12)
274 (6.21)
438 (10.7)

p
Const: ⌧, = L
342 (0.60)
437 (0.25)
527 (0.28)
78 (0.184)
281 (0.669)
927 (2.17)
501 (12.54)
908 (20.6)
1505 (34.2)

Const: ⌧ -final
156 (0.27)
197 (0.11)
277 (0.15)
48 (0.121)
97 (0.228)
152 (0.369)
246 (6.03)
437 (9.94)
435 (9.95)

Scaled Lasso We test our methods on (8) using the synthetic problem suggested in [21]. The test
problem recovers a 1000 dimensional vector with 10 nonzero components using a Gaussian matrix.
Total Variation Minimization We apply the model (10) with A = I to the “Cameraman” image.
The image is scaled to the range [0, 255], and noise contaminated with standard deviation 10. The
image is denoised with µ = 0.25, 0.05, and 0.01. See Table 1 for time trial results. Note the similar
performance of Algorithm 1 with and without backtracking, indicating that there is no advantage to
knowing the constant L = ⇢(AT A) 1 . We plot convergence curves and show the evolution of ⌧k in
Figure 1. Note that ⌧k is large for the first several iterates and then decays over time.
Compressed Sensing We reconstruct a Shepp-Logan phantom from sub-sampled Hadamard measurements. Data is generated by applying the Hadamard transform to a 256 ⇥ 256 discretization of
the Shepp-Logan phantom, and then sampling 5%, 10%, and 20% of the coefficients are random.

7

Discussion and Conclusion

Several interesting observations can be made from the results in Table 1. First, both the backtracking
(“Adapt: Backtrack”) and non-backtracking (“Adapt: ⌧ = L”) methods have similar performance
on average for the imaging problems, with neither algorithm showing consistently better performance. Thus there is no cost to using backtracking instead of knowing the ideal stepsize ⇢(AT A).
Finally, the method “Const: ⌧ -final” (using non-adaptive, “optimized” stepsizes) did not always outperform the constant, non-optimized stepsizes. This occurs because the true “best” stepsize choice
depends on the active set of the problem and the structure of the remaining error and thus evolves
over time. This is depicted in Figure 1, which shows the time dependence of ⌧k . This show that
adaptive methods can achieve superior performance by evolving the stepsize over time.

8

Acknowledgments

This work was supported by the National Science Foundation ( #1535902), the Office of Naval
Research (#N00014-15-1-2676), and the Hong Kong Research Grants Council’s General Research
Fund (HKBU 12300515). The second author was supported in part by the Program for New Century
Excellent University Talents under Grant No. NCET-12-0111, and the Qing Lan Project.
8

References
[1] R. Glowinski and A. Marroco. Sur l’approximation, par éléments finis d’ordre un, et la résolution, par
pénalisation-dualité d’une classe de problèmes de Dirichlet non linéaires. Rev. Française d’Automat. Inf.
Recherche Opérationelle, 9(2):41–76, 1975.
[2] Roland Glowinski and Patrick Le Tallec. Augmented Lagrangian and Operator-Splitting Methods in
Nonlinear Mechanics. Society for Industrial and Applied Mathematics, Philadephia, PA, 1989.
[3] Tom Goldstein and Stanley Osher. The Split Bregman method for `1 regularized problems. SIAM J. Img.
Sci., 2(2):323–343, April 2009.
[4] Ernie Esser, Xiaoqun Zhang, and Tony F. Chan. A general framework for a class of first order primal-dual
algorithms for convex optimization in imaging science. SIAM Journal on Imaging Sciences, 3(4):1015–
1046, 2010.
[5] Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems with
applications to imaging. Convergence, 40(1):1–49, 2010.
[6] Yuyuan Ouyang, Yunmei Chen, Guanghui Lan, and Eduardo Pasiliao Jr. An accelerated linearized alternating direction method of multipliers. arXiv preprint arXiv:1401.6607, 2014.
[7] B. He, H. Yang, and S.L. Wang. Alternating direction method with self-adaptive penalty parameters for
monotone variational inequalities. Journal of Optimization Theory and Applications, 106(2):337–356,
2000.
[8] L.D. Popov. A modification of the arrow-hurwicz method for search of saddle points. Mathematical notes
of the Academy of Sciences of the USSR, 28:845–848, 1980.
[9] Mingqiang Zhu and Tony Chan. An efficient primal-dual hybrid gradient algorithm for total variation
image restoration. UCLA CAM technical report, 08-34, 2008.
[10] T. Pock, D. Cremers, H. Bischof, and A. Chambolle. An algorithm for minimizing the mumford-shah
functional. In Computer Vision, 2009 IEEE 12th International Conference on, pages 1133–1140, 2009.
[11] Bingsheng He and Xiaoming Yuan. Convergence analysis of primal-dual algorithms for a saddle-point
problem: From contraction perspective. SIAM J. Img. Sci., 5(1):119–149, January 2012.
[12] Laurent Condat. A primal-dual splitting method for convex optimization involving lipschitzian, proximable and linear composite terms. Journal of Optimization Theory and Applications, 158(2):460–479,
2013.
[13] Silvia Bonettini and Valeria Ruggiero. On the convergence of primal–dual hybrid gradient algorithms for
total variation image restoration. Journal of Mathematical Imaging and Vision, 44(3):236–253, 2012.
[14] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed Optimization and Statistical Learning
via the Alternating Direction Method of Multipliers. Foundations and Trends in Machine Learning, 2010.
[15] A. Belloni, Victor Chernozhukov, and L. Wang. Square-root lasso: pivotal recovery of sparse signals via
conic programming. Biometrika, 98(4):791–806, 2011.
[16] Tingni Sun and Cun-Hui Zhang. Scaled sparse linear regression. Biometrika, 99(4):879–898, 2012.
[17] L Rudin, S Osher, and E Fatemi. Nonlinear total variation based noise removal algorithms. Physica. D.,
60:259–268, 1992.
[18] Tom Goldstein, Lina Xu, Kevin Kelly, and Richard Baraniuk. The STONE transform: Multi-resolution
image enhancement and real-time compressive video. Preprint available at Arxiv.org (arXiv:1311.34056),
2013.
[19] M. Lustig, D. Donoho, and J. Pauly. Sparse MRI: The application of compressed sensing for rapid MR
imaging. Magnetic Resonance in Medicine, 58:1182–1195, 2007.
[20] Xiaoqun Zhang and J. Froment. Total variation based fourier reconstruction and regularization for computer tomography. In Nuclear Science Symposium Conference Record, 2005 IEEE, volume 4, pages
2332–2336, Oct 2005.
[21] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society, Series B, 58:267–288, 1994.

9

