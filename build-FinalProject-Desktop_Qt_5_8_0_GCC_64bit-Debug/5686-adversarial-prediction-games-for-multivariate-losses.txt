Adversarial Prediction Games for Multivariate Losses
Hong Wang

Wei Xing
Kaiser Asif
Brian D. Ziebart
Department of Computer Science
University of Illinois at Chicago
Chicago, IL 60607
{hwang27, wxing3, kasif2, bziebart}@uic.edu

Abstract
Multivariate loss functions are used to assess performance in many modern prediction tasks, including information retrieval and ranking applications. Convex
approximations are typically optimized in their place to avoid NP-hard empirical risk minimization problems. We propose to approximate the training data
instead of the loss function by posing multivariate prediction as an adversarial
game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data. This avoids
the non-convexity of empirical risk minimization, but game sizes are exponential
in the number of predicted variables. We overcome this intractability using the
double oracle constraint generation method. We demonstrate the efficiency and
predictive performance of our approach on tasks evaluated using the precision at
k, the F-score and the discounted cumulative gain.

1

Introduction

For many problems in information retrieval and learning to rank, the performance of a predictor is
evaluated based on the combination of predictions it makes for multiple variables. Examples include the precision when limited to k positive predictions (P@k), the harmonic mean of precision
and recall (F-score), and the discounted cumulative gain (DCG) for assessing ranking quality. These
stand in contrast to measures like the accuracy and (log) likelihood, which are additive over independently predicted variables. Many multivariate performance measures are not concave functions of
predictor parameters, so maximizing them over empirical training data (or, equivalently, empirical
risk minimization over a corresponding non-convex multivariate loss function) is computationally
intractable [11] and can only be accomplished approximately using local optimization methods [10].
Instead, convex surrogates for the empirical risk are optimized using either an additive [21, 12, 22]
or a multivariate approximation [14, 24] of the loss function. For both types of approximations,
the gap between the application performance measure and the surrogate loss measure can lead to
substantial sub-optimality of the resulting predictions [4].
Rather than optimizing an approximation of the multivariate loss for available training data, we take
an alternate approach [26, 9, 1] that robustly minimizes the exact multivariate loss function using
approximations of the training data. We formalize this using a zero-sum game between a predictor
player and an adversarial evaluator player. Learned weights parameterize this gameâ€™s payoffs and
enable generalization from training data to new predictive settings. The key computational challenge
this approach poses is that the size of multivariate prediction games grows exponentially in the
number of variables. We leverage constraint generation methods developed for solving large zerosum games [20] and efficient methods for computing best responses [6] to tame this complexity.
In many cases, the structure of the multivariate loss function enables the zero-sum gameâ€™s Nash
equilibrium to be efficiently computed. We formulate parameter estimation as a convex optimization
problem and solve it using standard convex optimization methods. We demonstrate the benefits of
this approach on prediction tasks with P@k, F-score and DCG multivariate evaluation measures.
1

2

Background and Related Work

2.1

Notation and multivariate performance functions

We consider the general task of making a multivariate prediction for variables y =
{y1 , y2 , . . . , yn } âˆˆ Y n (with random variables denoted as Y = {Y1 , Y2 , . . . , Yn }) given some
contextual information x = {x1 , x2 , . . . , xn } âˆˆ X = {X1 , X2 , . . . , Xn } (with random variable,
X). Each xi is the information relevant to predicted variable yi . We denote the estimatorâ€™s predicted
values as yÌ‚ = {yÌ‚1 , yÌ‚2 , . . . , yÌ‚n }. The multivariate performance measure when predicting yÌ‚ when the
true multivariate value is actually y is represented as a scoring function: score(yÌ‚, y). Equivalently,
a complementary loss function for any score function based on the maximal score can be defined as:
loss(yÌ‚, y) = maxy0 ,y00 score(y0 , y00 ) âˆ’ score(yÌ‚, y).
For information retrieval, a vector of retrieved items from the pool of n items can be represented
as yÌ‚ âˆˆ {0, 1}n and a vector of relevant items as y âˆˆ {0, 1}n with x = {x1 , x2 , . . . , xn } denoting side contextual information (e.g., search terms and document contents). Precision and recall
are important measures for information retrieval systems. However, maximizing either leads to
degenerate solutions (predict all to maximize recall or predict none to maximize precision). The
precision when limited to exactly k positive predictions, P@k(yÌ‚, y) = yÌ‚Â·y
k where ||yÌ‚||1 = k,
is one popular multivariate performance measure that avoids these extremes. Another is the Fscore, which is the harmonic mean of the precision and recall often used in information retrieval tasks. Using this notation, the F-score for a set of items can be simply represented as:
F1 (yÌ‚, y) = ||yÌ‚ ||2yÌ‚Â·y
and F1 (0, 0) = 1.
1 +||y||1
In other information retrieval tasks, a ranked list of retrieved items is desired. This can be represented as a permutation, Ïƒ, where Ïƒ(i) denotes the ith -ranked item (and Ïƒ âˆ’1 (j) denotes the
rank of the j th item). Evaluation measures that emphasize the top-ranked items are used, e.g.,
to produce search engine results attuned to actual usage. The discounted cumulative gain (DCG)
measures the performance of item rankings with k relevancy scores, yi âˆˆ {0, . . . , k âˆ’ 1} as:
Pn yÏƒÌ‚(i)
Pn 2yÏƒÌ‚(i) âˆ’1
0
DCG(ÏƒÌ‚, y) = i=1 log
i=2 log i .
(i+1) or DCG (ÏƒÌ‚, y) = yÏƒÌ‚(1) +
2

2.2

2

Multivariate empirical risk minimization

Empirical risk minimization [28] is a common supervised learning approach that seeks a predictor
PÌ‚ (yÌ‚|x) (from, e.g., a set of predictors Î“) that minimizes the loss under the empirical distribution of
training data, denoted PÌƒ (y, x): minPÌ‚ (yÌ‚|x)âˆˆÎ“ EPÌƒ (y,x)PÌ‚ (yÌ‚|x) [loss(YÌ‚, Y)]. Multivariate losses are often not convex and finding the optimal solution is computationally intractable for expressive classes
of predictors Î“ typically specified by some set of parameters Î¸ (e.g., linear discriminant functions:
PÌ‚ (yÌ‚|x) = 1 if Î¸ Â· Î¦(x, yÌ‚) > Î¸ Â· Î¦(x, y 0 ) âˆ€y 0 6= yÌ‚).
Given these difficulties, convex surrogates to P
the multivariate loss are instead employed that are
additive over yÌ‚i and yi (i.e., loss(yÌ‚, y) =
i loss(yÌ‚i , yi )). Employing the logarithmic loss,
loss(yÌ‚i , yi ) = âˆ’ log PÌ‚ (YÌ‚i = yi ) yields the logistic regression model [9]. Using the hinge loss
yields support vector machines [5]. Structured support vector machines [27] employ a convex approximation of the multivariate loss over a training dataset D using the hinge loss function:
X
min ||Î¸||2 + Î±
Î¾i such that âˆ€i, y0 âˆˆ Y, Î¸ Â· [Î¦(x(i) , y(i) ) âˆ’ Î¦(x(i) , y0 )] â‰¥ âˆ†(y0 , y(i) ) âˆ’ Î¾i .
Î¸,Î¾i â‰¥0

i

In other words, linear parameters Î¸ for feature functions Î¦(Â·, Â·) are desired that make the example
label y(i) have a potential value Î¸ Â· Î¦(x(i) , y(i) ) that is better than all alternative labels y0 by at
least the multivariate loss between y0 and y(i) , denoted âˆ†(y0 , y(i) ). When this is not possible for
a particular example, a hinge loss penalty Î¾i is incurred that grows linearly with the difference in
potentials. Parameter Î± controls a trade-off between obtaining a predictor with lower hinge loss or
better discrimination between training examples (the margin). The size of set Y is often too large
for explicit construction of the constraint set to be computationally tractable. Instead, constraint
generation methods are employed to find a smaller set of active constraints. This can be viewed as
either finding the most-violated constraint [27] or as a loss-augmented inference problem [25]. Our
2

approach employs similar constraint generation techniquesâ€”in the inference procedure rather than
the parameter learning procedureâ€”to improve its efficiency.

3

Multivariate Prediction Games

We formulate a minimax game for multivariate loss optimization, describe our approach for limiting the computational complexity of solving this game, and describe algorithms for estimating
parameters of the game and making predictions using this framework.
3.1

Game formulation

Following a recent adversarial formulation for classification [1], we view multivariate prediction as
a two-player game between player YÌ‚ making predictions and player YÌŒ determining the evaluation
distribution. Player YÌ‚ first stochastically chooses a predictive distribution of variable assignments,
PÌ‚ (yÌ‚|x), to maximize a multivariate performance measure, then player YÌŒ stochastically chooses
an evaluation distribution, PÌŒ (yÌŒ|x), that minimizes the performance measure. Further, player YÌŒ
must choose the relevant items in a way that (approximately) matches in expectation with a set of
statistics, Î¦(x, y), measured from labeled data. We denote this set as Îž.
Definition 1. The multivariate prediction game (MPG) for n predicted variables is:
h
i
max min EPÌƒ (x)PÌ‚ (yÌ‚|x)PÌŒ (yÌŒ|x) score(YÌ‚, YÌŒ) ,

(1)

PÌ‚ (yÌ‚|x) PÌŒ (yÌŒ|x)âˆˆÎž

where PÌ‚ (yÌ‚|x) and PÌŒ (yÌŒ|x) are distributions over combinations of labelsfor the n predicted variables and the set Îž corresponds to the constraint: EPÌƒ (x)P (yÌŒ|x) Î¦(X, YÌŒ) = EPÌƒ (y,x) [Î¦(X, Y)] .
Since the set Îž constrains the adversaryâ€™s multivariate label distribution over the entire distribution
of inputs PÌƒ (x), solving this game directly is impractical when the number of training examples is
large. Instead, we employ the method of Lagrange multipliers in Theorem 1, which allows the set
of games to be independently solved given Lagrange multipliers Î¸.
Theorem 1. The multivariate prediction gameâ€™s value (Definition 1) can be equivalently obtained
by solving a set of unconstrained maximin games parameterized by Lagrange multipliers Î¸:
max

min

PÌ‚ (yÌ‚|x) PÌŒ (yÌŒ|x)âˆˆÎž

h
i (a)
EPÌƒ (x)PÌ‚ (yÌ‚|x)PÌŒ (yÌŒ|x) score(YÌ‚, YÌŒ) =

ï£«
(b)

ï£¬
= max ï£¬
ï£­EPÌƒ (y,x) [Î¸ Â· Î¦(X, Y)] +
Î¸

min

h
i
max EPÌƒ (x)PÌ‚ (yÌ‚|x)PÌŒ (yÌŒ|x) score(YÌ‚, YÌŒ)

PÌŒ (yÌŒ|x)âˆˆÎž PÌ‚ (yÌ‚|x)

ï£¶ï£¶

ï£«
X

ï£·ï£·
ï£¬
ï£·
PÌƒ (x) min max ï£¬
yÌŒ) âˆ’ Î¸ Â· Î¦(x, yÌŒ)ï£·
ï£­score(yÌ‚,
ï£¸ï£¸ ,
|
{z
}
PÌŒ (yÌŒ|x) PÌ‚ (yÌ‚|x)
xâˆˆX

(2)

C 0yÌ‚,yÌŒ

where: Î¦(x, y) is a vector of features characterizing the set of prediction variables {yi } and provided contextual variables {xi } each related to predicted variable yi .
Proof (sketch). Equality (a) is a consequence of duality in zero-sum games [29]. Equality (b) is
obtained by writing the Lagrangian and taking the dual. Strong Lagrangian duality is guaranteed
when a feasible solution exists on the relative interior of the convex constraint set Îž [2]. (A small
amount of slack corresponds to regularization of the Î¸ parameter in the dual and guarantees the
strong duality feasibility requirement is satisfied in practice.)
The resulting gameâ€™s payoff matrix can be expressed as the original game scores of Eq. (1) augmented with Lagrangian potentials. The combination defines a new payoff matrix with entries
C 0yÌ‚,yÌŒ = score(yÌ‚, yÌŒ) âˆ’ Î¸ Â· Î¦(x, yÌŒ), as shown in Eq. (2).
3.2

Example multivariate prediction games and small-scale solutions

Examples of the Lagrangian payoff matrices for the P@2, F-score, and DCG P
games are shown in Tan
ble 1 for three variables. We employ additive feature functions, Î¦(x, yÌŒ) = i=1 Ï†(xi ) I(yÌŒi = 1),
3

Table 1: The payoff matrices for the zero-sum games between player YÌŒ choosing columns and
player YÌ‚ choosing rows with three variables for: precision at k (top); F-score (middle) and DCG
with binary relevance values, yÌŒi âˆˆ {0, 1}, and we let lg 3 , log2 3 (bottom).
P@2 000 001
1
011 0
2 âˆ’Ïˆ3
1
101 0
2 âˆ’Ïˆ3
110 0 0âˆ’Ïˆ3
F1 000 001
000 1 0âˆ’Ïˆ3
001 0 1âˆ’Ïˆ3
010 0 0âˆ’Ïˆ3
2
011 0
3 âˆ’Ïˆ3
100 0 0âˆ’Ïˆ3
2
101 0
3 âˆ’Ïˆ3
110 0 0âˆ’Ïˆ3
1
111 0
2 âˆ’Ïˆ3
DCG 000 001
1
123 0
2 âˆ’Ïˆ3
132 0 lg13 âˆ’Ïˆ3
1
213 0
2 âˆ’Ïˆ3
231 0 lg13 âˆ’Ïˆ3
312 0 1âˆ’Ïˆ3
321 0 1âˆ’Ïˆ3

010
0âˆ’Ïˆ2
1
2 âˆ’Ïˆ2

011
1âˆ’Ïˆ2 âˆ’Ïˆ3
1
2 âˆ’Ïˆ2 âˆ’Ïˆ3
1
2 âˆ’Ïˆ2 âˆ’Ïˆ3

100
0âˆ’Ïˆ1
1
2 âˆ’Ïˆ1
1
2 âˆ’Ïˆ1

1âˆ’Ïˆ1 âˆ’Ïˆ3
1
2 âˆ’Ïˆ1 âˆ’Ïˆ3

1âˆ’Ïˆ1 âˆ’Ïˆ2

111
1âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’Ïˆ3
1âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’Ïˆ3
1âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’Ïˆ3

010
0âˆ’Ïˆ2
0âˆ’Ïˆ2
1âˆ’Ïˆ2
2
3 âˆ’Ïˆ2
0âˆ’Ïˆ2
0âˆ’Ïˆ2
2
3 âˆ’Ïˆ2
1
2 âˆ’Ïˆ2

011
0âˆ’Ïˆ2 âˆ’ Ïˆ3
2
3 âˆ’Ïˆ2 âˆ’ Ïˆ3
2
3 âˆ’Ïˆ2 âˆ’ Ïˆ3
1âˆ’Ïˆ2 âˆ’ Ïˆ3
0âˆ’Ïˆ2 âˆ’ Ïˆ3
1
2 âˆ’Ïˆ2 âˆ’ Ïˆ3
1
2 âˆ’Ïˆ2 âˆ’ Ïˆ3
4
5 âˆ’Ïˆ2 âˆ’ Ïˆ3

100
0âˆ’Ïˆ1
0âˆ’Ïˆ1
0âˆ’Ïˆ1
0âˆ’Ïˆ1
1âˆ’Ïˆ1
2
3 âˆ’Ïˆ1
2
3 âˆ’Ïˆ1
1
2 âˆ’Ïˆ1

101
0âˆ’Ïˆ1 âˆ’Ïˆ3
2
3 âˆ’Ïˆ1 âˆ’Ïˆ3
0âˆ’Ïˆ1 âˆ’Ïˆ3
1
2 âˆ’Ïˆ1 âˆ’Ïˆ3
2
3 âˆ’Ïˆ1 âˆ’Ïˆ3
1âˆ’Ïˆ1 âˆ’Ïˆ3
1
2 âˆ’Ïˆ1 âˆ’Ïˆ3
4
5 âˆ’Ïˆ1 âˆ’Ïˆ3

110
0âˆ’Ïˆ1 âˆ’Ïˆ2
0âˆ’Ïˆ1 âˆ’Ïˆ2
2
3 âˆ’Ïˆ1 âˆ’Ïˆ2
1
2 âˆ’Ïˆ1 âˆ’Ïˆ2
2
3 âˆ’Ïˆ1 âˆ’Ïˆ2
1
2 âˆ’Ïˆ1 âˆ’Ïˆ2
1âˆ’Ïˆ1 âˆ’Ïˆ2
4
5 âˆ’Ïˆ1 âˆ’Ïˆ2

111
0âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’ Ïˆ3
1
2 âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’ Ïˆ3
1
2 âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’ Ïˆ3
4
5 âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’ Ïˆ3
1
2 âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’ Ïˆ3
4
5 âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’ Ïˆ3
4
5 âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’ Ïˆ3
1âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’ Ïˆ3

010

011

100

101

110
1+ lg13 âˆ’Ïˆ1 âˆ’Ïˆ2
3
2 âˆ’Ïˆ1 âˆ’Ïˆ2
1+ lg13 âˆ’Ïˆ1 âˆ’Ïˆ2
3
2 âˆ’Ïˆ1 âˆ’Ïˆ2
1
1
+
2 lg 3 âˆ’Ïˆ1 âˆ’Ïˆ2
1
1
2 + lg 3 âˆ’Ïˆ1 âˆ’Ïˆ2

1
3
2 + lg 3 âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’Ïˆ3
3
1
2 + lg 3 âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’Ïˆ3
3
1
2 + lg 3 âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’Ïˆ3
3
1
2 + lg 3 âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’Ïˆ3
1
3
2 + lg 3 âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’Ïˆ3
1
3
2 + lg 3 âˆ’Ïˆ1 âˆ’Ïˆ2 âˆ’Ïˆ3

1
2 âˆ’Ïˆ2

1
1
2 + lg 3 âˆ’Ïˆ2 âˆ’Ïˆ3 1âˆ’Ïˆ1
1
1
2 + lg 3 âˆ’Ïˆ2 âˆ’Ïˆ3 1âˆ’Ïˆ1
3
1
1âˆ’Ïˆ2
2 âˆ’Ïˆ2 âˆ’Ïˆ3
lg 3 âˆ’Ïˆ1
1
1âˆ’Ïˆ2 1+ lg 3 âˆ’Ïˆ2 âˆ’Ïˆ3 12 âˆ’Ïˆ1
1
3
1
2 âˆ’ Ïˆ2
2 âˆ’Ïˆ2 âˆ’Ïˆ3
lg 3 âˆ’Ïˆ1
1
1
1
lg 3 âˆ’Ïˆ2 1+ lg 3 âˆ’Ïˆ2 âˆ’Ïˆ3
2 âˆ’Ïˆ1
1
lg 3 âˆ’Ïˆ2
1
2 âˆ’ Ïˆ2

101

110

1
2 âˆ’Ïˆ1 âˆ’Ïˆ3

1
2 âˆ’Ïˆ1 âˆ’Ïˆ2
1
2 âˆ’Ïˆ1 âˆ’Ïˆ2

3
2 âˆ’Ïˆ1 âˆ’Ïˆ3
1+ lg13 âˆ’Ïˆ1 âˆ’Ïˆ3
1
1
2 + lg 3 âˆ’Ïˆ1 âˆ’Ïˆ3
1
1
2 + lg 3 âˆ’Ïˆ1 âˆ’Ïˆ3
1
1+ lg 3 âˆ’Ïˆ1 âˆ’Ïˆ3
3
2 âˆ’Ïˆ1 âˆ’Ïˆ3

111

in these examples (with indicator function I(Â·)). We compactly represent the Lagrangian potential
terms for each game with potential variables, Ïˆi , Î¸ Â· Ï†(Xi = xi ) when YÌŒi = 1 (and 0 otherwise).
Zero-sum games such as these can be solved using a pair of linear programs that have a constraint
for each pure action (set of variable assignments) in the game [29]:
X
X
C 0yÌ‚,yÌŒ âˆ€yÌ‚ âˆˆ Y and
PÌ‚ (yÌ‚|x) = 1;
(3)
PÌ‚ (yÌ‚|x)C
max v such that v â‰¤
v,PÌ‚ (yÌ‚|x)â‰¥0

min

v such that v â‰¥

v,PÌŒ (yÌŒ|x)â‰¥0

yÌ‚âˆˆY

yÌ‚âˆˆY

X

C 0yÌ‚,yÌŒ
PÌŒ (yÌŒ|x)C

yÌŒâˆˆY

âˆ€yÌŒ âˆˆ Y and

X

PÌŒ (yÌŒ|x) = 1,

(4)

yÌŒâˆˆY

where C 0 is the Lagrangian-augmented payoff and v is the value of the game. The second player to
act in a zero-sum game can maximize/minimize using a pure strategy (i.e., a single value assignment
to all variables). Thus, these LPs consider only the set of pure strategies of the opponent to find the
first playerâ€™s mixed equilibrium strategy. The equilibrium strategy for the predictor is a distribution
over rows and the equilibrium strategy for the adversary is a distribution over columns.

The size of each gameâ€™s payoff matrix grows exponentially with the number of variables, n: (2n ) nk
for the precision at k game; (2n )2 for the F-score game; and (n! k n ) for the DCG game with k
possible relevance levels. These sizes make explicit construction of the game matrix impractical for
all but the smallest of problems.
3.3

Large-scale strategy inference

More efficient methods for obtaining Nash equilibria are needed to scale our MPG approach to
large prediction tasks with exponentially-sized payoff matrices. Though much attention has focused
on efficiently computing -Nash equilibria (e.g., in O(1/) time or O(ln(1/)) time [8]), which
guarantee each player a payoff within  of optimal, we employ an approach for finding an exact
equilibrium that works well in practice despite not having as strong theoretical guarantees [20].
4

Consider the reduced game matrices of Table 2. The Nash equilibrium for the precision at k game with
potentials
 1 1Lagrangian

1
Ïˆ
=
Ïˆ
=
Ïˆ
=
0.4
is:
PÌ‚
(yÌ‚|x)
=
and
PÌŒ
(yÌŒ|x) =
3
3
3
 11 1 2 1  3
2
3
3
3 ; with a game value of âˆ’ 15 . The Nash equilibrium for
the reduced F-score
 game with no learning(i.e., Ïˆ1 = Ïˆ2= Ïˆ3 =
0) is: PÌ‚ (yÌ‚|x) = 13 32 and PÌŒ (yÌŒ|x) = 13 29 29 29 ; with a
game value of 23 . The reduced game equilibrium is also an equilibrium of the original game. Though the exact size of the subgame
and its specific actions depends on the values of Ïˆ, often a compact
sub-game with identical equilibrium or close approximation exists
[18]. Motivated by the compactness of the reduced game, we employ a constraint generation approach known as the double oracle
algorithm [20] to iteratively construct an appropriate reduced game
that provides the correct equilibrium but avoids the computational
complexity of the original exponentially sized game.

Table 2: The reduced precision at k game with Ïˆ1 =
Ïˆ2 = Ïˆ3 = 0.4 (top) and Fscore game with Ïˆ1 = Ïˆ2 =
Ïˆ3 = 0 (bottom).
011

101

110

0.2 -0.3 -0.3
101 -0.3 0.2 -0.3
111 -0.3 -0.3 0.2
011

000 001 010 100
000
111

0
1

1

1

1

1
2

1
2

1
2

Algorithm 1 Constraint generation game solver
Input: Lagrange potentials for
 = {Ïˆ1 , Ïˆ2 , . . . , Ïˆn }; initial action sets SÌ‚0 and SÌŒ0
 each variable, Ïˆ

Output: Nash equilibrium, PÌ‚ (yÌ‚|x), PÌŒ (yÌŒ|x)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:

Initialize Player YÌ‚ â€™s action set SÌ‚ â† SÌ‚0 and Player YÌŒ â€™s action set SÌŒ â† SÌŒ0
C 0 â† buildPayoffMatrix(SÌ‚, SÌŒ, Ïˆ)
. Using Eq. (2) for the sub-game matrix of SÌ‚ Ã— SÌŒ
repeat
C 0)
. Using the LP of Eq. (3)
[PÌ‚ (yÌ‚|x), vNash1 ] â† solveZeroSumGameYÌ‚ (C
[aÌŒ, vÌŒBR ] â† findBestResponseAction(P (yÌ‚|x), Ïˆ)
. aÌŒ denotes the best response action
if (vNash1 6= vÌŒBR ) then
. Check if best response provides improvement
SÌŒ â† SÌŒ âˆª aÌŒ
C 0 â† buildPayoffMatrix(SÌ‚, SÌŒ, Ïˆ)
. Add new row to game matrix
end if
C 0)
[PÌŒ (yÌŒ|x), vNash2 ] â† solveZeroSumGameYÌŒ (C
. Using the LP of Eq. (4)
[aÌ‚, vÌ‚BR ] â† findBestResponseAction(P (yÌŒ|x), Ïˆ)
if (vNash2 6= vÌ‚BR ) then
SÌ‚ â† SÌ‚ âˆª aÌ‚
C 0 â† buildPayoffMatrix(SÌ‚, SÌŒ, Ïˆ)
. Add new column to game matrix
end if
until (vNash1 = vNash2 = vÌ‚BR = vÌŒBR )
. Stop if neither best response provides improvement
return [PÌ‚ (yÌ‚|x), PÌŒ (yÌŒ|x)]

Neither player can improve upon their strategy with additional pure
strategies when Algorithm 1 terminates, thus the mixed strategies it returns are a Nash equilibrium
pair [20]. Additionally, the algorithm is efficient in practice so long as each playerâ€™s strategy is
compact (i.e., the number of actions with non-zero probability is a polynomial subset of the label combinations) and best responses to opponentsâ€™ strategies can be obtained efficiently (i.e., in
polynomial time) for each player. Additionally, this algorithm can be modified to find approximate
equilibria by limiting the number of actions for each playerâ€™s set SÌ‚ and SÌŒ.
3.4

Efficiently computing best responses

The tractability of our approach largely rests on our ability to efficiently find best responses to oppoC 0yÌ‚,YÌŒ ] and argminyÌŒâˆˆYÌŒ EPÌ‚ (yÌ‚|x) [C
C 0YÌ‚ ,yÌŒ ]. For some combinations
nent strategies: argmaxyÌ‚âˆˆYÌ‚ EPÌŒ (yÌŒ|x) [C
of loss functions and features, finding the best response is trivial using, e.g., a greedy selection algorithm. Other loss function/feature combinations require specialized algorithms or are NP-hard. We
illustrate each situation.
Precision at k best response Many best responses can be obtained using greedy algorithms that
are based on marginal probabilities of the opponentâ€™s strategy. For example, the expected payoff in
5

the precision at k game for the estimator player setting yÌ‚i = 1 is PÌŒ (yÌŒi = 1|x). Thus, the set of top k
variables with the largest marginal label probability provides the best response. For the adversaryâ€™s
best response, the Lagrangian terms must also be included. Since k is a known variable, as long as
the value of each included term, PÌ‚ (yÌ‚i = 1, ||yÌ‚||1 = k|x) âˆ’ kÏˆi , is negative, the sum is the smallest,
and the corresponding response is the best for the adversary.
F-score game best response We leverage a recently developed method for efficiently maximizing
the F-score when a distribution over relevant documents is given [6]. The key insight is that the
problem can be separated into an inner greedy maximization over item sets of a certain size k and an
outer maximization to select the best set size k from {0, . . . , n}. This method can be directly applied
to find the best response of the estimator player, YÌ‚ , since the Lagrangian terms of the cost matrix
are invariant to the choice of yÌ‚. Algorithm 2 obtains the best response for the adversary player, YÌŒ ,
using slight modifications to incorporate the Lagrangian potentials into the objective function.
Algorithm 2 Lagrangian-augmented F-measure Maximizer for adversary player YÌŒ
Input: vector PÌ‚ of estimator probabilities and Lagrange potentials Ïˆ (Ïˆ1 , Ïˆ2 , ..., Ïˆn )
1
1: define matrix W with element W s,k = s+k
,
s, k âˆˆ {1, ..., n}
1 T
n
F
W
2: construct matrix = PÌ‚ Ã—
âˆ’ 2Ïˆ Ã— 1
. 1n is the all ones 1 Ã— n vector
3: for k = 1 to n do
4:
solve the inner optimization
Pn problem:
Pn
âˆ—
5:
a(k) = argminaâˆˆAk 2 i=1 ai fik
. Ak = {a âˆˆ {0, 1}n | i=1 ai = k}
(k)
6:
by setting ai = 1 for the k-th column of F â€™s smallest k elements, and ai = 0 for the rest;
âˆ—
P
âˆ—
F (y, a(k) )] = 2 ni=1 a(k)
7:
store a value of Eyâˆ¼p(YÌ‚|x) [F
fik
i
8: end for
âˆ—
F (y, 0n )] = p(YÌ‚ = 0n |x)
9: for k = 0 take a(k) = 0n , and Eyâˆ¼P (YÌ‚|x) [F
10: solve the outer optimization problem:
F (y, a)]
11: aâˆ— = argminaâˆˆ{a(0)âˆ— ,...,a(n)âˆ— } Eyâˆ¼p(YÌ‚|x) [F
âˆ—
âˆ—
F
12: return a and Eyâˆ¼p(YÌ‚|x) [F (y, a )]
Order inversion best response Another common loss measure when comparing two rankings is
the number of pairs of items with inverted order across rankings (i.e., one variable may occur before
another in one ranking, but not in the other ranking). Only the marginal probabilities of pairwise
P
orderings, PÌ‚ (ÏƒÌ‚ âˆ’1 (i) > ÏƒÌ‚ âˆ’1 (j)) , ÏƒÌ‚ PÌ‚ (ÏƒÌ‚) I(Ïƒ âˆ’1 (i) > Ïƒ âˆ’1 (j)), are needed to construct the
portion of the payoff received for ÏƒÌŒ ranking item i over item j, PÌ‚ (ÏƒÌ‚ âˆ’1 (i) > ÏƒÌ‚ âˆ’1 (j))(1 + Ïˆi>j ),
where Ïˆi>j is a Lagrangian potential based on pair-wise features for ranking item i over item j.
One could construct a fully connected directed graph with edges weighted by these portions of the
payoff for ranking pairs of items. The best response for ÏƒÌŒ corresponds to a set of acyclic edges
with the smallest sum of edge weights. Unfortunately, this problem is NP-hard in general because
the NP-complete minimum feedback arc set problem [15], which seeks to form an acyclic graph by
removing the set of edges with the minimal sum of edge weights, can be reduced to it.
DCG best response Although we cannot find an efficient algorithm to get the best response using
order inversion, solving best response of DCG has a known efficient algorithm. In this problem the
maximizer is a permutation of the documents while the minimizer is the relevance score of each
document pair. The estimatorâ€™s best response ÏƒÌ‚ maximizes:
!
!
n
n
X
X
X
X
1
2yÌŒÏƒÌ‚(i) âˆ’ 1
P (yÌŒ|x)
âˆ’ Î¸ Â· Ï†(x, yÌŒ) =
P (yÌŒ|x)2yÌŒÏƒÌ‚(i) âˆ’ 1 âˆ’ c,
log
(i
+
1)
log
(i
+
1)
2
2
yÌŒ
yÌŒ
i=1
i=1
where c is a constant that has
Pno relationship with ÏƒÌ‚. Since 1/log2 (i + 1) is monotonically decreasing, computing and sorting yÌŒ P (yÌŒ|x)2yÌŒi âˆ’ 1 with descending order and greedily assign the order
to ÏƒÌ‚ is optimal. The adversaryâ€™s best response using additive features minimizes:
X
ÏƒÌ‚

!
n
n
n
X
X
X
X
2yÌŒi âˆ’ 1
2yÌŒÏƒÌ‚(i) âˆ’ 1
P (ÏƒÌ‚|x)
âˆ’
Î¸i Â· Ï†i (xi , yÌŒi ) =
P (ÏƒÌ‚|x)
âˆ’ Î¸i Â· Ï†i (xi , yÌŒi ) .
log2 (i + 1) i=1
log2 (Ïƒ âˆ’1 (i) + 1)
i=1
i=1
ÏƒÌ‚

6

Thus, by using the expectation of a function of each variableâ€™s rank, 1/(log2 (Ïƒ âˆ’1 (i) + 1), which is
easily computed from PÌ‚ (Ïƒ), each variableâ€™s relevancy score yÌŒi can be independently chosen.
3.5

Parameter estimation

Predictive model parameters, Î¸, must be chosen to ensure that the adversarial distribution is similar
to training data. Though adversarial prediction can be posed as a convex optimization problem
[1], the objective function is not smooth. General subgradient methods require O(1/2 ) iterations
to provide an  approximation to the optima. We instead employ L-BFGS [19], which has been
empirically shown to converge at a faster rate in many cases despite lacking theoretical guarantees
for non-smooth objectives [16]. We also employ L2 regularization to avoid overfitting to the training
data sample. The addition of the smooth regularizer often helps to improve the rate of convergence.
The gradient in these optimizations with L2 regularization, âˆ’ Î»2 ||Î¸||2 , for training dataset D =
{(x(i) , y(i))} is the difference between feature moments
with additional regularization term:

P|D|
P
1
(i)
(i)
(i)
(i)
(i)
j=1 Î¦(x , y ) âˆ’
yÌŒâˆˆY PÌŒ (yÌŒ|x )Î¦(x , yÌŒ) âˆ’ Î»Î¸. The adversarial strategies PÌŒ (Â·|x )
|D|
needed for calculating this gradient are computed via Alg. 1.

4

Experiments

We evaluate our approach, Multivariate Prediction Games (MPG), on the three performance measures of interest in this work: precision at k, F-score, and DCG. Our primary point of comparison
is with structured support vector machines (SSVM)[27] to better understand the trade-offs between
convexly approximating the loss function with the hinge loss versus adversarially approximating the
training data using our approach. We employ an optical recognition of handwritten digits (OPTDIGITS) dataset [17] (10 classes, 64 features, 3,823 training examples, 1,797 test examples), an income
prediction dataset (â€˜a4aâ€™ ADULT1 [17] (two classes, 123 features, 3,185 training examples, 29,376
test examples), and query-document pairs from the million query TREC 2007 (MQ2007) dataset
of LETOR4.0 [23] (1700 queries, 41.15 documents on average per query, 46 features per document). Following the same evaluation method used in [27] for OPTDIGITS, the multi-class dataset
is converted into multiple binary datasets and we report the macro-average of the performance of all
classes on test data. For OPTDIGITS/ADULT, we use a random 31 of the training data as a holdout
validation data to select the L2 regularization parameter trade-off C âˆˆ {2âˆ’6 , 2âˆ’5 , ..., 26 }.
We evaluate the performance of our approach and comparison methods (SSVM variants2 and logistic regression Table 3: Precision at k (top) and F-score
(LR)) using precision at k, where k is half the number of performance (bottom).
positive examples (i.e. k = 12 P OS), and F-score. For
precision at k, we restrict the pure strategies of the adverPrecision@k OPTDIGITS ADULT
MPG
0.990
0.805
sary to select k positive labels. This prevents adversary
SSVM
0.956
0.638
strategies with no positive labels. From the results in TaSSVMâ€™
0.989
0.805
ble 3, we see that our approach, MPG, works better than
F-score
OPTDIGITS
ADULT
SSVM on the OPTDGITS datasets: slightly better on preMPG
0.920
0.697
cision at k and more significantly better on F-measure.
SSVM
0.915
0.673
For the ADULT dataset, MPG provides equivalent perLR
0.914
0.639
formance for precision at k and better performance on Fmeasure. The nature of the running time required for validation and testing is very different for
SSVM, which must find the maximizing set of variable assignments, and MPG, which must interactively construct a game and its equilibrium. Model validation and testing require â‰ˆ 30 seconds for
SSVM on the OPTDIGITS dataset and â‰ˆ 3 seconds on the ADULT dataset, while requiring â‰ˆ 9
seconds and â‰ˆ 25 seconds for MPG precision at k and â‰ˆ 1397 seconds and â‰ˆ 252 seconds for
MPG F-measure optimization, respectively. For precision at k, MPG is within an order of magni1

http://www.csie.ntu.edu.tw/Ëœcjlin/libsvmtools/datasets/binary.html)
For precision at k, the original SSVMâ€™s implementation uses the restriction k during training, but not
during testing. We modified the code by ordering SSVMâ€™s prediction value for each test example, and select
the top k predictions as positives, the rest are considered as negatives. We denote the original implementation
as SSVM, and the modified version as SSVMâ€™.
2

7

tude (better for OPTDIGITS, worse for ADULT). For the more difficult problem of maximizing the
F-score of ADULT over 29, 376 test examples, the MPG game becomes quite large and requires
significantly more computational time. Though our MPG method is not as finely optimized as existing SSVM implementations, this difference in run times will remain as the game formulation is
inherently more computationally demanding for difficult prediction tasks.
We compare the performance of our approach and comparison methods using five-fold cross validation on the
MQ2007 dataset. We measure performance using Normalized DCG (NDCG), which divides the realized DCG
by the maximum possible DCG for the dataset, based on a
slightly different variant of DCG employed by LETOR4.0:
yÏƒÌ‚(i)
Pn
DCG00 (ÏƒÌ‚, y) = 2yÏƒÌ‚(1) âˆ’ 1 + i=2 2 log âˆ’1
. The compari2i
son methods are: RankSVM-Struct [13], part of SVMstruct
which uses structured SVM to predict the rank; ListNet
[3], a list-wise ranking algorithm employing cross entropy loss; AdaRank-NDCG [30], a boosting method using â€˜weak rankersâ€™ and data reweighing to achieve good
NDCG performance; AdaRank-MAP uses Mean Average Figure 1: NDCG@K as K increases.
Precision (MAP) rather than NDCG; and RankBoost [7],
which reduces ranking to binary classification problems on instance pairs.
Table 4: MQ2007 NDCG Results.
Method
MPG
RankSVM
ListNet
AdaRank-NDCG
AdaRank-MAP
RankBoost

5

Mean NDCG
0.5220
0.4966
0.4988
0.4914
0.4891
0.5003

Table 4 reports the NDCG@K averaged over all values of K
(between 1 and, on average 41) while Figure 1 reports the results for each value of K between 1 and 10. From this, we can
see that our MPG approach provides better rankings on average than the baseline methods except when K is very small
(K = 1, 2). In other words, the adversary focuses most of its
effort in reducing the score received from the first item in the
ranking, but at the expense of providing a better overall NDCG
score for the ranking as a whole.

Discussion

We have extended adversarial prediction games [1] to settings with multivariate performance measures in this paper. We believe that this is an important step in demonstrating the benefits of this
approach in settings where structured support vector machines [14] are widely employed. Our future work will investigate improving the computational efficiency of adversarial methods and also
incorporating structured statistical relationships amongst variables in the constraint set in addition
to multivariate performance measures.

Acknowledgments
This material is based upon work supported by the National Science Foundation under Grant No.
#1526379, Robust Optimization of Loss Functions with Application to Active Learning.

References
[1] Kaiser Asif, Wei Xing, Sima Behpour, and Brian D. Ziebart. Adversarial cost-sensitive classification. In
Proceedings of the Conference on Uncertainty in Artificial Intelligence, 2015.
[2] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
[3] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: from pairwise approach
to listwise approach. In Proceedings of the International Conference on Machine Learning, pages 129â€“
136. ACM, 2007.
[4] Corinna Cortes and Mehryar Mohri. AUC optimization vs. error rate minimization. In Advances in Neural
Information Processing Systems, pages 313â€“320, 2004.
[5] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273â€“297, 1995.

8

[6] Krzysztof J Dembczynski, Willem Waegeman, Weiwei Cheng, and Eyke HuÌˆllermeier. An exact algorithm
for F-measure maximization. In Advances in Neural Information Processing Systems, pages 1404â€“1412,
2011.
[7] Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram Singer. An efficient boosting algorithm for combining preferences. The Journal of machine learning research, 4:933â€“969, 2003.
[8] Andrew Gilpin, Javier PenÌƒa, and Tuomas Sandholm. First-order algorithm with o (ln (1/e)) convergence
for e-equilibrium in two-person zero-sum games. In AAAI Conference on Artificial Intelligence, pages
75â€“82, 2008.
[9] Peter D. GruÌˆnwald and A. Phillip Dawid. Game theory, maximum entropy, minimum discrepancy, and
robust Bayesian decision theory. Annals of Statistics, 32:1367â€“1433, 2004.
[10] Tamir Hazan, Joseph Keshet, and David A McAllester. Direct loss minimization for structured prediction.
In Advances in Neural Information Processing Systems, pages 1594â€“1602, 2010.
[11] Klaus-Uwe Hoffgen, Hans-Ulrich Simon, and Kevin S Vanhorn. Robust trainability of single neurons.
Journal of Computer and System Sciences, 50(1):114â€“125, 1995.
[12] Martin Jansche. Maximum expected F-measure training of logistic regression models. In Proceedings of
the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 692â€“699. Association for Computational Linguistics, 2005.
[13] Thorsten Joachims. Optimizing search engines using clickthrough data. In Proceedings of the International Conference on Knowledge Discovery and Data Mining, pages 133â€“142. ACM, 2002.
[14] Thorsten Joachims. A support vector method for multivariate performance measures. In Proceedings of
the International Conference on Machine Learning, pages 377â€“384. ACM, 2005.
[15] Richard M. Karp. Reducibility among combinatorial problems. Springer, 1972.
[16] Adrian S Lewis and Michael L Overton. Nonsmooth optimization via BFGS. 2008.
[17] M. Lichman. UCI machine learning repository, 2013.
[18] Richard J Lipton and Neal E Young. Simple strategies for large zero-sum games with applications to
complexity theory. In Proc. of the ACM Symposium on Theory of Computing, pages 734â€“740. ACM,
1994.
[19] Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization.
Mathematical programming, 45(1-3):503â€“528, 1989.
[20] H Brendan McMahan, Geoffrey J Gordon, and Avrim Blum. Planning in the presence of cost functions
controlled by an adversary. In Proceedings of the International Conference on Machine Learning, pages
536â€“543, 2003.
[21] David R Musicant, Vipin Kumar, and Aysel Ozgur. Optimizing F-measure with support vector machines.
In FLAIRS Conference, pages 356â€“360, 2003.
[22] Shameem Puthiya Parambath, Nicolas Usunier, and Yves Grandvalet. Optimizing F-measures by costsensitive classification. In Advances in Neural Information Processing Systems, pages 2123â€“2131, 2014.
[23] Tao Qin and Tie-Yan Liu. Introducing LETOR 4.0 datasets. arXiv preprint arXiv:1306.2597, 2013.
[24] Mani Ranjbar, Greg Mori, and Yang Wang. Optimizing complex loss functions in structured prediction.
In Proceedings of the European Conference on Computer Vision, pages 580â€“593. Springer, 2010.
[25] Ben Taskar, Vassil Chatalbashev, Daphne Koller, and Carlos Guestrin. Learning structured prediction
models: A large margin approach. In Proceedings of the International Conference on Machine Learning,
pages 896â€“903. ACM, 2005.
[26] Flemming TopsÃ¸e. Information theoretical optimization techniques. Kybernetika, 15(1):8â€“27, 1979.
[27] Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the International Conference on Machine Learning, page 104. ACM, 2004.
[28] Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in Neural Information
Processing Systems, pages 831â€“838, 1992.
[29] John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton University Press, 1947.
[30] Jun Xu and Hang Li. Adarank: a boosting algorithm for information retrieval. In Proc. of the International
Conference on Research and Development in Information Retrieval, pages 391â€“398. ACM, 2007.

9

