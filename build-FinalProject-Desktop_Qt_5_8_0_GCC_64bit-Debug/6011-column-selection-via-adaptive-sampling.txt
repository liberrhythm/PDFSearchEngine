Column Selection via Adaptive Sampling

Saurabh Paul
Global Risk Sciences, Paypal Inc.
saupaul@paypal.com

Malik Magdon-Ismail
CS Dept., Rensselaer Polytechnic Institute
magdon@cs.rpi.edu

Petros Drineas
CS Dept., Rensselaer Polytechnic Institute
drinep@cs.rpi.edu

Abstract
Selecting a good column (or row) subset of massive data matrices has found many
applications in data analysis and machine learning. We propose a new adaptive sampling algorithm that can be used to improve any relative-error column
selection algorithm. Our algorithm delivers a tighter theoretical bound on the approximation error which we also demonstrate empirically using two well known
relative-error column subset selection algorithms. Our experimental results on
synthetic and real-world data show that our algorithm outperforms non-adaptive
sampling as well as prior adaptive sampling approaches.

1

Introduction

In numerous machine learning and data analysis applications, the input data are modelled as a matrix
A ‚àà Rm√ón , where m is the number of objects (data points) and n is the number of features. Often,
it is desirable to represent your solution using a few features (to promote better generalization and
interpretability of the solutions), or using a few data points (to identify important coresets of the
data), for example PCA, sparse PCA, sparse regression, coreset based regression, etc. [1, 2, 3, 4].
These problems can be reduced to identifying a good subset of the columns (or rows) in the data
matrix, the column subset selection problem (CSSP). For example, finding an optimal sparse linear
encoder for the data (dimension reduction) can be explicitly reduced to CSSP [5]. Motivated by the
fact that in many practical applications, the left and right singular vectors of a matrix A lacks any
physical interpretation, a long line of work [6, 7, 8, 9, 10, 11, 12, 13, 14, 15], focused on extracting
a subset of columns of the matrix A, which are approximately as good as Ak at reconstructing A.
To make our discussion more concrete, let us formally define CSSP.
Column Subset
CSSP: Find a matrix C ‚àà Rm√óc containing c columns of
 Selection Problem,

A for which A ‚àí CC+ AF is small.1 In the prior work, one measures the quality of a CSSPsolution against Ak , the best rank-k approximation to A obtained via the singular value decomposition (SVD), where k is a user specified target rank parameter.
For example,
[15] gives efficient


algorithms to find C with c ‚âà 2k/ columns, for which A ‚àí CC+ AF ‚â§ (1 + ) kA ‚àí Ak kF .
Our contribution is not to directly attack CSSP. We present a novel algorithm that can improve an
existing CSSP algorithm by adaptively invoking it, in a sense actively learning which columns to
sample next based on the columns you have already sampled. If you use the CSSP-algorithm from
[15] as a strawman benchmark, you can obtain c columns all at once and incur an error roughly
(1 + 2k/c) kA ‚àí Ak kF . Or, you can invoke the algorithm to obtain, for example, c/2 columns,
and then allow the algorithm to adapt to the columns already chosen (for example by modifying A)
before choosing the remaining c/2 columns. We refer to the former as continued sampling and to the
1

CC+ A is the best possible reconstruction of A by projection into the space spanned by the columns of C.

1

latter as adaptive sampling. We prove performance guarantees which show that adaptive sampling
improves upon continued sampling, and we present experiments on synthetic and real data that
demonstrate significant empirical performance gains.
1.1

Notation

A, B, . . . denote matrices and a, b, . . . denote column vectors; In is the n √ó n identity matrix.
[A, B] and [A; B] denote matrix concatenation operations in a column-wise and row-wise manner,
respectively. Given a set S ‚äÜ {1, . . . n}, AS is the matrix that contains the columns of A ‚àà Rm√ón
indexed by S. Let rank(A) = œÅ ‚â§ min {m, n}. The (economy) SVD of A is
 X


œÅ
VTk
Œ£k
0
=
œÉi (A)ui viT
A = (Uk UœÅ‚àík )
T
0 Œ£œÅ‚àík
VœÅ‚àík
i=1

m√ók

where Uk ‚àà R
and UœÅ‚àík ‚àà R
contain the left singular vectors ui , Vk ‚àà Rn√ók
n√ó(œÅ‚àík)
and VœÅ‚àík ‚àà R
contain the right singular vectors vi , and Œ£ ‚àà RœÅ√óœÅ is a diagonal matrix
2
containing the singular values œÉ1 (A) ‚â• . . . ‚â• œÉœÅ (A) > 0. The Frobenius norm of A is kAkF =
P
2
+
‚àí1 T
U ; and, Ak , the best
i,j Aij ; Tr(A) is the trace of A; the pseudoinverse of A is A = VŒ£
Pk
rank-k approximation to A under any unitarily invariant norm is Ak = Uk Œ£k VTk = i=1 œÉi ui viT .
1.2

m√ó(œÅ‚àík)

Our Contribution: Adaptive Sampling

We design a novel CSSP-algorithm that adaptively selects columns from the matrix A in rounds. In
each round we remove from A the information that has already been ‚Äúcaptured‚Äù by the columns that
have been thus far selected. Algorithm 1 selects tc columns of A in t rounds, where in each round
c columns of A are selected using a relative-error CSSP-algorithm from prior work.

Input: A ‚àà Rm√ón ; target rank k; # rounds t; columns per round c
Output: C ‚àà Rm√ótc , tc columns of A and S, the indices of those columns.
1: S = {}; E0 = A
2: for ` = 1, ¬∑ ¬∑ ¬∑ , t do
3:
Sample indices S` of c columns from E`‚àí1 using a CSSP-algorithm.
4:
S ‚Üê S ‚à™ S` .
5:
Set C = AS and E` = A ‚àí (CC+ A)`k .
6: return C, S
Algorithm 1: Adaptive Sampling
At round ` in Step 3, we compute column indices S (and C = AS ) using a CSSP-algorithm on the
residual E`‚àí1 of the previous round. To compute this residual, remove from A the best rank-(`‚àí1)k
approximation to A in the span of the columns selected from the first ` ‚àí 1 rounds,
E`‚àí1 = A ‚àí (CC+ A)(`‚àí1)k .
A similar strategy was developed in [8] with sequential adaptive use of (additive error) CSSPalgorithms. These (additive error) CSSP-algorithms select columns according to column norms [11].
In [8], the residual in step 5 is defined differently, as E` = A ‚àí CC+ A. To motivate our result, it
helps to take a closer look at the reconstruction error E = A ‚àí CC+ A after t adaptive rounds of
the strategy in [8] with the CSSP-algorithm in [11].
# rounds
t=2
t

Continued sampling: tc columns using
CSSP-algorithm from [11]. ( = k/c)

kEk2F ‚â§ kA ‚àí Ak k2F + kAk2F
2

kEk2F ‚â§ kA ‚àí Ak k2F + kAk2F
t

2

Adaptive sampling: t rounds of the strategy
in [8] with the CSSP-algorithm from [11].
kEk2F ‚â§ (1 + ) kA ‚àí Ak k2F + 2 kAk2F
kEk2F ‚â§ (1 + O()) kA ‚àí Ak k2F +t kAk2F

2

2

Typically kAkF  kA ‚àí Ak kF and  is small (i.e., c  k), so adaptive sampling aÃÄ la [8] wins
over continued sampling for additive error CSSP-algorithms. This is especially apparent after t
2
rounds, where continued sampling only attenuates the big term kAkF by /t, but adaptive sampling
t
exponentially attenuates this term by  .
Recently, powerful CSSP-algorithms have been developed which give relative-error guarantees [15].
We can use the adaptive strategy from [8] together with these newer relative error CSSP-algorithms.
If one carries out the analysis from [8] by replacing the additive error CSSP-algorithm from [11]
with the relative error CSSP-algorithm in [15], the comparison of continued and adaptive sampling
using the strategy from [8] becomes (t = 2 rounds suffices to see the problem):
# rounds
t=2

Continued sampling: tc columns using
CSSP-algorithm from [15]. ( = 2k/c)


kEk2F ‚â§ 1 +
kA ‚àí Ak k2F
2

Adaptive sampling: t rounds of the strategy
in [8] with the CSSP-algorithm from [15].



2
kEk2F ‚â§ 1 + +
kA ‚àí Ak k2F
2
2

Adaptive sampling from [8] gives a worse theoretical guarantee than continued sampling for relative
error CSSP-algorithms. In a nutshell, no matter how many rounds of adaptive sampling you do,
2
the theoretical bound will not be better than (1 + k/c)kA ‚àí Ak kF if you are using a relative error
CSSP-algorithm. This raises an obvious question: is it possible to combine relative-error CSSPalgorithms with adaptive sampling to get (provably and empirically) improved CSSP-algorithms?
The approach of [8] does not achieve this objective. We provide a positive answer to this question.
Our approach is a subtle modification to the approach in [8]: in Step 5 of Algorithm 1. When we
compute the residual matrix in round `, we subtract (CC+ A)`k from A, the best rank-`k approximation to the projection of A onto the current columns selected, as opposed to subtracting the full
projection CC+ A. This subtle change, is critical in our new analysis which gives a tighter bound
on the final error, allowing us to boost relative-error CSSP-algorithms. For t = 2 rounds of adaptive
sampling, we get a reconstruction error of
2

2

2

kEkF ‚â§ (1 + ) kA ‚àí A2k kF + (1 + ) kA ‚àí Ak kF ,
where  = 2k/c. The critical improvement in the bound is that the dominant O(1)-term depends on
2
2
kA ‚àí A2k kF , and the dependence on kA ‚àí Ak kF is now O(). To highlight this improved theoretical bound in an extreme case, consider a matrix A that has rank exactly 2k, then kA ‚àí A2k kF = 0.
2
Continued sampling gives an error-bound (1+ 2 )kA ‚àí Ak kF , where as our adaptive sampling gives
2
an error-bound ( + 2 )kA ‚àí Ak kF , which is clearly better in this extreme case. In practice, data
matrices have rapidly decaying singular values, so this extreme case is not far from reality (See
Figure 1).
Singular Values of HGDP avgd over 22 chromosomes
800

Singular Values of TechTC‚àí300 avgd over 49 datasets
1000

Singular Values

Singular Values

800
600
400
200
0
0

20

40

60

80

600

400

200

0
0

100

200

400

600

800

1000

1200

Figure 1: Figure showing the singular value decay for two real world datasets.
To state our main theoretical result, we need to more formally define a relative error CSSP-algorithm.
Definition 1 (Relative Error CSSP-algorithm A(X, k, c)). A relative error CSSP-algorithm A takes
as input a matrix X, a rank parameter k < rank(X) and a number of columns c, and outputs column
indices S with |S| = c, so that the columns C = XS satisfy:
h
i
2
2
EC kX ‚àí (CC+ X)k kF ‚â§ (1 + (c, k))kX ‚àí Xk kF ,
3

where (c, k) depends on A and the expectation is over random choices made in the algorithm.2
Our main theorem bounds the reconstruction error when our adaptive sampling approach is used to
boost A. The boost in performance depends on the decay of the spectrum of A.
Theorem 1. Let A ‚àà Rm√ón be a matrix of rank œÅ and let k < œÅ be a target rank. If, in Step 3 of
Algorithm 1, we use the relative error CSSP-algorithm A with (c, k) =  < 1, then
t‚àí1
h
i
X
2
2
2
EC kA ‚àí (CC+ A)tk kF ‚â§ (1 + ) kA ‚àí Atk kF + 
(1 + )t‚àíi kA ‚àí Aik kF .
i=1

Comments.
1. The dominant O(1) term in our bound is kA ‚àí Atk kF , not kA ‚àí Ak kF . This is a major improvement since the former is typically much smaller than the latter in real data. Further, we
need a bound on the reconstruction error kA ‚àí CC+ AkF . Our theorem give a stronger result
than needed because kA ‚àí CC+ AkF ‚â§ kA ‚àí (CC+ A)tk kF .
2. We presented our result for the case of a relative error CSSP-algorithm with a guarantee on the
expected reconstruction error. Clearly, if the CSSP-algorithm is deterministic, then Theorem 1
will also hold deterministically. The result in Theorem 1 can also be boosted to hold with high
probability, by repeating the process log 1Œ¥ times and picking the columns which performed best.
Then, with probability at least 1 ‚àí Œ¥,
2

2

kA ‚àí (CC+ A)tk kF ‚â§ (1 + 2) kA ‚àí Atk kF + 2

t‚àí1
X

2

(1 + )t‚àíi kA ‚àí Aik kF .

i=1

If the CSSP-algorithm itself only gives a high-probability (at least 1‚àíŒ¥) guarantee, then the bound
in Theorem 1 also holds with high probability, at least 1 ‚àí tŒ¥, which is obtained by applying a
union bound to the probability of failure in each round.
3. Our results hold for any relative error CSSP-algorithm combined with our adaptive sampling
strategy. The relative error CSSP-algorithm in [15] has (c, k) ‚âà 2k/c. The relative error CSSPalgorithm in [16] has (c, k) = O(k log k/c). Other algorithms can be found in [8, 9, 17].
We presented the simplest form of the result, which can be generalized to sample a different
number of columns in each round, or even use a different CSSP-algorithm in each round. We
have not optimized the sampling schedule, i.e. how many columns to sample in each round. At
the moment, this is largely dictated by the CSSP algorithm itself, which requires a minimum
number of samples in each round to give a theoretical guarantee. From the empirical perspective
(for example using leverage score sampling to select columns), strongest performance may be
obtained by adapting after every column is selected.
4. In the context of the additive error CSSP-algorithm from [11], our adaptive sampling strategy
gives a theoretical performance guarantee which is at least as good as the adaptive sampling
strategy from [8].
Lastly, we also provide the first empirical evaluation of adaptive sampling algorithms. We implemented our algorithm using two relative-error column selection algorithms (the near-optimal column
selection algorithm of [18, 15] and the leverage-score sampling algorithm of [19]) and compared it
against the adaptive sampling algorithm of [8] on synthetic and real-world data. The experimental
results show that our algorithm outperforms prior approaches.
1.3

Related Work

Column selection algorithms have been extensively studied in prior literature. Such algorithms
include rank-revealing QR factorizations [6, 20] for which only weak performance guarantees can
be derived. The QR approach was improved in [21] where the authors proposed a memory efficient
implementation. The randomized additive error CSSP-algorithm [11] was a breakthrough, which led
to a series of improvements producing relative CSSP-algorithms using a variety of randomized and
2

h
i
2
For an additive-error CSSP algorithm, EC kX ‚àí (CC+ X)k kF ‚â§ kX ‚àí Xk k2F + (c, k)kXk2F .

4

deterministic techniques. These include leverage score sampling [19, 16], volume sampling [8, 9,
17], the two-stage hybrid sampling approach of [22], the near-optimal column selection algorithms
of [18, 15], as well as deterministic variants presented in [23]. We refer the reader to Section 1.5
of [15] for a detailed overview of prior work. Our focus is not on CSSP-algorithms per se, but rather
on adaptively invoking existing CSSP-algorithms. The only prior adaptive sampling with a provable
guarantee was introduced in [8] and further analyzed in [24, 9, 25]; this strategy is specifically boosts
the additive error CSSP-algorithm, but does not work with relative error CSSP-algorithms which are
currently in use. Our modification of the approach in [8] is delicate, but crucial to the new analysis
we perform in the context of relative error CSSP-algorithms.
Our work is motivated by relative error CSSP-algorithms satisfying definition 1. Such algorithms
exist which give expected guarantees [15] as well as high probability guarantees [19]. Specifically,
given X ‚àà Rm√ón and
 a target rank k, the leverage-score sampling approach of [19] selects c =
O k/2 log k/2 columns of A to form a matrix C ‚àà Rm√óc to give a (1+)-relative error with
probability at least 1 ‚àí Œ¥. Similarly, [18, 15] proposed near-optimal relative error CSSP-algorithms
selecting c ‚âà 2c/ columns and giving a (1 + )-relative error guarantee in expectation, which can
be boosted to a high probability guarantee via independent repetition.

2

Proof of Theorem 1

We now prove the main result which analyzes the performance of our adaptive sampling in Algorithm 1 for a relative error CSSP-algorithm. We will need the following linear algebraic Lemma.
Lemma 1. Let X, Y ‚àà Rm√ón and suppose that rank(Y) = r. Then,
œÉi (X ‚àí Y) ‚â• œÉr+i (X).
Proof. Observe that œÉi (X ‚àí Y) = k(X ‚àí Y) ‚àí (X ‚àí Y)i‚àí1 k2 . The claim is now immediate from
the Eckart-Young theorem because Y + (X ‚àí Y)i‚àí1 has rank at most r + i ‚àí 1, therefore
œÉi (X ‚àí Y) = kX ‚àí (Y + (X ‚àí Y)i‚àí1 )k2 ‚â• kX ‚àí Xr+i‚àí1 k2 = œÉr+i (X).

We are now ready to prove Theorem 1 by induction on t, the number of rounds of adaptive sampling.
When t = 1, the claim is that
h
i
2
2
E kA ‚àí (CC+ A)k kF ‚â§ (1 + ) kA ‚àí Ak kF ,
which is immediate from the definition of the relative error CSSP-algorithm. Now for the induction.
Suppose that after t rounds, columns Ct are selected, and we have the induction hypothesis that
t‚àí1
h
i
X
2
2
2
ECt kA ‚àí (Ct Ct+ A)tk kF ‚â§ (1 + ) kA ‚àí Atk kF + 
(1 + )t‚àíi kA ‚àí Aik kF .

(1)

i=1

In the (t + 1)th round, we use the residual Et = A ‚àí (Ct Ct+ A)tk to select new columns C0 . Our
relative error CSSP-algorithm A gives the following guarantee:
 i
h

2
2
+
EC0 kEt ‚àí (C0 C0 Et )k kF  Et
‚â§ (1 + ) Et ‚àí Etk F
!
k
 t 2 X
t
= (1 + ) E  ‚àí
œÉi2 (E )
F

i=1

‚â§

(1 + )

!
k
 t 2 X
2
E  ‚àí
œÉtk+i (A) .
F

(2)

i=1

(The last step follows because œÉi2 (Et ) = œÉi2 (A ‚àí (Ct Ct+ A)tk ) and we can apply Lemma 1 with
2
X = A, Y = (Ct Ct+ A)tk and r = rank(Y) = tk, to obtain œÉi2 (Et ) ‚â• œÉtk+i
(A).) We now take
5

the expectation of both sides with respect to the columns Ct ,
 ii
h
h
2
+
ECt EC0 kEt ‚àí (C0 C0 Et )k kF  Et
!
k
h  2 i X
t
2

‚â§ (1 + ) ECt E
‚àí
œÉtk+i (A) .
F

i=1
(a)

‚â§

2

(1 + )2 kA ‚àí Atk kF + 

t‚àí1
X

2

(1 + )t+1‚àíi kA ‚àí Aik kF ‚àí (1 + )

i=1

=

2

(1 + ) kA ‚àí Atk kF ‚àí

k
X

k
X

2
œÉtk+i
(A)

i=1

!
2
œÉtk+i
(A)

2

+ (1 + )kA ‚àí Atk kF

i=1
t‚àí1
X
2
+
(1 + )t+1‚àíi kA ‚àí Aik kF
i=1

=

2

(1 + )kA ‚àí A(t+1)k kF + 

t
X

2

(1 + )t+1‚àíi kA ‚àí Aik kF

(3)

i=1

(a) follows, because of the induction hypothesis (eqn. 1). The columns chosen after round t + 1 are
Ct+1 = [Ct , C0 ]. By the law of iterated expectation,
 ii
h
h
h
2
2i
+
+
ECt EC0 kEt ‚àí (C0 C0 Et )k kF  Et = ECt+1 kEt ‚àí (C0 C0 Et )k kF .
+

+

Observe that Et ‚àí (C0 C0 Et )k = A ‚àí (Ct Ct+ A)tk ‚àí (C0 C0 Et )k = A ‚àí Y, where Y is in the
+
column space of Ct+1 = [Ct , C0 ]; further, rank(Y) ‚â§ (t + 1)k. Since (Ct+1 Ct+1 A)(t+1)k is the
best rank-(t + 1)k approximation to A in the column space of Ct+1 , for any realization of Ct+1 ,
2

+

2

+

kA ‚àí (Ct+1 Ct+1 A)(t+1)k kF ‚â§ kEt ‚àí (C0 C0 Et )k kF .
(4)
Combining (4) with (3), we have that


t
X
2
+
2
2
ECt+1 kA ‚àí (Ct+1 Ct+1 A)(t+1)k kF ‚â§ (1+)kA ‚àí A(t+1)k kF +
(1+)t+1‚àíi kA ‚àí Aik kF .
i=1

This is the desired bound after t + 1 rounds, concluding the induction.
It is instructive to understand where our new adaptive sampling strategy is needed for the proof to
go through. The crucial step is (2) where we use Lemma 1 ‚Äì it is essential that the residual was a
low-rank perturbation of A.

3

Experiments

We compared three adaptive column sampling methods, using two real and two synthetic data sets.3
Adaptive Sampling Methods
ADP-AE: the prior adaptive method which uses the additive error CSSP-algorithm [8].
ADP-LVG: our new adaptive method using the relative error CSSP-algorithm [19].
ADP-Nopt: our adaptive method using the near optimal relative error CSSP-algorithm [15].
Data Sets
HGDP 22 chromosomes: SNPs human chromosome data from the HGDP database [26]. We
use all 22 chromosome matrices (1043 rows; 7,334-37,493 columns) and report the average.
Each matrix contains +1, 0, ‚àí1 entries, and we randomly filled in missing entries.
TechTC-300: 49 document-term matrices [27] (150-300 rows (documents); 10,000-40,000
columns (words)). We kept 5-letter or larger words and report averages over 49 data-sets.
Synthetic 1: Random 1000 √ó 10000 matrices with œÉi = i‚àí0.3 (power law).
Synthetic 2: Random 1000 √ó 10000 matrices with œÉi = exp(1‚àíi)/10 (exponential).
3
ADP-Nopt: has two stages. The first stage is a deterministic dual set spectral-Frobenius column selection
in which ties could occur. We break ties in favor of the column not already selected with the maximum norm.

6

For randomized algorithms, we repeat the experiments five times and take the average. We use the
synthetic data sets to provide a controlled environment in which we can see performance for different
types of singular value spectra on very large matrices. In prior work it is common to report on the
quality of the columns selected C by comparing the best rank-k approximation
within the column
span of C to Ak . Hence, we report the relative error A ‚àí (CC+ A)k F / kA ‚àí Ak kF when
comparing the algorithms. We set the target rank k = 5 and the number of columns in each round to
c = 2k. We have tried several choices for k and c and the results are qualitatively identical so we only
report on one choice. Our first set of results in Figure 2 is to compare the prior adaptive algorithm
ADP-AE with the new adaptive ones ADP-LVG and ADP-Nopt which boose relative error CSSPalgorithms. Our two new algorithms are both performing better the prior existing adaptive sampling
algorithm. Further, ADP-Nopt is performing better than ADP-LVG, and this is also not surprising,
because ADP-Nopt produces near-optimal columns ‚Äì if you boost a better CSSP-algorithm, you get
better results. Further, by comparing the performance on Synthetic 1 with Synthetic 2, we see that
our algorithm (as well as prior algorithms) gain significantly in performance for rapidly decaying
singular values; our new theoretical analysis reflects this behavior, whereas prior results do not.
HGDP 22 chromosomes, k:10,c=2k
||A‚àí(CC+A)k||F/||A‚àíAk||F

The theory bound depends on  = c/k. The figure
to the right shows a result for k = 10; c = 2k
(k increases but  is constant). Comparing the figure with the HGDP plot in Figure 2, we see that
the quantitative performance is approximately the
same, as the theory predicts (since c/k has not
changed). The percentage error stays the same
even though we are sampling more columns because the benchmark kA ‚àí Ak kF also get smaller
when k increases. Since ADP-Nopt is the superior algorithm, we continue with results only for
this algorithm.

ADP‚àíAE
ADP‚àíLVG
ADP‚àíNopt

1.06
1.04
1.02
1
1

2

3
4
# of rounds

5

||A-(CC+A)k|| F /||A-Ak|| F

Our next experiment is to test which adaptive strategy works better in practice given the same iniTechTC-300 49 Datasets, k:5,c=2k
tial selection of columns. That is, in Figure 2,
1.2
ADP-AE uses an adaptive sampling based on the
ADP-AE
residual A ‚àí CC+ A and then adaptively samADP-LVG
1.15
ADP-Nopt
ples according to the adaptive strategy in [8]; the
initial columns are chosen with the additive error
1.1
algorithm. Our approach chooses initial columns
with the relative error CSSP-algorithm and then
1.05
continues to sample adaptively based on the relative error CSSP-algorithm and the residual A ‚àí
1
1
2
3
4
5
(CC+ A)tk . We now give all the adaptive sam# of rounds
pling algorithms the benefit of the near-optimal
initial columns chosen in the first round by the algorithm from [15]. The result shown to the right confirms that ADP-Nopt is best even if all adaptive
strategies start from the same initial near-optimal columns.

7

TechTC300 49 datasets, k:5,c:2k

||A-(CC+A)k||F /||A-Ak||F

Adaptive versus Continued Sequential Sampling. Our last experiment is to demonstrate that
adaptive sampling works better than continued sequential sampling. We consider the relative error
CSSP-algorithm in [15] in two modes. The first
is ADP-Nopt, which is our adaptive sampling algorithms which selects tc columns in t rounds of
c columns each. The second is SEQ-Nopt, which
is just the relative error CSSP-algorithm in [15]
sampling tc columns, all in one go. The results
are shown on the right. The adaptive boosting of
the relative error CSSP-algorithm can gives up to
a 1% improvement in this data set.

1.02

ADP-Nopt
SEQ-Nopt

1.015
1.01
1.005
1
1

2

3

# of rounds

4

5

||A‚àí(CC+A) || /||A‚àíA ||

k F

ADP‚àíAE
ADP‚àíLVG
ADP‚àíNopt

1.06

1.02
1
1

TechTC‚àí300 49 Datasets, k:5,c=2k
1.15
ADP‚àíAE
ADP‚àíLVG
ADP‚àíNopt
1.1

k F

1.04

+

||A‚àí(CC A)k||F/||A‚àíAk||F

HGDP 22 chromosomes, k:5,c=2k

2

3
4
# of rounds

1.05

1
1

5

Synthetic Data 1, k:5,c=2k

1.1

k F

||A‚àí(CC+A) || /||A‚àíA ||

1.02
1.01
1
1

5

ADP‚àíAE
ADP‚àíLVG
ADP‚àíNopt

k F

ADP‚àíAE
ADP‚àíLVG
ADP‚àíNopt

1.03

3
4
# of rounds

Synthetic Data 2, k:5,c=2k

+

||A‚àí(CC A)k||F/||A‚àíAk||F

1.04

2

2

3
4
# of rounds

1.05

1
1

5



2

3
4
# of rounds

5



Figure 2: Plots of relative error ratio A ‚àí (CC+ A)k F / kA ‚àí Ak kF for various adaptive sampling al-

gorithms for k = 5 and c = 2k. In all cases, performance improves with more rounds of sampling, and rapidly
converges to a relative reconstruction error of 1. This is most so in data matrices with singular values that decay
quickly (such as TechTC and Synthetic 2). The HGDP singular values decay slowly because missing entries
are selected randomly, and Synthetic 1 has slowly decaying power-law singular values by construction.

4

Conclusion

We present a new approach for adaptive sampling algorithms which can boost relative error CSSPalgorithms, in particular the near optimal CSSP-algorithm in [15]. We showed theoretical and experimental evidence that our new adaptively boosted CSSP-algorithm is better than the prior existing
adaptive sampling algorithm which is based on the additive error CSSP-algorithm in [11]. We also
showed evidence (theoretical and empirical) that our adaptive sampling algorithms are better than
sequentially sampling all the columns at once. In particular, our theoretical bounds give a result
which is tighter for matrices whose singular values decay rapidly.
Several interesting questions remain. We showed that the simplest adaptive sampling algorithm
which samples a constant number of columns in each round improves upon sequential sampling all
at once. What is the optimal sampling schedule, and does it depend on the singular value spectrum
of the data matric? In particular, can improved theoretical bounds or empirical performance be
obtained by carefully choosing how many columns to select in each round?
It would also be interesting to see the improved adaptive sampling boosting of CSSP-algorithms in
the actual applications which require column selection (such as sparse PCA or unsupervised feature
selection). How do the improved theoretical estimates we have derived carry over to these problems
(theoretically or empirically)? We leave these directions for future work.
Acknowledgements
Most of the work was done when SP was a graduate student at RPI. PD was supported by IIS1447283 and IIS-1319280. MMI was partially supported by the Army Research Laboratory under
Cooperative Agreement Number W911NF-09-2-0053 (the ARL Network Science CTA). The views
and conclusions contained in this document are those of the authors and should not be interpreted
as representing the official policies, either expressed or implied, of the Army Research Laboratory
or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for
Government purposes notwithstanding any copyright notation here on.
8

References
[1] Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near optimal coresets for least-squares
regression. IEEE Transactions on Information Theory, 59(10), October 2013.
[2] C. Boutsidis and M. Magdon-Ismail. A note on sparse least-squares regression. Information Processing
Letters, 115(5):273‚Äì276, 2014.
[3] Christos Boutsidis and Malik Magdon-Ismail. Deterministic feature selection for k-means clustering.
IEEE Transactions on Information Theory, 59(9), September 2013.
[4] Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Sparse features for pca-like regression. In
Proc. 25th Annual Conference on Neural Information Processing Systems (NIPS), 2011. to appear.
[5] Malik Magdon-Ismail and Christos Boutsidis. Optimal sparse linear auto-encoders and sparse pca.
arXiv:1502.06626, 2015.
[6] T. F. Chan and P. C. Hansen. Some applications of the rank revealing QR factorization. SIAM J. Sci. Stat.
Comput., 13(3):727‚Äì741, 1992.
[7] A. Deshpande and L. Rademacher. Efficient volume sampling for row/column subset selection. In Proceedings of the IEEE 51st FOCS, pages 329‚Äì338, 2010.
[8] A. Deshpande and S. Vempala. Adaptive sampling and fast low-rank matrix approximation. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 292‚Äì303.
Springer, 2006.
[9] A. Deshpande, L. Rademacher, S. Vempala, and G. Wang. Matrix approximation and projective clustering
via volume sampling. Theory of Computing, 2(1):225‚Äì247, 2006.
[10] P. Drineas, I. Kerenidis, and P. Raghavan. Competitive recommendation systems. In Proceedings of the
34th STOC, pages 82‚Äì90, 2002.
[11] A. Frieze, R. Kannan, and S. Vempala. Fast monte-carlo algorithms for finding low-rank approximations.
Journal of the ACM (JACM), 51(6):1025‚Äì1041, 2004.
[12] N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms
for constructing approximate matrix decompositions. SIAM Rev., 53(2):217‚Äì288, May 2011.
[13] E. Liberty, F. Woolfe, P.G. Martinsson, V. Rokhlin, and M. Tygert. Randomized algorithms for the lowrank approximation of matrices. PNAS, 104(51):20167‚Äì20172, 2007.
[14] Michael W Mahoney and Petros Drineas. CUR matrix decompositions for improved data analysis. PNAS,
106(3):697‚Äì702, 2009.
[15] C. Boutsidis, P. Drineas, and M. Magdon-Ismail. Near-optimal column-based matrix reconstruction.
SIAM Journal of Computing, 43(2):687‚Äì717, 2014.
[16] P. Drineas, M. W Mahoney, and S Muthukrishnan. Subspace sampling and relative-error matrix approximation: Column-based methods. In Approximation, Randomization, and Combinatorial Optimization.
Algorithms and Techniques, pages 316‚Äì326. Springer, 2006.
[17] Venkatesan Guruswami and Ali Kemal Sinop. Optimal column-based low-rank matrix reconstruction. In
Proceedings of the 23rd Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1207‚Äì1214, 2012.
[18] C. Boutsidis, P. Drineas, and M. Magdon-Ismail. Near optimal column-based matrix reconstruction. In
IEEE 54th Annual Symposium on FOCS, pages 305‚Äì314, 2011.
[19] Petros Drineas, Michael W Mahoney, and S Muthukrishnan. Relative-error cur matrix decompositions.
SIAM Journal on Matrix Analysis and Applications, 30(2):844‚Äì881, 2008.
[20] T.F. Chan. Rank revealing QR factorizations. Linear Algebra and its Applications, 8889(0):67 ‚Äì 82, 1987.
[21] Crystal Maung and Haim Schweitzer. Pass-efficient unsupervised feature selection. In Advances in Neural
Information Processing Systems, pages 1628‚Äì1636, 2013.
[22] C. Boutsidis, M. W Mahoney, and P. Drineas. An improved approximation algorithm for the column
subset selection problem. In Proceedings of the 20th SODA, pages 968‚Äì977, 2009.
[23] D. Papailiopoulos, A. Kyrillidis, and C. Boutsidis. Provable deterministic leverage score sampling. In
Proc. SIGKDD, pages 997‚Äì1006, 2014.
[24] A. Deshpande, L. Rademacher, S. Vempala, and G. Wang. Matrix approximation and projective clustering
via volume sampling. In Proc. SODA, pages 1117‚Äì1126, 2006.
[25] P. Drineas and M. W Mahoney. A randomized algorithm for a tensor-based generalization of the singular
value decomposition. Linear algebra and its applications, 420(2):553‚Äì571, 2007.
[26] P. Paschou, J. Lewis, A. Javed, and P. Drineas. Ancestry informative markers for fine-scale individual
assignment to worldwide populations. Journal of Medical Genetics, 47(12):835‚Äì47, 2010.
[27] D. Davidov, E. Gabrilovich, and S. Markovitch. Parameterized generation of labeled datasets for text
categorization based on a hierarchical directory. In Proc. SIGIR, pages 250‚Äì257, 2004.

9

