Closed-form Estimators for High-dimensional
Generalized Linear Models
Eunho Yang
IBM T.J. Watson Research Center
eunhyang@us.ibm.com

Aurélie C. Lozano
IBM T.J. Watson Research Center
aclozano@us.ibm.com

Pradeep Ravikumar
University of Texas at Austin
pradeepr@cs.utexas.edu

Abstract
We propose a class of closed-form estimators for GLMs under high-dimensional
sampling regimes. Our class of estimators is based on deriving closed-form variants of the vanilla unregularized MLE but which are (a) well-defined even under
high-dimensional settings, and (b) available in closed-form. We then perform
thresholding operations on this MLE variant to obtain our class of estimators. We
derive a unified statistical analysis of our class of estimators, and show that it enjoys strong statistical guarantees in both parameter error as well as variable selection, that surprisingly match those of the more complex regularized GLM MLEs,
even while our closed-form estimators are computationally much simpler. We derive instantiations of our class of closed-form estimators, as well as corollaries
of our general theorem, for the special cases of logistic, exponential and Poisson
regression models. We corroborate the surprising statistical and computational
performance of our class of estimators via extensive simulations.

1

Introduction

We consider the estimation of generalized linear models (GLMs) [1], under high-dimensional settings where the number of variables p may greatly exceed the number of observations n. GLMs are
a very general class of statistical models for the conditional distribution of a response variable given
a covariate vector, where the form of the conditional distribution is specified by any exponential
family distribution. Popular instances of GLMs include logistic regression, which is widely used
for binary classification, as well as Poisson regression, which together with logistic regression, is
widely used in key tasks in genomics, such as classifying the status of patients based on genotype
data [2] and identifying genes that are predictive of survival [3], among others. Recently, GLMs
have also been used as a key tool in the construction of graphical models [4]. Overall, GLMs have
proven very useful in many modern applications involving prediction with high-dimensional data.
Accordingly, an important problem is the estimation of such GLMs under high-dimensional sampling regimes. Under such sampling regimes, it is now well-known that consistent estimators cannot be obtained unless low-dimensional structural constraints are imposed upon the underlying regression model parameter vector. Popular structural constraints include that of sparsity, which encourages parameter vectors supported with very few non-zero entries, group-sparse constraints, and
low-rank structure with matrix-structured parameters, among others. Several lines of work have
focused on consistent estimators for such structurally constrained high-dimensional GLMs. A popular instance, for the case of sparsity-structured GLMs, is the `1 regularized maximum likelihood
estimator (MLE), which has been shown to have strong theoretical guarantees, ranging from risk
1

consistency [5], consistency in the `1 and `2 -norm [6, 7, 8], and model selection consistency [9].
Another popular instance is the `1 /`q (for q 2) regularized MLE for group-sparse-structured logistic regression, for which prediction consistency has been established [10]. All of these estimators
solve general non-linear convex programs involving non-smooth components due to regularization.
While a strong line of research has developed computationally efficient optimization methods for
solving these programs, these methods are iterative and their computational complexity scales polynomially with the number of variables and samples [10, 11, 12, 13], making them expensive for very
large-scale problems.
A key reason for the popularity of these iterative methods is that while the number of iterations
are some function of the required accuracy, each iteration itself consists of a small finite number
of steps, and can thus scale to very large problems. But what if we could construct estimators
that overall require only a very small finite number of steps, akin to a single iteration of popular
iterative optimization methods? The computational gains of such an approach would require that
the steps themselves be suitably constrained, and moreover that the steps could be suitably profiled
and optimized (e.g. efficient linear algebra routines implemented in BLAS libraries), a systematic
study of which we defer to future work. We are motivated on the other hand by the simplicity of
such a potential class of “closed-form” estimators.
In this paper, we thus address the following question: “Is it possible to obtain closed-form estimators
for GLMs under high-dimensional settings, that nonetheless have the sharp convergence rates of the
regularized convex programs and other estimators noted above?” This question was first considered
for linear regression models [14], and was answered in the affirmative. Our goal is to see whether
a positive response can be provided for the more complex statistical model class of GLMs as well.
In this paper we focus specifically on the class of sparse-structured GLMs, though our framework
should extend to more general structures as well.
As an inkling of why closed-form estimators for high-dimensional GLMs is much trickier than that
for high-dimensional linear models is that under small-sample settings, linear regression models do
have a statistically efficient closed-form estimator — the ordinary least-squares (OLS) estimator,
which also serves as the MLE under Gaussian noise. For GLMs on the other hand, even under
small-sample settings, we do not yet have statistically efficient closed-form estimators. A classical
algorithm to solve for the MLE of logistic regression models for instance is the iteratively reweighted
least squares (IRLS) algorithm, which as its name suggests, is iterative and not available in closedform. Indeed, as we show in the sequel, developing our class of estimators for GLMs requires far
more advanced mathematical machinery (moment polytopes, and projections onto an interior subset
of these polytopes for instance) than the linear regression case.
Our starting point to devise a closed-form estimator for GLMs is to nonetheless revisit this classical
unregularized MLE estimator for GLMs from a statistical viewpoint, and investigate the reasons
why the estimator fails or is even ill-defined in the high-dimensional setting. These insights enable
us to propose variants of the MLE that are not only well-defined but can also be easily computed
in analytic-form. We provide a unified statistical analysis for our class of closed-form GLM estimators, and instantiate our theoretical results for the specific cases of logistic, exponential, and
Poisson regressions. Surprisingly, our results indicate that our estimators have comparable statistical guarantees to the regularized MLEs, in terms of both variable selection and parameter estimation
error, which we also corroborate via extensive simulations (which surprisingly even show a slight
statistical performance edge for our closed-form estimators). Moreover, our closed-form estimators
are much simpler and competitive computationally, as is corroborated by our extensive simulations.
With respect to the conditions we impose on the GLM models, we require that the population covariance matrix of our covariates be weakly sparse, which is a different condition than those typically
imposed for regularized MLE estimators; we discuss this further in Section 3.2. Overall, we hope
our simple class of statistically as well as computationally efficient closed-form estimators for GLMs
would open up the use of GLMs in large-scale machine learning applications even to lay users on the
one hand, and on the other hand, encourage the development of new classes of “simple” estimators
with strong statistical guarantees extending the initial proposals in this paper.

2

2

Setup

We consider the class of generalized linear models (GLMs), where a response variable y 2 Y,
conditioned on a covariate vector x 2 Rp , follows an exponential family distribution:
P(y|x; ✓⇤ ) = exp

⇢

h(y) + yh✓⇤ , xi A h✓⇤ , xi
c( )

(1)

where 2 R > 0 is fixed and known scale parameter, ✓⇤ 2 Rp is the GLM parameter of interest,
and A(h✓⇤ , xi) is the log-partition function or the log-normalization constant of the distribution. Our
n
goal is to estimate the GLM parameter ✓⇤ given n i.i.d. samples (x(i) , y (i) ) i=1 . By properties of
exponential families, the conditional moment of the response given the covariates can be written as
µ(h✓⇤ , xi) ⌘ E(y|x; ✓⇤ ) = A0 (h✓⇤ , xi).
Examples. Popular instances of (1) include the standard linear regression model, the logistic regression model, and the Poisson regression model, among others. In the case of the linear regression
a response
variable y 2 R, with the conditional distribution P(y|x, ✓⇤ ):
n 2model, ⇤we have
o
y /2+yh✓ ,xi h✓ ⇤ ,xi2 /2
exp
, where the log-partition function (or log-normalization constant)
2
A(a) of (1) in this specific case is given by A(a) = a2 /2. Another popular GLM instance is
⇤
the logistic regression
output variable y 2 Y ⌘ { 1, 1},
⇥ model ⇤P(y|x, ✓ ), for ⇤a categorical
⇤
⇤
exp yh✓ , xi log exp( h✓ , xi) + exp(h✓ , xi) where the log-partition function A(a) =
log exp( a) + exp(a) . The exponential regression model P(y|x, ✓⇤ ) in turn is given by:
exp yh✓⇤ , xi + log
h✓⇤ , xi . Here, the domain of response variable Y = R+ is the set
of non-negative real numbers (it is typically used to model time intervals between events for instance), and the log-partition function A(a) = log( a). Our final example is the Poisson regression model, P(y|x, ✓⇤ ): exp
log(y!) + yh✓⇤ , xi exp h✓⇤ , xi where the response variable is
count-valued with domain Y ⌘ {0, 1, 2, ...}, and with log-partition function A(a) = exp(a).

Any exponential family distribution can be used to derive a canonical GLM regression model (1)
of a response y conditioned on covariates x, by setting the canonical parameter of the exponential
family distribution to h✓⇤ , xi. For the parameterization to be valid, the conditional density should be
normalizable, so that A h✓⇤ , xi < +1.
High-dimensional Estimation Suppose that we are given n covariate vectors, x(i) 2 Rp , drawn
i.i.d. from some distribution, and corresponding response variables, y (i) 2 Y, drawn from the
distribution P(y|x(i) , ✓⇤ ) in (1). A key goal in statistical estimation is to estimate the parameters
n
✓⇤ 2 Rp , given just the samples (x(i) , y (i) ) i=1 . Such estimation becomes particularly challenging
in a high-dimensional regime, where the dimension of covariate vector p is potentially even larger
than the number of samples n. In such high-dimensional regimes, it is well understood that structural
constraints on ✓⇤ are necessary in order to find consistent estimators. In this paper, we focus on the
structural constraint of element-wise sparsity, so that the number of non-zero elements in ✓⇤ is less
than or equal to some value k much smaller than p: k✓⇤ k0  k.
Estimators: Regularized Convex Programs The `1 norm is known to encourage the estimation of such sparse-structured parameters ✓⇤ . Accordingly, a popular class of M -estimators
for sparse-structured GLM parameters is the `1 regularized maximum log-likelihood estimator
n
for (1). Given n samples (x(i) , y (i) ) i=1 from P(y|x, ✓⇤ ), the `1 regularized MLEs can be
⌦ 1 Pn
↵
Pn
written as: minimize ✓
✓, n i=1 y (i) x(i) + n1 i=1 A h✓, x(i) i + n k✓k1 . For notational simplicity, we collate the n observations in vector and matrix forms where we overload
the notation y 2 Rn to denote the vector of n responses so that i-th element of y, yi , is
y (i) , and X 2 Rn⇥p to denote the design matrix whose i-th row is [x(i) ]> . With this notation we can rewrite optimization problem characterizing the `1 -regularized MLE simply as
1 > >
1 >
minimize ✓
n ✓ X y + n 1 A(X✓) + n k✓k1 where we overload the notation A(·) for an
input vector ⌘ 2 Rn to denote A(⌘) ⌘ A(⌘1 ), A(⌘2 ), . . . , A(⌘n )
3

>

, and 1 ⌘ (1, . . . , 1)> 2 Rn .

3

Closed-form Estimators for High-dimensional GLMs

The goal of this paper is to derive a general class of closed-form estimators for high-dimensional
GLMs, in contrast to solving huge, non-differentiable `1 regularized optimization problems. Before
introducing our class of such closed-form estimators, we first introduce some notation.
For any u 2 Rp , we use [S (u)]i = sign(ui ) max(|ui |
, 0) to denote the element-wise softthresholding operator, with thresholding parameter . For any given matrix M 2 Rp⇥p , we denote
by T⌫ (M ) : Rp⇥p 7! Rp⇥p a family of matrix thresholding operators that are defined point-wise,
so that they can be written as [T⌫ (M )]ij := ⇢⌫ (Mij ), for any scalar thresholding operator ⇢⌫ (·)
that satisfies the following conditions: for any input a 2 R, (a) |⇢⌫ (a)|  |a|, (b) |⇢⌫ (a)| = 0 for
|a|  ⌫, and (c) |⇢⌫ (a) a|  ⌫. The standard soft-thresholding and hard-thresholding operators
are both pointwise operators that satisfy these properties. See [15] for further discussion of such
pointwise matrix thresholding operators.
For any ⌘ 2 Rn , we let rA(⌘) denote the element-wise gradients: rA(⌘) ⌘ A0 (⌘1 ), A0 (⌘2 ), . . . ,
>

A0 (⌘n ) . We assume that the exponential family underlying the GLM is minimal, so that this map
is invertible, and so that for any µ 2 Rn in the range of rA(·), we can denote [rA] 1 (µ) as an
>
element-wise inverse map of rA(·): (A0 ) 1 (µ1 ), (A0 ) 1 (µ2 ), . . . , (A0 ) 1 (µn ) .
Consider the response moment polytope M := {µ : µ = Ep [y], for some distribution p over
y 2 Y}, and let Mo denote the interior of M. Our closed-form estimator will use a carefully
selected subset
M ✓ Mo .

(2)

[⇧M̄ (y)]i := ⇧M̄ (yi ).

(3)

Denote the projection of a response variable y 2 Y onto this subset as ⇧M̄ (y) = arg minµ2M̄ |y
µ|, where the subset M is selected so that the projection step is always well-defined, and the minimum exists. Given a vector y 2 Y n , we denote the vector of element-wise projections of entries in
y as ⇧M̄ (y) so that:
As the conditions underlying our theorem will make clear, we will need the operator [rA] 1 (·)
defined above to be both well-defined and Lipschitz in the subset M of the interior of the response
moment polytope. In later sections, we will show how to carefully construct such a subset M for
different GLM models.
We now have the machinery to describe our class of closed-form estimators:
!
h ⇣ X > X ⌘i 1 X > [rA] 1 ⇧ (y)
M̄
✓bElem = S n
T⌫
,
n
n

(4)

where the various mathematical terms were defined above. It can be immediately seen that the
estimator is available in closed-form. In a later section, we will see instantiations of this class of
estimators for various specific GLM models, and where we will see that these estimators take very
simple forms. Before doing so, we first describe some insights that led to our particular construction
of the high-dimensional GLM estimator above.
3.1

Insights Behind Construction of Our Closed-Form Estimator

We first revisit the classical unregularized MLE for GLMs:
✓b
2
1 > >
1 >
arg min✓
✓
X
y
+
1
A(X✓)
.
Note
that
this
optimization
problem
does
not
have
a
n
n
unique minimum in general, especially under high-dimensional sample settings where p > n.
Nonetheless, it is instructive to study why this unregularized MLE is either ill-suited or even
ill-defined under high-dimensional settings. The stationary condition of unregularized MLE
optimization problem can be written as:
b
X > y = X > rA(X ✓).

(5)

There are two main caveats to solving for a unique ✓b satisfying this stationary condition, which we
clarify below.
4

(Mapping to mean parameters) In a high dimensional sampling regime where p
n, (5) can
T
b
be seen to reduce to y = rA(X ✓) (so long as X has rank n). This then suggests solving for
X ✓b = [rA] 1 (y), where we recall the definition of the operator rA(·) in terms of element-wise
operations involving A0 (·). The caveat however is that A0 (·) is only onto the interior Mo of the
response moment polytope [16], so that [A0 (·)] 1 is well-defined only when given µ 2 Mo . When
entries of the sample response vector y however lie outside of Mo , as will typically be the case and
which we will illustrate for multiple instances of GLM models in later sections, the inverse mapping
would not be well-defined. We thus first project the sample response vector y onto M ✓ Mo
to obtain ⇧M̄ (y) as defined in (3). Armed with this approximation, we then consider the more
b instead of the original stationary condition in (5).
amenable ⇧M̄ (y) ⇡ rA(X ✓),
(Sample covariance) We thus now have the approximate characterization of the MLE as X ✓b ⇡
[rA] 1 ⇧M̄ (y) . This then suggests solving for an approximate MLE ✓b via least squares as
✓b = [X > X] 1 X > [rA] 1 ⇧M̄ (y) . The high-dimensional regime with p > n poses a caveat

here, since the sample covariance matrix (X > X)/n would then be rank-deficient, and hence not
>
invertible. Our approach is to then use a thresholded sample covariance matrix T⌫ X n X defined
in the previous subsection instead, which can be shown to be invertible and consistent to the population covariance matrix ⌃ with high probability [15, 17]. In particular, recent work [15] has shown
>
that thresholded sample covariance T⌫ X n X is consistent with respect to the spectral norm with
⇣ q
⌘
>
convergence rate T⌫ X n X
⌃ op  O c0 logn p , under some mild conditions detailed in our
main theorem. Plugging in this thresholded sample covariance matrix, to get an approximate least
squares solution for the GLM parameters ✓, and then performing soft-thresholding precisely yields
our closed-form estimator in (4).
Our class of closed-form estimators in (4) can thus be viewed as surgical approximations to the MLE
so that it is well-defined in high-dimensional settings, as well as being available in closed-form. But
would such an approximation actually yield rigorous consistency guarantees? Surprisingly, as we
show in the next section, not only is our class of estimators consistent, but in our corollaries we
show that the statistical guarantees are comparable to those of the state of the art iterative ways like
regularized MLEs.
We note that our class of closed-form estimators in (4) can also be written in an equivalent form that
is more amenable to analysis:
minimize k✓k1
(6)
✓

h

⇣ X > X ⌘i

1 X > [rA] 1

⇧M̄ (y)

 n.
n
n
1
The equivalence between (4) and (6) easily follows from the fact that the optimization problem (6)
is decomposable into independent element-wise sub-problems, and each sub-problem corresponds
to soft-thresholding. It can be seen that this form is also amenable to extending the framework in
this paper to structures beyond sparsity, by substituting in alternative regularizers. Due to space
constraints, the computational complexity is discussed in detail in the Appendix.
s. t

3.2

✓

T⌫

Statistical Guarantees

In this subsection, we provide an unified statistical analysis for the class of estimators (4) under the
following standard conditions, namely sparse ✓⇤ and sub-Gaussian design X:
(C1) The parameter ✓⇤ in (1) is exactly sparse with k non-zero elements indexed by the support
set S, so that ✓S⇤ c = 0.
(C2) Each row of the design matrix X 2 Rn⇥p is i.i.d. sampled from a zero-mean distribution
with covariance matrix ⌃ such that for any v 2 Rp , the variable hv, Xi i is sub-Gaussian with
parameter at most u kvk2 for every row of X, Xi .

Our next assumption is on the covariance matrix of the covariate random vector:

(C3) The covariance matrix ⌃ of X satisfies that for all w 2 Rp , k⌃wk1
` kwk1 with
fixed constant ` > 0. Moreover, ⌃ is approximately sparse, along the lines of [17]: for some
5

positive
Ppconstant D, ⌃ii  D for all diagonal entries, and moreover, for some 0  q < 1 and c0 ,
maxi j=1 |⌃ij |q  c0 . If q = 0, then this condition will be equivalent with ⌃ being sparse.

We also introduce some notations used in the followingptheorem. Under the condition (C2), we
⇤
(i)
⇤
have that with high
p probability, |h✓ , x i|  2u k✓ k2 log n for all samples, i = 1, . . . , n. Let
⌧ ⇤ := 2u k✓⇤ k2 log n. We then let M0 be the subset of M such that
n
o
M0 := µ : µ = A0 (↵) , where ↵ 2 [ ⌧ ⇤ , ⌧ ⇤ ] .
(7)
We also define u,A and `,A on the upper bounds of A00 (·) and (A
max

↵2[ ⌧ ⇤ ,⌧ ⇤ ]

|A00 (↵)|  u,A ,

max

a2M0 [M̄

|(A

) (·), respectively:

1 0

1 0

) (a)|  `,A .

(8)

Armed with these conditions and notations, we derive our main theorem:
Theorem 1. Consider any generalized linear model in (1) where all the conditions (C1), (C2) and
(C3) hold.
problem (4) setting the thresholding parameter
q Now, suppose that we solve the estimation
p
log p0
⌫ = C1
where C1 := 16(maxj ⌃jj ) 10⌧ for any constant ⌧ > 2, and p0 := max{n, p}.
n
q
0
Furthermore, suppose also that we set the constraint bound n as C2 lognp + E where C2 :=
⇣
⌘
p
2
2u,A + C1 k✓⇤ k1 and where E depends on the approximation error induced by the
` u `,A
⇣
⇥
⇤ ⌘ 4u `,A q log p0
projection (3), and is defined as: E := maxi=1,...,n y (i)
⇧M̄ (y) i
`
n .
2

(A) Then, as long as n > 2c1`c0 1 q log p0 where c1 is a constant related only on ⌧ and maxi ⌃ii ,
any optimal solution ✓b of (4) is guaranteed to be consistent:
✓b

✓⇤

1

✓ q
◆
0
 2 C2 lognp + E , ✓b

✓⇤

2

✓ q
◆
p
0
 4 k C2 lognp + E , ✓b

✓⇤

1

✓ q
◆
0
 8k C2 lognp + E .

(B) Moreover, the support set of the estimate ✓b correctly excludes all true zero values of ✓⇤ . Moreover, when mins2S |✓s⇤ | 3 n , it correctly includes all non-zero true supports of ✓⇤ ,
0
with probability at least 1 cp0 c for some universal constants c, c0 > 0 depending on ⌧ and u .

Remark 1. While our class of closed-form estimators and analyses consider sparse-structured parameters, these can be seamlessly extended to more general structures (such as group sparsity and
low rank), using appropriate thresholding functions.
Remark 2. The condition (C3) required in Theorem 1 is different from (and possibly stronger)
than the restricted strong convexity [8] required for `2 error bound of `1 regularized MLE. A key
facet of our analysis with our Condition (C3) however is that it provides much simpler and clearer
identifying constants in our non-asymptotic error bounds. Deriving constant factors in the analysis
of the `1 -regularized MLE on the other hand, with its restricted strong convexity condition, involves
many probabilistic statements, and is non-trivial, as shown in [8].
Another key facet of our analysis in Theorem 1 is that it also provides an `1 error bound, and
guarantees the sparsistency of our closed-form estimator. For `1 regularized MLEs, this requires a
separate sparsistency analysis. In the case of the simplest standard linear regression models, [18]
showed that the incoherence condition of |||⌃S c S ⌃SS1 |||1 < 1 is required for sparsistency, where
||| · |||1 is the maximum of absolute row sum. As discussed in [18], instances of such incoherent
covariance matrices ⌃ include the identity, and Toeplitz matrices: these matrices can be seen to
also satisfy our condition (C3). On the other hand, not all matrices that satisfy our condition (C3)
need satisfy the stringent incoherence condition in turn. For example, consider ⌃ where ⌃SS =
0.95I3 + 0.0513⇥3 for a matrix 1 of ones, ⌃SS c is all zeros but the last column is 0.413⇥1 , and
⌃S c S c = I(p 3)⇥(p 3) . Then, this positive definite ⌃ can be seen to satisfy our Condition (C3),
since each row has only 4 non-zeros. However, |||⌃S c S ⌃SS1 |||1 is equal to 1.0909 and larger than 1,
and consequently, the incoherence condition required for the Lasso will not be satisfied. We defer
relaxing our condition (C3) further as well as a deeper investigation of all the above conditions to
future work.
6

Remark 3. The constant C2 in the statement
depends on k✓⇤ k1 , which in the worst case where
p
⇤
only k✓ k2 is bounded, may scale with k. On the other hand, our theorem does not require an
explicit sample complexity condition that n be larger than some function on k, while the analysis
of `1 -regularized MLEs do additionally require that n
c k log p for some constant c. In our
experiments, we verify that our closed-form estimators outperform the `1 -regularized MLEs even
when k is fairly large (for instant, when (n, p, k) = (5000, 104 , 1000)).
In order to apply Theorem 1 to a specific instance of GLMs, we need to specify the quantities in (8),
as well as carefully construct a subset M of the interior of the response moment polytope. In case
of the simplest linear models described in Section 2, we have the identity mapping µ = A0 (⌘) = ⌘.
The inequalities in (8) can thus be seen to be satisfied with `,A = u,A = 1 . Moreover, we can set
M := Mo = R so that ⇧M̄ (y) = y, and trivially recover the previous results in [14] as a special
case. In the following sections, we will derive the consequences of our framework for the complex
instances of logistic and Poisson regression models, which are also important members in GLMs.

4

Key Corollaries

In order to derive corollaries of our main Theorem 1, we need to specify the response polytope
subsets M, M0 in (2) and (7) respectively, as well as bound the two quantities `,A and u,A in (8).
Logistic regression models. The exponential family log-partition
function of logistic
regression
⇥
⇤
models described in Section 2 can be seen to be A(⌘) = log exp( ⌘) + exp(⌘) . Consequently,
4 exp(2⌘)
its double derivative A00 (⌘) = (exp(2⌘)+1)
2  1 for any ⌘, so that (8) holds with u,A = 1.
The response moment polytope for the binary response variable y 2 Y ⌘ { 1, 1} is the interval M = [ 1, 1], so that its interior is given by Mo = ( 1, 1). For the subset of the interior,
we define M = [ 1 + ✏, 1 ✏], for some 0 < ✏ < 1. At the same time, the forward mapping
is given by A0 (⌘) = exp(2⌘) 1)/(exp(2⌘) + 1), and hence M0 becomes [ aa+11 , aa+11 ] where
4u k✓ ⇤ k2
p

a := n log n . The inverse mapping of logistic models is given by (A0 ) 1 (µ) = 12 log 11+µµ , and
given M and M0 , it can be seen that (A0 ) 1 (µ) is Lipschitz for M [ M0 with constant less than
n
o
4u k✓ ⇤ k2
p
`,A := max 12 + 12 n log n , 1/✏ in (8). Note that with this setting of the subset M, we have
⇥
⇤
that maxi=1,...,n (y (i)
⇧M̄ (y) i ) = ✏, and moreover, ⇧M̄ (yi ) = yi (1 ✏), which we will use in
the corollary below.
Poisson regression models. Another important instance of GLMs is the Poisson regression model,
that is becoming increasingly more relevant in modern big-data settings with varied multivariate
count data. For the Poisson regression model case, the double derivative
of A(·) is not uniformly
p
upper bounded: A00 (u) = exp(u). Denoting ⌧ ⇤ := 2u k✓⇤ k2 plog n, we then have that for any
p
⇤
↵ in [ ⌧ ⇤ , ⌧ ⇤ ], A00 (↵)  exp 2 u k✓⇤ k2 log n = n2 u k✓ k2 / log n , so that (8) is satisfied with
p
⇤
u,A = n2 u k✓ k2 / log n . The response moment polytope for the count-valued response variable
y 2 Y ⌘ {0, 1, . . .} is given by M = [0, 1), so that its interior is given by Mo = (0, 1). For the
subset of the interior, we define M = [✏, 1) for some ✏ s.t. 0 < ✏ < 1. The forward mapping in this
2u k✓ ⇤ k2
p

case is simply given by A0 (⌘) = exp(⌘), and M0 in (7) becomes [a 1 , a] where a is n log n . The
inverse mapping for the Poisson regression model then is given by (A0 ) 1 (µ) = log(µ), which can
2u k✓ ⇤ k2
p

be seen to be Lipschitz for M with constant `,A = max{n log n , 1/✏} in (8). With this setting
of M, it can be seen that the projection operator is given by ⇧M̄ (yi ) = I(yi = 0)✏ + I(yi 6= 0)yi .

Now, we are ready to recover the error bounds, as a corollary of Theorem 1, for logistic regression
and Poisson models when condition (C2) holds:
Corollary 1. Consider any logistic regression model or a Poisson regression model where all conditions in Theorem 1 hold. Supposeq
that we solve our closed-form estimation problem (4), setting
p

0

0

p
the thresholding parameter ⌫ = C1 lognp , and the constraint bound n = 2` n(1/2c clog
0 /plog n) +
q
0
C1 k✓⇤ k1 lognp where c and c0 are some constants depending only on u , k✓⇤ k2 and ✏. Then the

7

Table 1: Comparisons on simulated datasets when parameters are tuned to minimize `2 error on
independent validation sets.
M ETHOD

(n, p, k)

(n = 2000, `1 MLE1
p = 5000, `1 MLE2
k = 10)
`1 MLE3
E LEM
(n = 4000, `1 MLE1
p = 5000, `1 MLE2
k = 10)
`1 MLE3
E LEM
(n = 5000, `1 MLE1
p = 104 ,
`1 MLE2
k = 100) `1 MLE3
E LEM

TP

FP

`2 E RROR

T IME

1
1
1
0.9900
1
1
1
1
1
1
1
0.9975

0.1094
0.0873
0.1000
0.0184
0.1626
0.1327
0.1112
0.0069
0.1301
0.1695
0.2001
0.3622

4.5450
4.0721
3.4846
2.7375
4.2132
3.6569
2.9681
2.6213
18.9079
18.5567
18.2351
16.4148

63.9
133.1
348.3
26.5
155.5
296.8
829.3
40.2
500.1
983.8
2353.3
151.8

(n = 5000, `1 MLE1
p = 104 ,
`1 MLE2
k = 1000) `1 MLE3
E LEM
(n = 8000, `1 MLE1
4
p = 10 ,
`1 MLE2
k = 100) `1 MLE3
E LEM
(n = 8000, `1 MLE1
p = 104 ,
`1 MLE2
k = 1000) `1 MLE3
E LEM

optimal solution ✓b of (4) is guaranteed to be consistent:
✓b

✓

✓

⇤

p

1

4

`

c log p0

✓

p
n(1/2 c0 / log n)

p
c log p0

p
n(1/2 c0 / log n)

+ C1 k✓⇤ k1

with probability at least 1
Moreover, when mins2S |✓s⇤ |

r

⇤

+ C1 k✓ k1

log p
n

c 1 p0
6
`

◆
0

c01

,

✓b

M ETHOD

(n, p, k)

r

log p0
n

✓⇤

1

◆



,

16k
`

✓b

✓

✓

⇤

2

p

TP

FP

`2 E RROR

T IME

0.7990
0.7935
0.7965
0.8295
1
1
1
0.9450
0.7965
0.7900
0.7865
0.7015

1
1
1
1
0.1904
0.2181
0.2364
0.0359
1
1
1
0.5103

65.1895
65.1165
65.1024
63.2359
18.6186
18.1806
17.6762
11.9881
65.0714
64.9650
64.8857
61.0532

520.7
1005.8
2560.1
152.1
810.6
1586.2
3568.9
221.1
809.5
1652.8
4196.6
219.4

r

◆

p
8 k

`

c log p0

p
n(1/2 c0 / log n)

+ C1 k✓⇤ k1

log p0
n

,

0
0
for some universal constants
q c1 , c1 > 0 and p := max{n, p}.
p
0
0
c logpp
+ C1 k✓⇤ k1 log p , ✓b is sparsistent.
0

n(1/2

c /

log n)

n

Remarkably, the rates in Corollary 1 are asymptotically comparable to those for the `1 -regularized
MLE (see for instance Theorem 4.2 and Corollary 4.4 in [7]). In Appendix A, we place slightly
more stringent condition than (C2) and guarantee error bounds with faster convergence rates.

5

Experiments

We corroborate the performance of our elementary estimators on simulated data over varied regimes
of sample size n, number of covariates p, and sparsity size k. We consider two popular instances
of GLMs, logistic and Poisson regression models. We compare against standard `1 regularized
MLE estimators with iteration bounds of 50, 100, and 500, denoted by `1 MLE1 , `1 MLE2 and `1
MLE3 respectively. We construct the n ⇥ p design matrices X by sampling the rows independently
from N (0, ⌃) where ⌃i,j = 0.5|i j| . For each simulation, the entries of the true model coefficient
vector ✓⇤ are set to be 0 everywhere, except for a randomly chosen subset of k coefficients, which
are chosen independently and uniformly in the interval (1, 3). We report results averaged over 100
independent trials. Noting that our theoretical results were not sensitive to the setting of ✏ in ⇧M̄ (y),
we simply report the results when ✏ = 10 4 across all experiments.
While our theorem specified an optimal setting of the regularization parameter n and ⌫, this optimal
setting depended on unknown model parameters. Thus,
high-dimensional regup as is standard withp
larized estimators, we set tuning parameters n = c log p/n and ⌫ = c0 log p/n by a holdoutvalidated fashion; finding a parameter that minimizes the `2 error on an independent validation set.
Detailed experimental setup is described in the appendix.
Table 1 summarizes the performances of `1 MLE using 3 different stopping criteria and Elem-GLM.
Besides `2 errors, the target tuning metric, we also provide the true and false positives for the support
set recovery task on the new test set where the best tuning parameters are used. The computation
times in second indicate the overall training computation time summing over the whole parameter
tuning process. As we can see from our experiments, with respect to both statistical and computational performance our closed form estimators are quite competitive compared to the classical `1
regularized MLE estimators and in certain case outperform them. Note that `1 MLE1 stops prematurely after only 50 iterations, so that training computation time is sometimes comparable to
closed-form estimator. However, its statistical performance measured by `2 is much inferior to other
`1 MLEs with more iterations as well as Elem-GLM estimator. Due to the space limit, ROC curves,
results for other settings of p and more experiments on real datasets are presented in the appendix.
8

References
[1] P. McCullagh and J.A. Nelder. Generalized linear models. Monographs on statistics and applied probability 37. Chapman and Hall/CRC, 1989.
[2] G. E. Hoffman, B. A. Logsdon, and J. G. Mezey. Puma: A unified framework for penalized multiple
regression analysis of gwas data. Plos computational Biology, 2013.
[3] D. Witten and R. Tibshirani. Survival analysis with high-dimensional covariates. Stat Methods Med Res.,
19:29–51, 2010.
[4] E. Yang, P. Ravikumar, G. I. Allen, and Z. Liu. Graphical models via generalized linear models. In Neur.
Info. Proc. Sys. (NIPS), 25, 2012.
[5] S. Van de Geer. High-dimensional generalized linear models and the lasso. Annals of Statistics, 36(2):
614–645, 2008.
[6] F. Bach. Self-concordant analysis for logistic regression. Electron. J. Stat., 4:384–414, 2010.
[7] S. M. Kakade, O. Shamir, K. Sridharan, and A. Tewari. Learning exponential families in high-dimensions:
Strong convexity and sparsity. In Inter. Conf. on AI and Statistics (AISTATS), 2010.
[8] S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for high-dimensional
analysis of M-estimators with decomposable regularizers. Arxiv preprint arXiv:1010.2731v1, 2010.
[9] F. Bunea. Honest variable selection in linear and logistic regression models via l1 and l1 + l2 penalization.
Electron. J. Stat., 2:1153–1194, 2008.
[10] L. Meier, S. Van de Geer, and P. Bühlmann. The group lasso for logistic regression. Journal of the Royal
Statistical Society, Series B, 70:53–71, 2008.
[11] Y. Kim, J. Kim, and Y. Kim. Blockwise sparse regression. Statistica Sinica, 16:375–390, 2006.
[12] J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1):1–22, 2010.
[13] K. Koh, S. J. Kim, and S. Boyd. An interior-point method for large-scale `1 -regularized logistic regression. Jour. Mach. Learning Res., 3:1519–1555, 2007.
[14] E. Yang, A. C. Lozano, and P. Ravikumar. Elementary estimators for high-dimensional linear regression.
In International Conference on Machine learning (ICML), 31, 2014.
[15] A. J. Rothman, E. Levina, and J. Zhu. Generalized thresholding of large covariance matrices. Journal of
the American Statistical Association (Theory and Methods), 104:177–186, 2009.
[16] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families and variational inference.
Foundations and Trends in Machine Learning, 1(1–2):1—305, December 2008.
[17] P. J. Bickel and E. Levina. Covariance regularization by thresholding. Annals of Statistics, 36(6):2577–
2604, 2008.
[18] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using `1 -constrained
quadratic programming (Lasso). IEEE Trans. Information Theory, 55:2183–2202, May 2009.
[19] Daniel A. Spielman and Shang-Hua Teng. Solving sparse, symmetric, diagonally-dominant linear systems
in time 0(m1.31 ). In 44th Symposium on Foundations of Computer Science (FOCS 2003), 11-14 October
2003, Cambridge, MA, USA, Proceedings, pages 416–427, 2003.
[20] Michael B. Cohen, Rasmus Kyng, Gary L. Miller, Jakub W. Pachocki, Richard Peng, Anup B. Rao, and
Shen Chen Xu. Solving sdd linear systems in nearly mlog1/2n time. In Proceedings of the 46th Annual
ACM Symposium on Theory of Computing, STOC ’14, pages 343–352. ACM, 2014.
[21] Daniel A. Spielman and Shang-Hua Teng. Nearly linear time algorithms for preconditioning and solving
symmetric, diagonally dominant linear systems. SIAM J. Matrix Analysis Applications, 35(3):835–885,
2014.
[22] P. Ravikumar, M. J. Wainwright, G. Raskutti, and B. Yu. High-dimensional covariance estimation by
minimizing `1 -penalized log-determinant divergence. Electronic Journal of Statistics, 5:935–980, 2011.
[23] E. Yang, A. C. Lozano, and P. Ravikumar. Elementary estimators for sparse covariance matrices and other
structured moments. In International Conference on Machine learning (ICML), 31, 2014.
[24] E. Yang, A. C. Lozano, and P. Ravikumar. Elementary estimators for graphical models. In Neur. Info.
Proc. Sys. (NIPS), 27, 2014.

9

