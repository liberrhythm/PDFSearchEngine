On Elicitation Complexity

Rafael Frongillo
University of Colorado, Boulder

Ian A. Kash
Microsoft Research

raf@colorado.edu

iankash@microsoft.com

Abstract
Elicitation is the study of statistics or properties which are computable via empirical risk minimization. While several recent papers have approached the general question of which properties are elicitable, we suggest that this is the wrong
questionâ€”all properties are elicitable by first eliciting the entire distribution or
data set, and thus the important question is how elicitable. Specifically, what is
the minimum number of regression parameters needed to compute the property?
Building on previous work, we introduce a new notion of elicitation complexity
and lay the foundations for a calculus of elicitation. We establish several general
results and techniques for proving upper and lower bounds on elicitation complexity. These results provide tight bounds for eliciting the Bayes risk of any loss, a
large class of properties which includes spectral risk measures and several new
properties of interest.

1

Introduction

Empirical risk minimization (ERM) is a domininant framework for supervised machine learning,
and a key component of many learning algorithms. A statistic or property is simply a functional
assigning a vector of values to each distribution. We say that such a property is elicitable, if for
some loss function it can be represented as the unique minimizer of the expected loss under the
distribution. Thus, the study of which properties are elicitable can be viewed as the study of which
statistics are computable via ERM [1, 2, 3].
The study of property elicitation began in statistics [4, 5, 6, 7], and is gaining momentum in machine
learning [8, 1, 2, 3], economics [9, 10], and most recently, finance [11, 12, 13, 14, 15]. A sequence of
papers starting with Savage [4] has looked at the full characterization of losses which elicit the mean
of a distribution, or more generally the expectation of a vector-valued random variable [16, 3]. The
case of real-valued properties is also now well in hand [9, 1]. The general vector-valued case is still
generally open, with recent progress in [3, 2, 15]. Recently, a parallel thread of research has been
underway in finance, to understand which financial risk measures, among several in use or proposed
to help regulate the risks of financial institutions, are computable via regression, i.e., elicitable (cf.
references above). More often than not, these papers have concluded that most risk measures under
consideration are not elicitable, notable exceptions being generalized quantiles (e.g. value-at-risk,
expectiles) and expected utility [13, 12].
Throughout the growing momentum of the study of elicitation, one question has been central: which
properties are elicitable? It is clear, however, that all properties are â€œindirectlyâ€ elicitable if one first
elicits the distribution using a standard proper scoring rule. Therefore, in the present work, we
suggest replacing this question with a more nuanced one: how elicitable are various properties?
Specifically, heeding the suggestion of Gneiting [7], we adapt to our setting the notion of elicitation
complexity introduced by Lambert et al. [17], which captures how many parameters one needs to
maintain in an ERM procedure for the property in question. Indeed, if a real-valued property is
found not to be elicitable, such as the variance, one should not abandon it, but rather ask how many
parameters are required to compute it via ERM.
1

Our work is heavily inspired by the recent progress along these lines of Fissler and Ziegel [15], who
show that spectral risk measures of support k have elicitation complexity at most k + 1. Spectral
risk measures are among those under consideration in the finance community, and this result shows
that while not elicitable in the classical sense, their elicitation complexity is still low, and hence one
can develop reasonable regression procedures for them. Our results extend to these and many other
risk measures (see Â§ 4.6), often providing matching lower bounds on the complexity as well.
Our contributions are the following. We first introduce an adapted definition of elicitation complexity which we believe to be the right notion to focus on going forward. We establish a few simple but
useful results which allow for a kind of calculus of elicitation; for example, conditions under which
the complexity of eliciting two properties in tandem is the sum of their individual complexities. In
Â§ 3, we derive several techniques for proving both upper and lower bounds on elicitation complexity
which apply primarily to the Bayes risks from decision theory, or optimal expected loss functions.
The class includes spectral risk measures among several others; see Â§ 4. We conclude with brief
remarks and open questions.

2

Preliminaries and Foundation

Let â„¦ be a set of outcomes and P âŠ† âˆ†(â„¦) be a convex set of probability measures. The goal of
elicitation is to learn something about the distribution p âˆˆ P, specifically some function Î“(p) such
as the mean or variance, by minimizing a loss function.
Definition 1. A property is a function Î“ : P â†’ Rk , for some k âˆˆ N, which associates a desired
.
report value to each distribution.1 We let Î“r = {p âˆˆ P | r = Î“(p)} denote the set of distributions p
corresponding to report value r.
Given a property Î“, we want to ensure that the best result is to reveal the value of the property using
a loss function that evaluates the report using a sample from the distribution.
Definition 2. A loss function L : Rk Ã— â„¦ â†’ R elicits a property Î“ : P â†’ Rk if for all p âˆˆ P,
.
Î“(p) = arginf r L(r, p), where L(r, p) = Ep [L(r, Â·)]. A property is elicitable if some loss elicits it.
For example, when â„¦ = R, the mean Î“(p) = Ep [Ï‰] is elicitable via squared loss L(r, Ï‰) = (râˆ’Ï‰)2 .
A well-known necessary condition for elicitability is convexity of the level sets of Î“.
Proposition 1 (Osband [5]). If Î“ is elicitable, the level sets Î“r are convex for all r âˆˆ Î“(P).
One can easily check that the mean Î“(p) = Ep [Ï‰] has convex level sets, yet the variance Î“(p) =
Ep [(Ï‰ âˆ’ Ep [Ï‰])2 ] does not, and hence is not elicitable [9].
It is often useful to work with a stronger condition, that not only is Î“r convex, but it is the intersection
of a linear subspace with P. This condition is equivalent the existence of an identification function,
a functional describing the level sets of Î“ [17, 1].
Definition 3. A function V : RÃ—â„¦ â†’ Rk is an identification function for Î“ : P â†’ Rk , or identifies
Î“, if for all r âˆˆ Î“(P) it holds that p âˆˆ Î“r â‡â‡’ V (r, p) = 0 âˆˆ Rk , where as with L(r, p) above we
.
write V (r, p) = Ep [V (r, Ï‰)]. Î“ is identifiable if there exists a V identifying it.
One can check for example that V (r, Ï‰) = Ï‰ âˆ’ r identifies the mean.
We can now define the classes of identifiable and elicitable properties, along with the complexity
of identifying or eliciting a given property. Naturally, a property is k-identifiable if it is the link of
a k-dimensional identifiable property, and k-elicitable if it is the link of a k-dimensional elicitable
property. The elicitation complexity of a property is then simply the minimum dimension k needed
for it to be k-elicitable.
Definition 4. Let Ik (P) denote the class of all identifiable properties Î“ : P â†’SRk , and Ek (P)
k
denote the
S class of all elicitable properties Î“ : P â†’ R . We write I(P) = kâˆˆN Ik (P) and
E(P) = kâˆˆN Ek (P).
Definition 5. A property Î“ is k-identifiable if there exists Î“Ì‚ âˆˆ Ik (P) and f such that Î“ = f â—¦ Î“Ì‚.
The identification complexity of Î“ is defined as iden(Î“) = min{k : Î“ is k-identifiable}.
1

We will also consider Î“ : P â†’ RN .

2

Definition 6. A property Î“ is k-elicitable if there exists Î“Ì‚ âˆˆ Ek (P) and f such that Î“ = f â—¦ Î“Ì‚. The
elicitation complexity of Î“ is defined as elic(Î“) = min{k : Î“ is k-elicitable}.
To make the above definitions concrete, recall that the variance Ïƒ 2 (p) = Ep [(Ep [Ï‰]âˆ’Ï‰)2 ] is not elicitable, as its level sets are not convex, a necessary condition by Prop. 1. Note however that we may
write Ïƒ 2 (p) = Ep [Ï‰ 2 ]âˆ’Ep [Ï‰]2 , which can be obtained from the property Î“Ì‚(p) = (Ep [Ï‰], Ep [Ï‰ 2 ]). It
is well-known [4, 7] that Î“Ì‚ is both elicitable and identifiable as the expectation of a vector-valued random variable X(Ï‰) = (Ï‰, Ï‰ 2 ), using for example L(r, Ï‰) = kr âˆ’X(Ï‰)k2 and V (r, Ï‰) = r âˆ’X(Ï‰).
Thus, we can recover Ïƒ 2 as a link of the elicitable and identifiable Î“Ì‚ : P â†’ R2 , and as no such
Î“Ì‚ : P â†’ R exists, we have iden(Ïƒ 2 ) = elic(Ïƒ 2 ) = 2.
In this example, the variance has a stronger property than merely being 2-identifiable and 2elicitable, namely that there is a single Î“Ì‚ that satisfies both of these simultaneously. In fact this
is quite common, and identifiability provides geometric structure that we make use of in our lower
bounds. Thus, most of our results use this refined notion of elicitation complexity.
Definition 7. A property Î“ has (identifiable) elicitation complexity
elicI (Î“) = min{k : âˆƒÎ“Ì‚, f such that Î“Ì‚ âˆˆ Ek (P) âˆ© Ik (P) and Î“ = f â—¦ Î“Ì‚}.
Note that restricting our attention to elicI effectively requires elicI (Î“) â‰¥ iden(Î“); specifically, if Î“
is derived from some elicitable Î“Ì‚, then Î“Ì‚ must be identifiable as well. This restriction is only relevant
for our lower bounds, as our upper bounds give losses explicitly.2 Note however that some restriction
on Ek (P) is necessary, as otherwise pathological constructions giving injective mappings from R to
Rk would render all properties 1-elicitable. To alleviate this issue, some authors require continuity
(e.g. [1]) while others like we do require identifiability (e.g. [15]), which can be motivated by the
fact that for any differentiable loss L for Î“, V (r, Ï‰) = âˆ‡r L(Â·, Ï‰) will identify Î“ provided Ep [L]
has no inflection points or local minima. An important future direction is to relax this identifiability
assumption, as there are very natural (set-valued) properties with iden > elic.3
Our definition of elicitation complexity differs from the notion proposed by Lambert et al. [17], in
that the components of Î“Ì‚ above do not need to be individually elicitable. This turns out to have
a large impact, as under their definition the property Î“(p) = maxÏ‰âˆˆâ„¦ p({Ï‰}) for finite â„¦ has
elicitation complexity |â„¦| âˆ’ 1, whereas under our definition elicI (Î“) = 2; see Example 4.3. Fissler
and Ziegel [15] propose a closer but still different definition, with the complexity being the smallest
k such that Î“ is a component of a k-dimensional elicitable property. Again, this definition can lead
to larger complexities than necessary; take for example the squared mean Î“(p) = Ep [Ï‰]2 when
â„¦ = R, which has elicI (Î“) = 1 with Î“Ì‚(p) = Ep [Ï‰] and f (x) = x2 , but is not elicitable and thus has
complexity 2 under [15]. We believe that, modulo regularity assumptions on Ek (P), our definition is
better suited to studying the difficulty of eliciting properties: viewing f as a (potentially dimensionreducing) link function, our definition captures the minimum number of parameters needed in an
ERM computation of the property in question, followed by a simple one-time application of f .
2.1

Foundations of Elicitation Complexity

In the remainder of this section, we make some simple, but useful, observations about iden(Î“) and
elicI (Î“). We have already discussed one such observation after Definition 7: elicI (Î“) â‰¥ iden(Î“).
It is natural to start with some trivial upper bounds. Clearly, whenever p âˆˆ P can be uniquely determined by some number of elicitable parameters then the elicitation complexity of every property is
at most that number. The following propositions give two notable applications of this observation.4
Proposition 2. When |â„¦| = n, every property Î“ has elicI (Î“) â‰¤ n âˆ’ 1.
Proof. The probability distribution is determined by the probability of any n âˆ’ 1 outcomes, and the
probability associated with a given outcome is both elicitable and identifiable.
2

Our main lower bound (Thm 2) merely requires Î“ to have convex level sets, which is necessary by Prop. 1.
One may take for example Î“(p) = argmaxi p(Ai ) for a finite measurable partition A1 , . . . , An of â„¦.
4
Note that these restrictions on â„¦ may easily be placed on P instead; e.g. finite â„¦ is equivalent to P having
support on a finite subset of â„¦, or even being piecewise constant on some disjoint events.
3

3

Proposition 3. When â„¦ = R,5 every property Î“ has elicI (Î“) â‰¤ Ï‰ (countable).6
One well-studied class of properties are those where Î“ is linear, i.e., the expectation of some vectorvalued random variable. All such properties are elicitable and identifiable (cf. [4, 8, 3]), with
elicI (Î“) â‰¤ k, but of course the complexity can be lower if the range of Î“ is not full-dimensional.
Lemma 1. Let X : â„¦ â†’ Rk be P-integrable and Î“(p) = Ep [X]. Then elicI (Î“) =
dim(affhull(Î“(P))), the dimension of the affine hull of the range of Î“.
It is easy to create redundant properties in various ways. For example, given elicitable properties
.
Î“1 and Î“2 the property Î“ = {Î“1 , Î“2 , Î“1 + Î“2 } clearly contains redundant information. A concrete
case is Î“ = {mean squared, variance, 2nd moment}, which, as we have seen, has elicI (Î“) = 2. The
following definitions and lemma capture various aspects of a lack of such redundancy.
Definition 8. Property Î“ : P â†’ Rk in I(P) is of full rank if iden(Î“) = k.
Note that there are two ways for a property to fail to be full rank. First, as the examples above
suggest, Î“ can be â€œredundantâ€ so that it is a link of a lower-dimensional identifiable property. Full
rank can also be violated if more dimensions are needed to identify the property than to specify it.
This is the case with, e.g., the variance which is a 1 dimensional property but has iden(Ïƒ 2 ) = 2.
Definition 9. Properties Î“, Î“0 âˆˆ I(P) are independent if iden({Î“, Î“0 }) = iden(Î“) + iden(Î“0 ).
Lemma 2. If Î“, Î“0 âˆˆ E(P) are full rank and independent, then elicI ({Î“, Î“0 }) = elicI (Î“)+elicI (Î“0 ).
To illustrate the lemma, elicI (variance) = 2, yet Î“ = {mean,variance} has elicI (Î“) = 2, so clearly
the mean and variance are not both independent and full rank. (As we have seen, variance is not full
rank.) However, the mean and second moment satisfy both by Lemma 1.
Another important case is when Î“ consists of some number of distinct quantiles. Osband [5] essentially showed that quantiles are independent and of full rank, so their elicitation complexity is the
number of quantiles being elicited.
Lemma 3. Let â„¦ = R and P be a class of probability measures with continuously differentiable and invertible CDFs F , which is sufficiently rich in the sense that for all x1 , . . . , xk âˆˆ R,
span({F âˆ’1 (x1 ), . . . , F âˆ’1 (xk )}, F âˆˆ P) = Rk . Let qÎ± , denote the Î±-quantile function. Then if
Î±1 , . . . , Î±k are all distinct, Î“ = {qÎ±1 , . . . , qÎ±k } has elicI (Î“) = k.
The quantile example in particular allows us to see that all complexity classes, including Ï‰, are
occupied. In fact, our results to follow will show something stronger: even for real-valued properties
Î“ : P â†’ R, all classes are occupied; we give here the result that follows from our bounds on spectral
risk measures in Example 4.4, but this holds for many other P; see e.g. Example 4.2.
Proposition 4. Let P as in Lemma 3. Then for all k âˆˆ N there exists Î³ : P â†’ R with elicI (Î³) = k.

3

Eliciting the Bayes Risk

In this section we prove two theorems that provide our main tools for proving upper and lower
bounds respectively on elicitation complexity. Of course many properties are known to be elicitable, and the losses that elicit them provide such an upper bound for that case. We provide such
a construction for properties that can be expressed as the pointwise minimum of an indexed set of
functions. Interestingly, our construction does not elicit the minimum directly, but as a joint elicitation of the value and the function that realizes this value. The form (1) is that of a scoring rule for
the linear property p 7â†’ Ep [Xa ], except that here the index a itself is also elicited.7
Theorem 1. Let {Xa : â„¦ â†’ R}aâˆˆA be a set of P-integrable functions indexed by A âŠ† Rk . Then if
inf a Ep [Xa ] is attained, the property Î³(p) = mina Ep [Xa ] is (k + 1)-elicitable. In particular,
L((r, a), Ï‰) = H(r) + h(r)(Xa âˆ’ r)
elicits p 7â†’ {(Î³(p), a) : Ep [Xa ] = Î³(p)} for any strictly decreasing h : R â†’ R+ with

(1)
d
dr H

= h.

Here and throughout, when â„¦ = Rk we assume the Borel Ïƒ-algebra.
Omitted proofs can be found in the appendix of the full version of this paper.
7
As we focus on elicitation complexity, we have not tried to characterize all ways to elicit this joint property,
or other properties we give explicit losses for. See Â§ 4.1 for an example where additional losses are possible.
5
6

4

Proof. We will work with gains instead of losses, and show that S((r, a), Ï‰) = g(r) + dgr (Xa âˆ’ r)
elicits p 7â†’ {(Î³(p), a) : Ep [Xa ] = Î³(p)} for Î³(p) = maxa Ep [Xa ]. Here g is convex with strictly
increasing and positive subgradient dg.
For any fixed a, we have by the subgradient inequality,
S((r, a), p) = g(r) + dgr (Ep [Xa ] âˆ’ r) â‰¤ g(Ep [Xa ]) = S((Ep [Xa ], a), p) ,
and as dg is strictly increasing, g is strictly convex, so r = Ep [Xa ] is the unique maximizer. Now
letting SÌƒ(a, p) = S((Ep [Xa ], a), p), we have
argmax SÌƒ(a, p) = argmax g(Ep [Xa ]) = argmax Ep [Xa ] ,
aâˆˆA

aâˆˆA

aâˆˆA

because g is strictly increasing. We now have


argmax S((r, a), p) = (Ep [Xa ], a) : a âˆˆ argmax Ep [Xa ] .
aâˆˆA

aâˆˆA,râˆˆR

One natural way to get such an indexed set of functions is to take an arbitrary loss function L(r, Ï‰),
in which case this pointwise minimum corresponds to the Bayes risk, which is simply the minimum
possible expected loss under some distribution p.
Definition 10. Given loss function L : A Ã— â„¦ â†’ R on some prediction set A, the Bayes risk of L
is defined as L(p) := inf aâˆˆA L(a, p).
One illustration of the power of Theorem 1 is that the Bayes risk of a loss eliciting a k-dimensional
property is itself (k + 1)-elicitable.
Corollary 1. If L : Rk Ã— â„¦ â†’ R is a loss function eliciting Î“ : P â†’ Rk , then the loss
L((r, a), Ï‰) = L0 (a, Ï‰) + H(r) + h(r)(L(a, Ï‰) âˆ’ r)
elicits {L, Î“}, where h : R â†’ R+ is any positive strictly decreasing function, H(r) =
and L0 is any surrogate loss eliciting Î“.8 If Î“ âˆˆ Ik (P), elicI (L) â‰¤ k + 1.

(2)
Rr
0

h(x)dx,

We now turn to our second theorem which provides lower bounds for the elicitation complexity
of the Bayes risk. A first observation, which follows from standard convex analysis, is that L is
concave, and thus it is unlikely to be elicitable directly, as the level sets of L are likely to be nonconvex. To show a lower bound greater than 1, however, we will need much stronger techniques.
In particular, while L must be concave, it may not be strictly so, thus enabling level sets which are
potentially amenable to elicitation. In fact, L must be flat between any two distributions which share
a minimizer. Crucial to our lower bound is the fact that whenever the minimizer of L differs between
two distributions, L is essentially strictly concave between them.
Lemma 4. Suppose loss L with Bayes risk L elicits Î“ : P â†’ Rk . Then for any p, p0 âˆˆ P with
Î“(p) 6= Î“(p0 ), we have L(Î»p + (1 âˆ’ Î»)p0 ) > Î»L(p) + (1 âˆ’ Î»)L(p0 ) for all Î» âˆˆ (0, 1).
With this lemma in hand we can prove our lower bound. The crucial insight is that an identification
function for the Bayes risk of a loss eliciting a property can, through a link, be used to identify that
property. Corollary 1 tells us that k + 1 parameters suffice for the Bayes risk of a k-dimensional
property, and our lower bound shows this is often necessary. Only k parameters suffice, however,
when the property value itself provides all the information required to compute the Bayes risk; for
example, dropping the y 2 term from squared loss gives L(x, y) = x2 âˆ’ 2xy and L(p) = âˆ’Ep [y]2 ,
giving elic(L) = 1. Thus the theorem splits the lower bound into two cases.
Theorem 2. If a loss L elicits some Î“ âˆˆ Ek (P) with elicitation complexity elicI (Î“) = k, then its
Bayes risk L has elicI (L) â‰¥ k. Moreover, if we can write L = f â—¦ Î“ for some function f : Rk â†’ R,
then we have elicI (L) = k; otherwise, elicI (L) = k + 1.
Proof. Let Î“Ì‚ âˆˆ E` such that L = g â—¦ Î“Ì‚ for some g : R` â†’ R.
8
Note that one could easily lift the requirement that Î“ be a function, and allow Î“(p) to be the set of minimizers of the loss (cf. [18]). We will use this additional power in Example 4.4.

5

We show by contradiction that for all p, p0 âˆˆ P, Î“Ì‚(p) = Î“Ì‚(p0 ) implies Î“(p) = Î“(p0 ). Otherwise, we
have p, p0 with Î“Ì‚(p) = Î“Ì‚(p0 ), and thus L(p) = L(p0 ), but Î“(p) 6= Î“(p0 ). Lemma 4 would then give
us some pÎ» = Î»p + (1 âˆ’ Î»)p0 with L(pÎ» ) > L(p). But as the level sets Î“Ì‚rÌ‚ are convex by Prop. 1,
we would have Î“Ì‚(pÎ» ) = Î“Ì‚(p), which would imply L(pÎ» ) = L(p).
We now can conclude that there exists h : R` â†’ Rk such that Î“ = h â—¦ Î“Ì‚. But as Î“Ì‚ âˆˆ E` , this implies
elicI (Î“) â‰¤ `, so clearly we need ` â‰¥ k. Finally, if ` = k we have L = g â—¦ Î“Ì‚ = g â—¦ hâˆ’1 â—¦ Î“. The
upper bounds follow from Corollary 1.

4

Examples and Applications

We now give several applications of our results. Several upper bounds are novel, as well as all lower
bounds greater than 2. In the examples, unless we refer to â„¦ explicitly we will assume â„¦ = R and
write y âˆˆ â„¦ so that y âˆ¼ p. In each setting, we also make several standard regularity assumptions
which we suppress for ease of exposition â€” for example, for the variance and variantile we assume
finite first and second moments (which must span R2 ), and whenever we discuss quantiles we will
assume that P is as in Lemma 3, though we will not require as much regularity for our upper bounds.
4.1

Variance

In Section 2 we showed that elicI (Ïƒ 2 ) = 2. As a warm up, let us see how to recover this statement
using our results on the Bayes risk. We can view Ïƒ 2 as the Bayes risk of squared loss L(x, y) = (xâˆ’
y)2 , which of course elicits the mean: L(p) = minxâˆˆR Ep [(x âˆ’ y)2 ] = Ep [(Ep [y] âˆ’ y)2 ] = Ïƒ 2 (p).
This gives us elicI (Ïƒ 2 ) â‰¤ 2 by Corollary 1, with a matching lower bound by Theorem 2, as the
variance is not simply a function of the mean. Corollary 1 gives losses such as L((x, v), y) =
eâˆ’v ((x âˆ’ y)2 âˆ’ v) âˆ’ eâˆ’v which elict {Ep [y], Ïƒ 2 (p)}, but in fact there are losses which cannot
be represented by the form (2), showing that we do not have a full characterization;
 for example,
LÌ‚((x, v), y) = v 2 + v(x âˆ’ y)(2(x + y) + 1) + (x âˆ’ y)2 (x + y)2 + x + y + 1 . This LÌ‚ was

h y i2
h
i


1 âˆ’1/2
generated via squared loss z âˆ’ y2  with respect to the norm kzk2 = z > âˆ’1/2 1
z, which
elicits the first two moments, and link function (z1 , z2 ) 7â†’ (z1 , z2 âˆ’ z12 ).
4.2

Convex Functions of Means

Another simple example is Î³(p) = G(Ep [X]) for some strictly convex function G : Rk â†’ R and
P-integrable X : â„¦ â†’ Rk . To avoid degeneracies, we assume dim affhull{Ep [X] : p âˆˆ P} = k,
i.e. Î“ is full rank. Letting {dGp }pâˆˆP be a selection of subgradients of G, the loss L(r, Ï‰) =
âˆ’(G(r) + dGr (X(Ï‰) âˆ’ r)) elicits Î“ : p 7â†’ Ep [X] (cf. [3]), and moreover we have Î³(p) = âˆ’L(p).
By Lemma 1, elicI (Î“) = k. One easily checks that L = G â—¦ Î“, so now by Theorem 2, elicI (Î³) = k
as well. Letting {Xk }kâˆˆN be a family of such â€œfull rankâ€ random variables, this gives us a sequence
of real-valued properties Î³k (p) = kEp [X]k2 with elicI (Î³k ) = k, proving Proposition 4.
4.3

Modal Mass

With â„¦ = R consider the property Î³Î² (p) = maxxâˆˆR p([x âˆ’ Î², x + Î²]), namely, the maximum
probability mass contained in an interval of width 2Î². Theorem 1 easily shows elicI (Î³Î² ) â‰¤ 2,
as Î³Ì‚Î² (p) = argmaxxâˆˆR p([x âˆ’ Î², x + Î²]) is elicited by L(x, y) = 1|xâˆ’y|>Î² , and Î³Î² (p) = 1 âˆ’
L(p). Similarly, in the case of finite â„¦, Î³(p) = maxÏ‰âˆˆâ„¦ p({Ï‰}) is simply the expected score (gain
rather than loss) of the mode Î³(p) = argmaxÏ‰âˆˆâ„¦ p({Ï‰}), which is elicitable for finite â„¦ (but not
otherwise; see Heinrich [19]).
In both cases, one can easily check that the level sets of Î³ are not convex, so elicI (Î³) = 2; alternatively Theorem 2 applies in the first case. As mentioned following Definition 6, the result for finite
â„¦ differs from the definitions of Lambert et al. [17], where the elicitation complexity of Î³ is |â„¦| âˆ’ 1.
6

4.4

Expected Shortfall and Other Spectral Risk Measures

One important application of our results on the elicitation complexity of the Bayes risk is the elicitability of various financial risk measures. One of the most popular financial risk measures is
expected shortfall ESÎ± : P â†’ R, also called conditional value at risk (CVaR) or average value at
risk (AVaR), which we define as follows (cf. [20, eq.(18)], [21, eq.(3.21)]):
 
	
 
	
ESÎ± (p) = inf Ep Î±1 (z âˆ’ y)1zâ‰¥y âˆ’ z = inf Ep Î±1 (z âˆ’ y)(1zâ‰¥y âˆ’ Î±) âˆ’ y .
(3)
zâˆˆR

zâˆˆR

Despite the importance of elicitability to financial regulation [11, 22], ESÎ± is not elicitable [7]. It
was recently shown by Fissler and Ziegel [15], however, that elicI (ESÎ± ) = 2. They
R also consider the
broader class of spectral risk measures, which can be represented as ÏÂµ (p) = [0,1] ESÎ± (p)dÂµ(Î±),
where Âµ is a probability measure on [0, 1] (cf. [20, eq. (36)]). In the case where Âµ has finite support
Pk
Âµ = i=1 Î²i Î´Î±i for point distributions Î´, Î²i > 0, we can rewrite ÏÂµ using the above as:
( " k
#)
k
X
X Î²i
ÏÂµ (p) =
Î²i ESÎ±i (p) = inf Ep
(zi âˆ’ y)(1zi â‰¥y âˆ’ Î±i ) âˆ’ y
.
(4)
Î±
zâˆˆRk
i=1
i=1 i
They conclude elicI (ÏÂµ ) â‰¤ k + 1 unless Âµ({1}) = 1 in which case elicI (ÏÂµ ) = 1. We show how
to recover these results together with matching lower bounds. It is well-known that the infimum in
eq. (4) is attained by any of the k quantiles in qÎ±1 (p), . . . , qÎ±k (p), so we conclude elicI (ÏÂµ ) â‰¤ k + 1
by Theorem 1, and in particular the property {ÏÂµ , qÎ±1 , . . . , qÎ±k } is elicitable. The family of losses
from Corollary 1 coincide with the characterization of Fissler and Ziegel [15] (see Â§ D.1). For a
lower bound, as elicI ({qÎ±1 , . . . , qÎ±k }) = k whenever the Î±i are distinct by Lemma 3, Theorem 2
gives us elicI (ÏÂµ ) = k + 1 whenever Âµ({1}) < 1, and of course elicI (ÏÂµ ) = 1 if Âµ({1}) = 1.
4.5

Variantile

The Ï„ -expectile, a type of generalized quantile introduced by Newey and Powell [23], is defined as
the solution x = ÂµÏ„ to the equation Ep [|1xâ‰¥y âˆ’ Ï„ |(x âˆ’ y)] = 0. (This also shows ÂµÏ„ âˆˆ I1 .) Here
we propose the Ï„ -variantile, an asymmetric variance-like measure with respect to the Ï„ -expectile:
just as the mean is the solution x = Âµ to the equation Ep [xâˆ’ y] = 0, and the variance
is Ïƒ 2 (p) =

Ep [(Âµ âˆ’ y)2 ], we define the Ï„ -variantile ÏƒÏ„2 by ÏƒÏ„2 (p) = Ep |1ÂµÏ„ â‰¥y âˆ’ Ï„ |(ÂµÏ„ âˆ’ y)2 .
It is well-known that ÂµÏ„ can be expressed as the minimizer of a asymmetric least squares problem:
the loss L(x, y) = |1xâ‰¥y âˆ’ Ï„ |(x âˆ’ y)2 elicits ÂµÏ„ [23, 7]. Hence, just as the variance turned out to
be a Bayes risk for the mean, so is the Ï„ -variantile for the Ï„ -expectile:




ÂµÏ„ = argmin Ep |1xâ‰¥y âˆ’ Ï„ |(x âˆ’ y)2 =â‡’ ÏƒÏ„2 = min Ep |1xâ‰¥y âˆ’ Ï„ |(x âˆ’ y)2 .
xâˆˆR

xâˆˆR

We now see the pair
4.6

{ÂµÏ„ , ÏƒÏ„2 }

is elicitable by Corollary 1, and by Theorem 2 we have elicI (ÏƒÏ„2 ) = 2.

Deviation and Risk Measures

Rockafellar and Uryasev [21] introduce â€œrisk quadranglesâ€ in which they relate a risk R, deviation
D, error E, and a statistic S, all functions from random variables to the reals, as follows:
R(X) = min{C + E(X âˆ’ C)},
C

D(X) = min{E(X âˆ’ C)},
C

S(X) = argmin{E(X âˆ’ C)} .
C

Our results provide tight bounds for many of the risk and deviation measures in their paper. The most
immediate case is the expectation quadrangle case, where E(X) = E[e(X)] for some e : R â†’ R.
In this case, if S(X) âˆˆ I1 (P) Theorem 2 implies elicI (R) = elicI (D) = 2 provided S is nonconstant and e non-linear. This includes several of their examples, e.g. truncated mean, log-exp, and
rate-based. Beyond the expectation case, the authors show a Mixing Theorem, where they consider
)
( k
)
( k
X
X
X
0

D(X) = min min
Î»i Ei (X âˆ’ C âˆ’ Bi )
Î»i Bi = 0 = min
Î»i Ei (X âˆ’ Bi ) .
C B1 ,..,Bk

i=1

i

B10 ,..,Bk0

i=1

Once again, if the Ei are all of expectation type and Si âˆˆ I1 , Theorem 1 gives elicI (D) =
elicI (R) â‰¤ k + 1, with a matching lower bound from Theorem 2 provided the Si are all independent. The Reverting Theorem for a pair E1 , E2 can be seen as a special case of the above where
7

one replaces E2 (X) by E2 (âˆ’X). Consequently, we have tight bounds for the elicitation complexity of several other examples, including superquantiles (the same as spectral risk measures), the
quantile-radius quadrangle, and optimized certainty equivalents of Ben-Tal and Teboulle [24].
Our results offer an explaination for the existence of regression procedures for some of these
risk/deviation measures. For example, a proceedure called superquantile regression was introduced
in Rockafellar et al. [25], which computes spectral risk measures. In light of Theorem 1, one could
interpret their procedure as simply performing regression on the k different quantiles as well as the
Bayes risk. In fact, our results show that any risk/deviation generated by mixing several expectation
quadrangles will have a similar procedure, in which the Bi0 variables are simply computed along side
the measure of interest. Even more broadly, such regression procedures exist for any Bayes risk.

5

Discussion

We have outlined a theory of elicitation complexity which we believe is the right notion of complexity for ERM, and provided techniques and results for upper and lower bounds. In particular, we now
have tight bounds for the large class of Bayes risks, including several applications of note such as
spectral risk measures. Our results also offer an explanation for why procedures like superquantile
regression are possible, and extend this logic to all Bayes risks.
There many natural open problems in elicitation complexity. Perhaps the most apparent are the
characterizations of the complexity classes {Î“ : elic(Î“) = k}, and in particular, determining the
elicitation complexity of properties which are known to be non-elicitabile, such as the mode [19]
and smallest confidence interval [18].
In this paper we have focused on elicitation complexity with respect to the class of identifiable
properties I, which we denoted elicI . This choice of notation was deliberate; one may define
elicC := min{k : âˆƒÎ“Ì‚ âˆˆ Ek âˆ© C, âˆƒf, Î“ = f â—¦ Î“Ì‚} to be the complexity with respect to some arbitrary
class of properties C. Some examples of interest might be elicE for expected values, of interest to
the prediction market literature [8], and eliccvx for properties elicitable by a loss which is convex in
r, of interest for efficiently performing ERM.
Another interesting line of questioning follows from the notion of conditional elicitation, properties
which are elicitable as long as the value of some other elicitable property is known. This notion
was introduced by Emmer et al. [11], who showed that the variance and expected shortfall are both
conditionally elicitable, on Ep [y] and qÎ± (p) respectively. Intuitively, knowing that Î“ is elicitable
conditional on an elicitable Î“0 would suggest that perhaps the pair {Î“, Î“0 } is elicitable; Fissler and
Ziegel [15] note that it is an open question whether this joint elicitability holds in general. The Bayes
risk L for Î“ is elicitable conditioned on Î“, and as we saw above, the pair {Î“, L} is jointly elicitable
as well. We give a counter-example in Figure 1, however, which also illustrates the subtlety of
characterizing all elicitable properties.
p2

p2

p3

p3

p1

p1

Figure 1: Depictions of the level sets of two properties, one elicitable and the other not. The left is a Bayes risk
together with its property, and thus elicitable, while the right is shown in [3] not to be elicitable. Here the planes
are shown to illustrate the fact that these are both conditionally elicitable: the height of the plane (the intersept
(p3 , 0, 0) for example) is elicitable from the characterizations for scalar properties [9, 1], and conditioned on
the plane, the properties are both linear and thus links of expected values, which are also elicitable.

8

References
[1] Ingo Steinwart, Chlo Pasin, Robert Williamson, and Siyu Zhang. Elicitation and Identification of Properties. In Proceedings of The 27th Conference on Learning Theory, pages 482â€“526, 2014.
[2] A. Agarwal and S. Agrawal. On Consistent Surrogate Risk Minimization and Property Elicitation. In
COLT, 2015.
[3] Rafael Frongillo and Ian Kash. Vector-Valued Property Elicitation. In Proceedings of the 28th Conference
on Learning Theory, pages 1â€“18, 2015.
[4] L.J. Savage. Elicitation of personal probabilities and expectations. Journal of the American Statistical
Association, pages 783â€“801, 1971.
[5] Kent Harold Osband. Providing Incentives for Better Cost Forecasting. University of California, Berkeley,
1985.
[6] T. Gneiting and A.E. Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the
American Statistical Association, 102(477):359â€“378, 2007.
[7] T. Gneiting. Making and Evaluating Point Forecasts. Journal of the American Statistical Association,
106(494):746â€“762, 2011.
[8] J. Abernethy and R. Frongillo. A characterization of scoring rules for linear properties. In Proceedings of
the 25th Conference on Learning Theory, pages 1â€“27, 2012.
[9] N.S. Lambert. Elicitation and Evaluation of Statistical Forecasts. Preprint, 2011.
[10] N.S. Lambert and Y. Shoham. Eliciting truthful answers to multiple-choice questions. In Proceedings of
the 10th ACM conference on Electronic commerce, pages 109â€“118, 2009.
[11] Susanne Emmer, Marie Kratz, and Dirk Tasche. What is the best risk measure in practice? A comparison
of standard measures. arXiv:1312.1645 [q-fin], December 2013. arXiv: 1312.1645.
[12] Fabio Bellini and Valeria Bignozzi. Elicitable risk measures. This is a preprint of an article accepted for
publication in Quantitative Finance (doi 10.1080/14697688.2014. 946955), 2013.
[13] Johanna F. Ziegel. Coherence and elicitability. Mathematical Finance, 2014. arXiv: 1303.1690.
[14] Ruodu Wang and Johanna F. Ziegel. Elicitable distortion risk measures: A concise proof. Statistics &
Probability Letters, 100:172â€“175, May 2015.
[15] Tobias Fissler and Johanna F. Ziegel. Higher order elicitability and Osbandâ€™s principle. arXiv:1503.08123
[math, q-fin, stat], March 2015. arXiv: 1503.08123.
[16] A. Banerjee, X. Guo, and H. Wang. On the optimality of conditional expectation as a Bregman predictor.
IEEE Transactions on Information Theory, 51(7):2664â€“2669, July 2005.
[17] N.S. Lambert, D.M. Pennock, and Y. Shoham. Eliciting properties of probability distributions. In Proceedings of the 9th ACM Conference on Electronic Commerce, pages 129â€“138, 2008.
[18] Rafael Frongillo and Ian Kash. General truthfulness characterizations via convex analysis. In Web and
Internet Economics, pages 354â€“370. Springer, 2014.
[19] C. Heinrich. The mode functional is not elicitable. Biometrika, page ast048, 2013.
[20] Hans Fllmer and Stefan Weber. The Axiomatic Approach to Risk Measures for Capital Determination.
Annual Review of Financial Economics, 7(1), 2015.
[21] R. Tyrrell Rockafellar and Stan Uryasev. The fundamental risk quadrangle in risk management, optimization and statistical estimation. Surveys in Operations Research and Management Science, 18(1):33â€“53,
2013.
[22] Tobias Fissler, Johanna F. Ziegel, and Tilmann Gneiting. Expected Shortfall is jointly elicitable with
Value at Risk - Implications for backtesting. arXiv:1507.00244 [q-fin], July 2015. arXiv: 1507.00244.
[23] Whitney K. Newey and James L. Powell. Asymmetric least squares estimation and testing. Econometrica:
Journal of the Econometric Society, pages 819â€“847, 1987.
[24] Aharon Ben-Tal and Marc Teboulle. AN OLD-NEW CONCEPT OF CONVEX RISK MEASURES: THE
OPTIMIZED CERTAINTY EQUIVALENT. Mathematical Finance, 17(3):449â€“476, 2007.
[25] R. T. Rockafellar, J. O. Royset, and S. I. Miranda. Superquantile regression with applications to buffered
reliability, uncertainty quantification, and conditional value-at-risk. European Journal of Operational
Research, 234:140â€“154, 2014.

9

