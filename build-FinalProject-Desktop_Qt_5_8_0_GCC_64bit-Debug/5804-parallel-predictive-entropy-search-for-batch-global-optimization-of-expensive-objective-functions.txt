Parallel Predictive Entropy Search for Batch Global
Optimization of Expensive Objective Functions

Amar Shah
Department of Engineering
Cambridge University
as793@cam.ac.uk

Zoubin Ghahramani
Department of Engineering
University of Cambridge
zoubin@eng.cam.ac.uk

Abstract
We develop parallel predictive entropy search (PPES), a novel algorithm for
Bayesian optimization of expensive black-box objective functions. At each iteration, PPES aims to select a batch of points which will maximize the information
gain about the global maximizer of the objective. Well known strategies exist
for suggesting a single evaluation point based on previous observations, while far
fewer are known for selecting batches of points to evaluate in parallel. The few
batch selection schemes that have been studied all resort to greedy methods to
compute an optimal batch. To the best of our knowledge, PPES is the first nongreedy batch Bayesian optimization strategy. We demonstrate the benefit of this
approach in optimization performance on both synthetic and real world applications, including problems in machine learning, rocket science and robotics.

1

Introduction

Finding the global maximizer of a non-concave objective function based on sequential, noisy observations is a fundamental problem in various real world domains e.g. engineering design [1], finance
[2] and algorithm optimization [3]. We are interesed in objective functions which are unknown but
may be evaluated pointwise at some expense, be it computational, economical or other. The challenge is to find the maximizer of the expensive objective function in as few sequential queries as
possible, in order to minimize the total expense.
A Bayesian approach to this problem would probabilistically model the unknown objective function,
f . Based on posterior belief about f given evaluations of the the objective function, you can decide
where to evaluate f next in order to maximize a chosen utility function. Bayesian optimization [4]
has been successfully applied in a range of difficult, expensive global optimization tasks including
optimizing a robot controller to maximize gait speed [5] and discovering a chemical derivative of a
particular molecule which best treats a particular disease [6].
Two key choices need to be made when implementing a Bayesian optimization algorithm: (i) a
model choice for f and (ii) a strategy for deciding where to evaluate f next. A common approach
for modeling f is to use a Gaussian process prior [7], as it is highly flexible and amenable to analytic
calculations. However, other models have shown to be useful in some Bayesian optimization tasks
e.g. Student-t process priors [8] and deep neural networks [9]. Most research in the Bayesian
optimization literature considers the problem of deciding how to choose a single location where
f should be evaluated next. However, it is often possible to probe several points in parallel. For
example, you may possess 2 identical robots on which you can test different gait parameters in
parallel. Or your computer may have multiple cores on which you can run algorithms in parallel
with different hyperparameter settings.
Whilst there are many established strategies to select a single point to probe next e.g. expected
improvement, probability of improvement and upper confidence bound [10], there are few well
known strategies for selecting batches of points. To the best of our knowledge, every batch selection
1

strategy proposed in the literature involves a greedy algorithm, which chooses individual points
until the batch is filled. Greedy choice making can be severely detrimental, for example, a greedy
approach to the travelling salesman problem could potentially lead to the uniquely worst global
solution [11]. In this work, our key contribution is to provide what we believe is the first non-greedy
algorithm to choose a batch of points to probe next in the task of parallel global optimization.
Our approach is to choose a set of points which in expectation, maximally reduces our uncertainty
about the location of the maximizer of the objective function. The algorithm we develop, parallel
predictive entropy search, extends the methods of [12, 13] to multiple point batch selection. In
Section 2, we formalize the problem and discuss previous approaches before developing parallel
predictive entropy search in Section 3. Finally, we demonstrate the benefit of our non-greedy strategy
on synthetic as well as real-world objective functions in Section 4.

2

Problem Statement and Background

Our aim is to maximize an objective function f : X ‚Üí R, which is unknown but can be (noisily)
evaluated pointwise at multiple locations in parallel. In this work, we assume X is a compact subset
of RD . At each decision, we must select a set of Q points St = {xt,1 , ..., xt,Q } ‚äÇ X , where the
objective function would next be evaluated in parallel. Each evaluation leads to a scalar observation
yt,q = f (xt,q ) + t,q , where we assume t,q ‚àº N (0, œÉ 2 ) i.i.d. We wish to minimize a future regret,
rT = [f (x‚àó ) ‚àí f (xÃÉT )], where x‚àó ‚àà argmaxx‚ààX f (x) is an optimal decision (assumed to exist)
and xÃÉT is our guess of where the maximizer of f is after evaluating T batches of input points. It is
highly intractable to make decisions T steps ahead in the setting described, therefore it is common
to consider the regret of the very next decision. In this work, we shall assume f is a draw from a
Gaussian process with constant mean Œª ‚àà R and differentiable kernel function k : X 2 ‚Üí R.
Most Bayesian optimization research focuses on choosing a single point to query at each decision i.e. Q = 1. A popular strategy in this setting is to choose the point with highest
expected improvement over the current hbest evaluation, i.e. the imaximizer of aEI (x|D) =
 



E max(f (x) ‚àí f (xbest ), 0)D = œÉ(x) œÜ œÑ (x) + œÑ (x)Œ¶ œÑ (x) , where D is the set of obp
servations, xbest is the best evaluation point so far, œÉ(x) = Var[f (x)|D], ¬µ(x) = E[f (x)|D],
œÑ (x) = (¬µ(x) ‚àí f (xbest ))/œÉ(x) and œÜ(.) and Œ¶(.) are the standard Gaussian p.d.f. and c.d.f.
Aside from being an intuitive approach, a key advantage of using the expected improvement strategy
is in the fact that it is computable analytically and is infinitely differentiable, making the problem
of finding argmaxx‚ààX aEI (x|D) amenable to a plethora of gradient based optimization methods.
Unfortunately, the corresponding strategy for selecting Q > 1 points to evaluate in parallel does not
lead to an analytic expression. [14] considered an approach which sequentially used the EI criterion
to greedily choose a batch of points to query next, which [3] formalized and utilized by defining
Z



aEI‚àíMCMC x|D, {xq0 }qq0 =1 =
aEI x|D‚à™{xq0 , yq0 }qq0 =1 p {yq0 }qq0 =1 |D, {xq0 }qq0 =1 dy1 ..dyq ,
Xq

the expected gain in evaluating x after evaluating {xq0 , yq0 }qq0 =1 , which can be approximated using
Monte Carlo samples, hence the name EI-MCMC. Choosing a batch of points St using the EIMCMC policy is doubly greedy: (i) the EI criterion is greedy as it inherently aims to minimize onestep regret, rt , and (ii) the EI-MCMC approach starts with an empty set and populates it sequentially
(and hence greedily), deciding the best single point to include until |St | = Q.
A similar but different approach called simulated matching (SM) was introduced by [15]. Let œÄ be a
baseline policy which chooses a single point to evaluate next (e.g. EI). SM aims to select a batch St
of size Q, which includes a point ‚Äòclose to‚Äô the best point which œÄ would have chosen when applied
sequentially Q times, with high probability. Formally, SM aims to maximize

h h
ii

aSM (St |D) = ‚àíESœÄQ Ef min (x ‚àí argmaxx0 ‚ààSœÄQ f (x0 ))2 D, SœÄQ ,
x‚ààSt

where SœÄQ is the set of Q points which policy œÄ would query if employed sequentially. A greedy
k-medoids based algorithm is proposed to approximately maximize the objective, which the authors
justify by the submodularity of the objective function.
The upper confidence bound (UCB) strategy [16] is another method used by practitioners to decide
where to evaluate an objective function next. The UCB approach is to maximize aUCB (x|D) =
1/2
¬µ(x) + Œ±t œÉ(x), where Œ±t is a domain-specific time-varying positive parameter which trades off
2

exploration and exploitation. In order to extend this approach to the parallel setting, [17] noted that
the predictive variance of a Gaussian process depends only on where observations are made, and
not the observations themselves. Therefore, they suggested the GP-BUCB method, which greedily
populates the set St by maximizing a UCB type equation Q times sequentially, updating œÉ at each
step, whilst maintaining the same ¬µ for each batch. Finally, a variant of the GP-UCB was proposed
by [18]. The first point of the set St is chosen by optimizing the UCB objective. Thereafter, a
‚Äòrelevant region‚Äô Rt ‚äÇ X which contains the maximizer of f with high probability is defined.
Points are greedily chosen from this region to maximize the information gain about f , measured
by expected reduction in entropy, until |St | = Q. This method was named Gaussian process upper
confidence bound with pure exploration (GP-UCB-PE).
Each approach discussed resorts to a greedy batch selection process. To the best of our knowledge,
no batch Bayesian optimization method to date has avoided a greedy algorithm. We avoid a greedy
batch selection approach with PPES, which we develop in the next section.

3

Parallel Predictive Entropy Search

Our approach is to maximize information [19] about the location of the global maximizer x‚àó , which
we measure in terms of the negative differential entropy of p(x‚àó |D). Analogous to [13], PPES aims
to choose the set of Q points, St = {xq }Q
q=1 , which maximizes
h 


i

 H p x‚àó |D ‚à™ {xq , yq }Q
aPPES (St |D) = H p(x‚àó |D) ‚àí E
,
(1)
Q 
q=1
p {yq }q=1 D,St
R
where H[p(x)] = ‚àí p(x) log p(x)dx is the differential entropy of its argument and the expectation
above is taken with respect to the posterior joint predictive distribution of {yq }Q
q=1 given the previous
evaluations, D, and the set St . Evaluating
(1)
exactly
is
typically
infeasible.
The
prohibitive aspects

are that p x‚àó |D ‚à™ {xq , yq }Q
would
have
to
be
evaluated
for
many
different
combinations of
q=1
Q
{xq , yq }q=1 , and the entropy computations are not analytically tractable in themselves. Significant
approximations need to be made to (1) before it becomes practically useful [12]. A convenient
equivalent formulation of the quantity in (1) can be written as the mutual information between x‚àó
and {yq }Q
q=1 given D [20]. By symmetry of the mutual information, we can rewrite aPPES as
h 


i
Q
‚àó
‚àó
aPPES (St |D) = H p {yq }Q
H
p
{y
}
|D,
S
‚àí
E
|D,
S
,
x
,
(2)
q q=1
t
t
p(x |D)
q=1

‚àó
where p {yq }Q
is the joint posterior predictive distibution for {yq }Q
q=1 |D, St , x
q=1 given the observed data, D and the location of the global maximizer of f . The key advantage of the formulation
in (2), is that the objective is based on entropies of predictive distributions of the observations, which
are much more easily approximated than the entropies of distributions on x‚àó .

In fact, the first term of (2) can be computed analytically. Suppose p {fq }Q
q=1 |D, St is multi

variate Gaussian with covariance K, then H p {yq }Q
= 0.5 log[det(2œÄe(K + œÉ 2 I))].
q=1 |D, St
We develop an approach to approximate the expectation of the predictive entropy in (2), using an
expectation propagation based method which we discuss in the following section.
3.1

Approximating the Predictive Entropy



‚àó
in
Assuming a sample of x‚àó , we discuss our approach to approximating H p {yq }Q
q=1 |D, St , x
(2) for a set of query points St . Note that we can write
Z
Q

Y
Q
Q
‚àó
‚àó
p {yq }q=1 |D, St , x = p {fq }q=1 |D, St , x
p(yq |fq ) df1 ...dfQ ,
(3)
q=1


‚àó
is the posterior distribution of the objective function at the locations
where p {fq }Q
q=1 |D, St , x
xq ‚àà St , given previous evaluations D, and that x‚àó is the global maximizer of f . Recall that
p(yq |fq ) is Gaussian for each q. Our approach will be to derive a Gaussian approximation to

‚àó
p {fq }Q
q=1 |D, St , x , which would lead to an analytic approximation to the integral in (3).

The posterior predictive distribution of the Gaussian process, p {fq }Q
q=1 |D, St , is multivariate
Gaussian distributed. However, by further conditioning on the location x‚àó , the global maximizer
of f , we impose the condition that f (x) ‚â§ f (x? ) for any x ‚àà X . Imposing this constraint for
3


‚àó
highly inall x ‚àà X is extremely difficult and makes the computation of p {fq }Q
q=1 |D, St , x
?
tractable. We instead impose the following two conditions (i) f (x) ‚â§ f (x ) for each x ‚àà St ,
and (ii) f (x? ) ‚â• ymax + , where ymax is the largest observed noisy objective function value and
 ‚àº N (0, œÉ 2 ). Constraint (i) is equivalent to imposing that f (x? ) is larger than objective function
values at current query locations, whilst condition (ii) makes f (x? ) larger than previous objective function evaluations, accounting for noise. Denoting the two conditions C, and the variables
f = [f1 , ..., fQ ]> and f + = [f ; f ? ], where f ? = f (x‚àó ), we incorporate the conditions as follows
Z
Q

  f ? ‚àí ymax  Y
p f |D, St , x‚àó ‚âà p f + |D, St , x‚àó Œ¶
I(f ? ‚â• fq ) df ? ,
(4)
œÉ
q=1
where I(.) is an indicator function. The integral in (4) can be approximated using expectation propagation [21]. The Gaussian process predictive p(f + |D, St , x‚àó ) is N (f + ; m+ , K+ ). We approximate
QQ+1
the integrand of (4) with w(f + ) = N (f + ; m+ , K+ ) q=1 ZÃÉq N (c>
q f + ; ¬µÃÉq , œÑÃÉq ), where each ZÃÉq
and œÑÃÉq are positive, ¬µÃÉq ‚àà R and for q ‚â§ Q, cq is a vector of length Q + 1 with q th entry ‚àí1, Q + 1st
entry 1, and remaining entries 0, whilst cQ+1 = [0, ..., 0, 1]> . The approximation w(f + ) approximates the Gaussian CDF, Œ¶(.), and each indicator function, I(.), with a univariate, scaled Gaussian
PDF. The site parameters, {ZÃÉq , ¬µÃÉq , œÑÃÉq }Q+1
q=1 , are learned using a fast EP algorithm, for which details
are given in the supplementary material, where we show that w(f + ) = ZN (f + ; ¬µ+ , Œ£+ ), where

‚àí1

‚àí1
Q+1
Q+1
X ¬µÃÉq
X 1
‚àí1
>
>
¬µ+ = Œ£+ K‚àí1
m
+
c
c
c
c
,
Œ£
=
K
+
,
(5)
+
q q
q q
+
+
+
œÑÃÉ
œÑÃÉ
q=1 q
q=1 q

and hence p f + |D, St , C ‚âà N (f + ; ¬µ+ , Œ£+ ). Since multivariate
 Gaussians are consistent under marginalization, a convenient corollary is that p f |D, St , x‚àó ‚âà N (f ; ¬µ, Œ£), where ¬µ is the
vector containing the first Q elements of ¬µ+ , and Œ£ is the matrix containing the first Q rows and
columns of Œ£+ . Since
Gaussians are also Gaussian distributed, we see that
 sums of independent
Q
‚àó
>
p {yq }q=1 |D, St , x ‚âà N ([y1 , ..., yQ ] ; ¬µ, Œ£ + œÉ 2 I). The final convenient attribute of our Gaussian approximation, is that
of a multivariate Gaussian can be computed
 the differential entropy

‚àó
analytically, such that H p {yq }Q
‚âà 0.5 log[det(2œÄe(Œ£ + œÉ 2 I))].
q=1 |D, St , x
3.2

Sampling from the Posterior over the Global Maximizer


‚àó
So far, we have considered how to approximate H p {yq }Q
, given the global maxiq=1 |D, St , x
mizer, x‚àó . We in fact would like the expected value of this quantity over the posterior distribution of
the global maximizer, p(x‚àó |D). Literally, p(x‚àó |D) ‚â° p(f (x‚àó ) = maxx‚ààX f (x)|D), the posterior
probability that x‚àó is the global maximizer of f . Computing the distribution p(x‚àó |D) is intractable,
but it is possible to approximately sample from it and compute a Monte Carlo based approximation
of the desired expectation. We consider two approaches to sampling from the posterior of the global
maximizer: (i) a maximum a posteriori (MAP) method, and (ii) a random feaure approach.
MAP sample from p(x‚àó |D). The MAP of p(x‚àó |D) is its posterior mode, given by x‚àóMAP =
argmaxx‚àó ‚ààX p(x‚àó |D). We may approximate the expected value of the predictive entropy by replacing the posterior distribution of x‚àó with a single point estimate at x‚àóMAP . There are two key
advantages to using the MAP estimate in this way. Firstly, it is simple to compute x‚àóMAP , as it is
the global maximizer of the posterior mean of f given the observations D. Secondly, choosing to
use x‚àóMAP assists the EP algorithm developed in Section 3.1 to converge as desired. This is because
the condition f (x‚àó ) ‚â• f (x) for x ‚àà X is easy to enforce when x‚àó = x‚àóMAP , the global maximizer
of the poserior mean of f . When x‚àó is sampled such that the posterior mean at x‚àó is significantly
suboptimal, the EP approximation may be poor. Whilst using the MAP estimate approximation is
convenient, it is after all a point estimate and fails to characterize the full posterior distribution. We
therefore consider a method to draw samples from p(x‚àó |D) using random features.
Random Feature Samples from p(x‚àó |D). A naive approach to sampling from p(x‚àó |D) would
be to sample g ‚àº p(f |D), and choosing argmaxx‚ààX g. Unfortunately, this would require sampling
g over an uncountably infinite space, which is infeasible. A slightly less naive method would be to
sequentially construct g, whilst optimizing it, instead of evaluating it everywhere in X . However,
this approach would have cost O(m3 ) where m is the number of function evaluations of g necessary
to find its optimum. We propose as in [13], to sample and optimize an analytic approximation to g.
4

By Bochner‚Äôs theorem [22], a stationary kernel function, k, has a Fourier dual s(w), which is equal
to the spectral density of k. Setting p(w) = s(w)/Œ±, a normalized density, we can write
>

0

k(x, x0 ) = Œ±Ep(w) [e‚àíiw (x‚àíx ) ] = 2Œ±Ep(w,b) [cos(w> x + b) cos(w> x0 + b)],
(6)
p
where b ‚àº U [0, 2œÄ]. Let œÜ(x) = 2Œ±/m cos(Wx + b) denote an m-dimensional feature mapping
where W and b consist of m stacked samples from p(w, b), then the kernel k can be approximated
by the inner product of these features, k(x, x0 ) ‚âà œÜ(x)> œÜ(x0 ) [23]. The linear model g(x) =
œÜ(x)> Œ∏ + Œª where Œ∏|D ‚àº N (A‚àí1 œÜ> (y ‚àí Œª1), œÉ 2 A‚àí1 ) is an approximate sample from p(f |D),
where y is a vector of objective function evaluations, A = œÜ> œÜ + œÉ 2 I and œÜ> = [œÜ(x1 )...œÜ(xn )].
In fact, limm‚Üí‚àû g is a true sample from p(f |D) [24].
The generative process above suggests the following approach to approximately sampling from
p(x‚àó |D): (i) sample random features œÜ(i) and corresponding posterior weights Œ∏ (i) using the
process above, (ii) construct g (i) (x) = œÜ(i) (x)> Œ∏ (i) + Œª, and (iii) finally compute x?(i) =
argmaxx‚ààX g (i) (x) using gradient based methods.
3.3

Computing and Optimizing the PPES Approximation

Let œà denote the set of kernel parameters and the observation noise variance, œÉ 2 . Our posterior
belief about œà is summarized by the posterior distribution p(œà|D) ‚àù p(œà)p(D|œà), where p(œà) is
our prior belief about œà and p(D|œà) is the GP marginal likelihood given the parameters œà. For a
fully Bayesian treatment of œà, we must marginalize aPPES with respect to p(œà|D). The expectation
with respect to the posterior distribution of œà is approximated with Monte Carlo samples. A similar
approach is taken in [3, 13]. Combining the EP based method to approximate the predictive entropy
with either of the two methods discussed in the previous section to approximately sample from
p(x‚àó |D), we can construct aÃÇPPES an approximation to (2), defined by
M
i
1 Xh
aÃÇPPES (St |D) =
log[det(K(i) + œÉ 2(i) I)] ‚àí log[det(Œ£(i) + œÉ 2(i) I)] ,
(7)
2M i=1
where K(i) is constructed using œà (i) the ith sample of M from p(œà|D), Œ£(i) is constructed as in
Section 3.1, assuming the global maximizer is x‚àó(i) ‚àº p(x‚àó |D, œà (i) ). The PPES approximation
is simple and amenable to gradient based optimization. Our goal is to choose St = {x1 , ..., xQ }
which maximizes aÃÇPPES in (7). Since our kernel function is differentiable, we may consider taking
the derivative of aÃÇPPES with respect to xq,d , the dth component of xq ,

M 
h
h
‚àÇ aÃÇPPES
‚àÇK(i) i
‚àÇŒ£(i) i
1 X
trace (K(i) + œÉ 2(i) I)‚àí1
‚àí trace (Œ£(i) + œÉ 2(i) I)‚àí1
=
. (8)
‚àÇ xq,d
2M i=1
‚àÇxq,d
‚àÇxq,d
Computing

‚àÇK(i)
‚àÇxq,d
(i)

is simple directly from the definition of the chosen kernel function. Œ£(i) is a
(i)

‚àÇK(i)
‚àÇxq,d , and that each cq
(i)
parameters, {œÉÃÉq }Q+1
q=1 , vary with

Q+1
function of K , {cq }Q+1
q=1 and {œÉÃÉq }q=1 , and we know how to compute

is a constant vector. Hence our only concern is how the EP site
xq,d . Rather remarkably, we may invoke a result from Section 2.1 of [25], which says that converged
‚àó
site parameters, {ZÃÉq , ¬µÃÉq , œÉÃÉq }Q+1
q=1 , have 0 derivative with respect to parameters of p(f + |D, St , x ).
There is a key distinction between explicit dependencies (where Œ£ actually depends on K) and
implicit dependencies where a site parameter, œÉÃÉq , might depend implicitly on K. A similar approach
is taken in [26], and discussed in [7]. We therefore compute
(i)

(i)

‚àÇŒ£+
(i) (i)‚àí1 ‚àÇK+
(i)‚àí1 (i)
= Œ£+ K+
K
Œ£+ .
(9)
‚àÇxq,d
‚àÇxq,d +
On first inspection, it may seem computationally too expensive to compute derivatives with respect
(i)‚àí1 (i)
to each q and d. However, note that we may compute and store the matrices K+ Œ£+ , (K(i) +
(i)

‚àÇK

+
œÉ 2(i) I)‚àí1 and (Œ£(i) + œÉ 2(i) I)‚àí1 once, and that ‚àÇxq,d
is symmetric with exactly one non-zero row
and non-zero column, which can be exploited for fast matrix multiplication and trace computations.

5

0.6
0.6

0.5
0.6

0.5

0.4

0.4
0.4

0.4

0.3

0.3

0.2

0.2

0.2

0.2
0.1

0.1
0

0

0.2

0.4

0.6

0.8

0

0

1

0

0.2

0.4

0.6

0.8

0

1

1

1

2

0.8

0.7

1.5
0.8

0.6

0.7

0.8

1

0.6
0.5
0.6

0.5

0.6

0.5

0.4
0.4

0

0.4

0.3

‚àí0.5

0.3

0.2

0.2

0.2

0.2

‚àí1
‚àí1.5

0.4

0.1

0

0.2

0.4

0.6

0.8

(a) Synthetic function

1

0

0

0.2

1

0.4

0.6

0.8

1

0

0.1
0

0

0.2

2

(b) aPPES (x, x0 )

0.4

0.6

0.8

0

1

(c) aÃÇPPES (x, x0 )

0.8
1.5

0.7
Figure 1: Assessing the quality1 0.8of our approximations to the parallel
predictive entropy search strat1
0.6
egy. (a) Synthetic objective function (blue line) defined on [0,
1], with noisy observations (black
0.6
0.5
squares). (b) Ground truth aPPES
defined on [0, 1]2 , obtained0.5by rejection
sampling. (c) Our ap0.4
proximation aÃÇPPES using expectation propagation. Dark regions
correspond
to pairs (x, x0 ) with
0
0.4
0
high utility, whilst faint regions correspond to pairs (x, x ) with0.3 low utility.

4

Empirical Study

0.2

0.2

0.1
0

0

0.2

0.4

0.6

0.8

1

0

‚àí0.5
‚àí1
‚àí1.5

0

0.2

0.4

0.6

0.8

1

In this section, we study the performance
of PPES in comparison to aforementioned methods. We
2
model f as a Gaussian process with constant mean Œª and covariance kernel k. Observations of the
1.5
objective function are considered to be independently drawn from N (f (x), œÉ 2 ). In our experiments,
1
P
0
2
1
we choose
to
use
a
squared-exponential
kernel
of
the
form
k(x,
x
)
=
Œ≥
exp
‚àí
0.5
(x
‚àí
d
d

0 2 2
0.5
xd ) /ld . Therefore the set of model hyperparameters is {Œª, Œ≥, l1 , ..., lD , œÉ}, a broad Gaussian
hyperprior is placed on Œª and uninformative
Gamma priors are used for the other hyperparameters.
0
‚àí0.5
It is worth investigating how well
aÃÇPPES (7) is able to approximate aPPES (2). In order to test
the approximation in a manner amenable
to visualization, we generate a sample f from a Gaussian
‚àí1
process prior on X = [0, 1], with Œ≥ 2 = 1, œÉ 2 = 10‚àí4 and l2 = 0.025, and consider batches of size
Q = 2. We set M = 200. A‚àí1.5
rejection
sampling
approach
is used to compute the ground
0
0.2
0.4
0.6 based
0.8
1
truth aPPES , defined on X Q = [0, 1]2 . We first discretize [0, 1]2 , and sample p(x‚àó |D) in (2) by
evaluating samples from p(f |D) on the discrete points and
 choosing the input with highest function
1
value. Given x‚àó , we compute H p y1 , y2 |D, x1 , x2 , x‚àó using
rejection sampling. Samples from
2
p(f |D) are evaluted on discrete points in [0, 1] and rejected if the highest function value occurs not
2
at x‚àó . We add independent Gaussian
noise with variance

œÉ to the non rejected samples from the
previous step and approximate H p y1 , y2 |D, x1 , x2 , x‚àó using kernel density estimation [27].

Figure 1 includes illustrations of (a) the objective function to be maximized, f , with 5 noisy observations, (b) the aPPES ground truth obtained using the rejection sampling method and finally (c)
aÃÇPPES using the EP method we develop in the previous section. The black squares on the axes of
Figures 1(b) and 1(c) represent the locations in X = [0, 1] where f has been noisily sampled, and
the darker the shade, the larger the function value. The lightly shaded horizontal and vertical lines
in these figures along the points The figures representing aPPES and aÃÇPPES appear to be symmetric, as is expected, since the set St = {x, x0 } is not an ordered set, since all points in the set are
probed in parallel i.e. St = {x, x0 } = {x0 , x}. The surface of aÃÇPPES is similar to that of aPPES .
In paticular, the aÃÇPPES approximation often appeared to be an annealed version of the ground truth
aPPES , in the sense that peaks were more pronounced, and non-peak areas were flatter. Since we are
interested in argmax{x,x0 }‚ààX 2 aPPES ({x, x0 }), our key concern is that the peaks of aÃÇPPES occur at
the same input locations as aPPES . This appears to be the case in our experiment, suggesting that
the argmax aÃÇPPES is a good approximation for argmax aPPES .
We now test the performance of PPES in the task of finding the optimum of various objective functions. For each experiment, we compare PPES (M = 200) to EI-MCMC (with 100 MCMC samples), simulated matching with a UCB baseline policy, GP-BUCB and GP-UCB-PE. We use the random features method to sample from p(x‚àó |D), rejecting samples which lead to failed EP runs. An
experiment of an objective function, f , consists of sampling 5 input points uniformly at random and
running each algorithm starting with these samples and their corresponding (noisy) function values.
We measure performance after t batch evaluations using immediate regret, rt = |f (xÃÉt ) ‚àí f (x‚àó )|,
where x‚àó is the known optimizer of f and xÃÉt is the recommendation of an algorithm after t batch
evaluations. We perform 100 experiments for each objective function, and report the median of the
6

regret
regret

regret
regret

6 6
0.4 0.4

4 4

0.3 0.3
0.2 0.2

2 2
0.1 0.1
0 0
0 0

8

4

4

2

0

7
6

7 7

0.7

6 6

0.6

0.6

0.5

0.5

0.4

3 3

2.5 2.5

5 5

2 2

4 4

0.4

1.5 1.5

3 3

0.3

0.3

0.2

0.2

0.1

0.1

1 1
2 2

0

05

510

1015 1520
t
t

2025

0

25

(a) Branin

3

3

2.5

2.5

0

0

0.5 0.5

1 1

05

510

1015 1520
t
t

2025

0 0
025 0

10 10
t

(b) Cosines

20 20
t

(c) Shekel

30 30

0 0
0 0 10 10 20 20 30 30 40 40 50 50
t t

(d) Hartmann

regret

4

regret

Figure 2: Median of the immediate regret of the PPES and 4 other algorithms over 100 experiments
on5 benchmark synthetic objective
functions, using batches of size Q = 3.
2
2
regret

regret

0

6

4

0.8

0.7

2

7

5

regret

6

regret

regret

6

0.8

5 5 10 10 15 15 20 20 25 25
t t

regret
regret

8

PPESPPES
EI-MCMC
EI-MCMC
SMUCB
SMUCB
BUCB
BUCB
UCBPE
UCBPE

regret
regret

10

regret

10

0 0
0 0

5 5 10 10 15 15 20 20 25 25
t t

1.5
immediate
regret obtained1.5 for
each algorithm. The confidence bands represent one standard devia3
3
tion obtained from bootstrapping.
The empirical distribution of the immediate regret is heavy tailed,
1
1
2
2 making
the median more representative of where most data points lie than the mean.

0.5 0.5
1 first set of experiments
Our
is on a set of synthetic benchmark objective functions including BraninHoo
[28],
a
mixture
of
cosines
[29], a Shekel function with 10 modes [30] (each defined on [0, 1]2 )
0
0
0
0
0 10 10 20 20 30 30
010 1020 2030 3040 4050 50
0
0
and the Hartmann-6
function [28] (defined
on [0, 1]6 ). We choose batches of size Q = 3 at each
t
t
t
t
decision time. The plots in Figure 2 illustrate the median immediate regrets found for each algo1
rithm. The results suggest that the PPES algorithm performs close to best if not 1the
best for each
problem considered. EI-MCMC does significantly better on the Hartmann function, which is a relatively smooth function with very few modes, where greedy search appears beneficial. Entropy-based
strategies are more exploratory in higher dimensions. Nevertheless, PPES does significantly better
than GP-UCB-PE on 3 of the 4 problems, suggesting that our non-greedy batch selection procedure
enhances performance versus a greedy entropy based policy.
1

We now consider maximization of real world objective functions. The first, boston, returns the
negative of the prediction error of a neural network trained on a random train/text split of the Boston
Housing dataset [31]. The weight-decay parameter and number of training iterations for the neural
network are the parameters to be optimized over. The next function, hydrogen, returns the amount
of hydrogen produced by particular bacteria as a function of pH and nitrogen levels of a growth
medium [32]. Thirdly we consider
a function, rocket, which runs a simulation of a rocket [33]
1
1
being launched from the Earth‚Äôs surface and returns the time taken for the rocket to land on the
Earth‚Äôs surface. The variables to be optimized over are the launch height from the surface, the mass
of fuel to use and the angle of launch with respect to the Earth‚Äôs surface. If the rocket does not
return, the function returns 0. Finally we consider a function, robot, which returns the walking
speed of a bipedal robot [34]. The function‚Äôs input parameters, which live in [0, 1]8 , are the robot‚Äôs
controller. We add Gaussian noise with œÉ = 0.1 to the noiseless function. Note that all of the
functions we consider are not available analytically. boston trains a neural network and returns
test error, whilst rocket and robot run physical simulations involving differential equations
before returning a desired quantity. Since the hydrogen dataset is available only for discrete points,
we define hydrogen to return the predictive mean of a Gaussian process trained on the dataset.
Figure 3 show the median values of immediate regret by each method over 200 random initializations. We consider batches of size Q = 2 and Q = 4. We find that PPES consistently outperforms
competing methods on the functions considered. The greediness and nonrequirement of MCMC
sampling of the SM-UCB, GP-BUCB and GP-UCB-PE algorithms make them amenable to large
batch experiments, for example, [17] consider optimization in R45 with batches of size 10. However,
these three algorithms all perform poorly when selecting batches of smaller size. The performance
on the hydrogen function illustrates an interesting phenemona; whilst the immediate regret of
PPES is mediocre initially, it drops rapidly as more batches are evaluated.
This behaviour is likely due to the non-greediness of the approach we have taken. EI-MCMC makes
good initial progress, but then fails to explore the input space as well as PPES is able to. Recall that
after each batch evaluation, an algorithm is required to output xÃÉt , its best estimate for the maximizer
of the objective function. We observed that whilst competing algorithms tended to evaluate points
which had high objective function values compared to PPES, yet when it came to recommending xÃÉt ,
7

¬∑10‚àí2¬∑10‚àí2

1

0

1
0

5

5

0
0 ¬∑10‚àí2
0 ¬∑105‚àí2 5

1
1

1
1

0.5
0.5

0.5
0.5

6
4

4
2

4
2

0

0
0

2
0
0

4

4

3

3

12 10
12
10

3

regretregret

regretregret

2

2

2

2

1

1

8
6

0

0

5

5

10 10 15 15 20 20
t
t
10 10 15 15 20 20
t
t

(a) boston

2
2

1.5
1.5

1.5
1.5

4
2

0.5
0.5

0.5
0.5

2
0

2
0

5

2
2

1
1

0

5

2.5
2.5

1
1

1
0

2.5
2.5

6
4

0

0

8
6

0

0
0

0

0
0

0
0
10 10 20 20 30 30 400 040 0 0 10
0 10
0
t
t
10 10 20 20 30 30 40 40
t
t

(b) hydrogen

10 20
10 20
t
t

20 30
20 30
t
t

30 40
30 40

40
40

3
3

6
4

1
0

3
3

10
8 108

4
2

0

0

10 10 20 20 30 30 40 40
0
0
t
t
0 0 10
00
10 10 20 20 30 30 40 040 0 10
t
t

14 14
14
12 14
12

3

0

10 20
10 20
t
t

20 30
20 30
t
t

30 40
30 40

(c) rocket

40
40

4
4
3.5
3.5

3
3

3
3

2.5
2.5

2.5
2.5

2
2

2
2

1.5
1.5

1.5
1.5

regret
regret

1.5
1.5

8
6

6
4

2
0

10 10 15 15 20 20
t
t
10 10 15 15 20 20
t
t

¬∑10‚àí2¬∑10‚àí2
4

4

regretregret

0

0

1.5
1.5

8
6

4
4
3.5
3.5

1
1

1
1

0.5
0.5

0.5
0.5

0
00
0

0
0 0 10
0 10

4
4

4
4

3.5
3.5

3.5
3.5

3
3

3
3

2.5
2.5

2.5
2.5

regret
regret

1

2
2

regret
regret

1

2
2

regret
regret

2

10
8 108

regret
regret

2

2

2.5
2.5

regret
regret

2

2.5
2.5

regret
regret

3

3
3

12
10 12
10

regret
regret

3

regretregret

3

3
3

14
14
12 12

regretregret

3

14 14

regretregret

regretregret

4

regretregret

4

0

PPES
PPES
EI-MCMC
EI-MCMC
PPES
PPES
SMUCB
SMUCB
EI-MCMC
EI-MCMC
BUCB
BUCB
SMUCB
SMUCB
UCBPE
UCBPE
BUCB
BUCB
UCBPE
UCBPE

¬∑10‚àí2¬∑10‚àí2
4

4

2
2

2
2

1.5
1.5

1.5
1.5

1
1

1
1

0.5
0.5

0.5
0.5

0
00
0

0
0 0 10
0 10

10 20
10 20
t
t

20 30
20 30
t
t

30 40
30 40

40
40

10 20
10 20
t
t

20 30
20 30
t
t

30 40
30 40

40
40

(d) robot

Figure 3: Median of the immediate regret of the PPES and 4 other algorithms over 100 experiments
on real world objective functions. Figures in the top row use batches of size Q = 2, whilst figues on
the bottom row use batches of size Q = 4.
PPES tended to do a better job. Our belief is that this occured exactly because the PPES objective
aims to maximize information gain rather than objective function value improvement.
The rocket function has a strong discontinuity making if difficult to maximize. If the fuel mass,
launch height and/or angle are too high, the rocket would not return to the Earth‚Äôs surface, resulting
in a 0 function value. It can be argued that a stationary kernel Gaussian process is a poor model for
this function, yet it is worth investigating the performance of a GP based models since a practitioner
1 black-box function is smooth apriori. PPES seemed to handle this
may not know whether or not 1their
function best and had fewer samples
which resulted in 0 function value than each
1 1
1 of the competing
1
1
methods and made fewer recommendations which led to a 0 function value. 1The relative increase
in PPES performance from increasing batch size from Q = 2 to Q = 4 is small for the robot
function compared to the other functions considered. We believe this is a consequence of using a
slightly naive optimization procedure to save computation time. Our optimization procedure first
computes aÃÇPPES at 1000 points selected uniformly at random, and performs gradient ascent from
the best point. Since aÃÇPPES is defined on X Q = [0, 1]32 , this method may miss a global optimum.
Other methods all select their batches greedily, and hence only need to optimize in X = [0, 1]8 .
However, this should easily be avoided by using a more exhaustive gradient based optimizer.

5

Conclusions

We have developed parallel predictive entropy search, an information theoretic approach to batch
Bayesian optimization. Our method is greedy in the sense that it aims to maximize the one-step
information gain about the location of x‚àó , but it is not greedy in how it selects a set of points to
evaluate next. Previous methods are doubly greedy, in that they look one step ahead, and also select
a batch of points greedily. Competing methods are prone to under exploring, which hurts their
perfomance on multi-modal, noisy objective functions, as we demonstrate in our experiments.

References
[1] G. Wang and S. Shan. Review of Metamodeling Techniques in Support of Engineering Design Optimization. Journal of Mechanical Design, 129(4):370‚Äì380, 2007.
[2] W. Ziemba & R. Vickson. Stochastic Optimization Models in Finance. World Scientific Singapore, 2006.

8

[3] J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian Optimization of Machine Learning Algorithms. NIPS, 2012.
[4] J. Mockus. Bayesian Approach to Global Optimization: Theory and Applications. Kluwer, 1989.
[5] D. Lizotte, T. Wang, M. Bowling, and D. Schuurmans. Automatic Gait Optimization with Gaussian
Process Regression. IJCAI, pages 944‚Äì949, 2007.
[6] D. M. Negoescu, P. I. Frazier, and W. B. Powell. The Knowledge-Gradient Algorithm for Sequencing
Experiments in Drug Discovery. INFORMS Journal on Computing, 23(3):346‚Äì363, 2011.
[7] Carl Rasmussen and Chris Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.
[8] A. Shah, A. G. Wilson, and Z. Ghahramani. Student-t Processes as Alternatives to Gaussian Processes.
AISTATS, 2014.
[9] J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. Patwary, Mr Prabat, and R. P.
Adams. Scalable Bayesian Optimization Using Deep Neural Networks. ICML, 2015.
[10] E. Brochu, M. Cora, and N. de Freitas. A Tutorial on Bayesian Optimization of Expensive Cost Functions,
with Applications to Active User Modeling and Hierarchical Reinforcement Learning. Technical Report
TR-2009-23, University of British Columbia, 2009.
[11] G. Gutin, A. Yeo, and A. Zverovich. Traveling salesman should not be greedy:domination analysis of
greedy-type heuristics for the TSP. Discrete Applied Mathematics, 117:81‚Äì86, 2002.
[12] P. Hennig and C. J. Schuler. Entropy Search for Information-Efficient Global Optimization. JMLR, 2012.
[13] J. M. HernaÃÅndez-Lobato, M. W. Hoffman, and Z. Ghahramani. Predictive Entropy Search for Efficient
Global Optimization of Black-box Functions. NIPS, 2014.
[14] D. Ginsbourger, J. Janusevskis, and R. Le Riche. Dealing with Asynchronicity in Parallel Gaussian
Process Based Optimization. 2011.
[15] J. Azimi, A. Fern, and X. Z. Fern. Batch Bayesian Optimization via Simulation Matching. NIPS, 2010.
[16] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian Process Optimization in the Bandit Setting:
No Regret and Experimental Design. ICML, 2010.
[17] T. Desautels, A. Krause, and J. Burdick. Parallelizing Exploration-Exploitation Tradeoffs with Gaussian
Process Bandit Optimization. ICML, 2012.
[18] E. Contal, D. Buffoni, D. Robicquet, and N. Vayatis. Parallel Gaussian Process Optimization with Upper
Confidence Bound and Pure Exploration. In Machine Learning and Knowledge Discovery in Databases,
pages 225‚Äì240. Springer Berlin Heidelberg, 2013.
[19] D. J. MacKay. Information-Based Objective Functions for Active Data Selection. Neural Computation,
4(4):590‚Äì604, 1992.
[20] N. Houlsby, J. M. HernaÃÅndez-Lobato, F. Huszar, and Z. Ghahramani. Collaborative Gaussian Processes
for Preference Learning. NIPS, 2012.
[21] T. P. Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis, Masachusetts
Institute of Technology, 2001.
[22] S. Bochner. Lectures on Fourier Integrals. Princeton University Press, 1959.
[23] A. Rahimi and B. Recht. Random Features for Large-Scale Kernel Machines. NIPS, 2007.
[24] R. M. Neal. Bayesian Learning for Neural Networks. PhD thesis, University of Toronto, 1995.
[25] M. Seeger. Expectation Propagation for Exponential Families. Technical Report, U.C. Berkeley, 2008.
[26] J. P. Cunningham, P. Hennig, and S. Lacoste-Julien. Gaussian Probabilities and Expectation Propagation.
arXiv, 2013. http://arxiv.org/abs/1111.6832.
[27] I. Ahmad and P. E. Lin. A Nonparametric Estimation of the Entropy for Absolutely Continuous Distributions. IEEE Trans. on Information Theory, 22(3):372‚Äì375, 1976.
[28] D. Lizotte. Practical Bayesian Optimization. PhD thesis, University of Alberta, 2008.
[29] B. S. Anderson, A. W. Moore, and D. Cohn. A Nonparametric Approach to Noisy and Costly Optimization. ICML, 2000.
[30] J. Shekel. Test Functions for Multimodal Search Techniques. Information Science and Systems, 1971.
[31] K. Bache and M. Lichman. UCI Machine Learning Repository, 2013.
[32] E. H. Burrows, W. K. Wong, X. Fern, F.W.R. Chaplen, and R.L. Ely. Optimization of ph and nitrogen
for enhanced hydrogen production by synechocystis sp. pcc 6803 via statistical and machine learning
methods. Biotechnology Progress, 25(4):1009‚Äì1017, 2009.
[33] J. E. Hasbun. In Classical Mechanics with MATLAB Applications. Jones & Bartlett Learning, 2008.
[34] E. Westervelt and J. Grizzle. Feedback Control of Dynamic Bipedal Robot Locomotion. Control and
Automation Series. CRC PressINC, 2007.

9

