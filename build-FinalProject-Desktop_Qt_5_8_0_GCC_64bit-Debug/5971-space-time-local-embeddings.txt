Space-Time Local Embeddings

Ke Sun1‚àó Jun Wang2
Alexandros Kalousis3,1
SteÃÅphane Marchand-Maillet1
1
Viper Group, Computer Vision and Multimedia Laboratory, University of Geneva
sunk.edu@gmail.com, Stephane.Marchand-Maillet@unige.ch, and 2 Expedia,
Switzerland, jwang1@expedia.com, and 3 Business Informatics Department, University
of Applied Sciences, Western Switzerland, Alexandros.Kalousis@hesge.ch

Abstract
Space-time is a profound concept in physics. This concept was shown to be
useful for dimensionality reduction. We present basic definitions with interesting counter-intuitions. We give theoretical propositions to show that space-time
is a more powerful representation than Euclidean space. We apply this concept
to manifold learning for preserving local information. Empirical results on nonmetric datasets show that more information can be preserved in space-time.

1

Introduction

As a simple and intuitive representation, the Euclidean space <d has been widely used in various
learning tasks. In dimensionality reduction, n given high-dimensional points in <D , or their pairwise (dis-)similarities, are usually represented as a corresponding set of points in <d (d < D).

The representation power of <d is limited. Some of its limitations are listed next. √Ä The maximum
number of points which can share a common nearest neighbor is limited (2 for <; 5 for <2 ) [1, 2],
while such centralized structures do exist in real data. √Å <d can at most embed (d + 1) points
with uniform pair-wise similarities. It is hard to model pair-wise relationships with less variance. √Ç
Even if d is large enough, <d as a metric space must satisfy the triangle inequality, and therefore
must admit transitive similarities [2], meaning that a neighbor‚Äôs neighbor should also be nearby.
Such relationships can be violated on real data, e.g. social networks. √É The Gram matrix of n
real vectors must be positive semi-definite (p. s. d.). Therefore <d cannot faithfully represent the
negative eigen-spectrum of input similarities, which was discovered to be meaningful [3].
To tackle the above limitations of Euclidean embeddings, a commonly-used method is to impose a
statistical mixture model. Each embedding point is a random point on several candidate locations
w. r. t. some mixture weights. These candidate locations can be in the same <d [4]. This allows an
embedding point to jump across a long distance through a ‚Äústatistical worm-hole‚Äù. Or, they can be
in m independent <d ‚Äôs [2, 5], resulting in m different views of the input data.

Another approach beyond Euclidean embeddings is to change the embedding destination to a curved
space Md . This Md can be a Riemannian manifold [6] with a positive definite metric, or equivalently, a curved surface embedded in a Euclidean space [7, 8]. To learn such an embedding requires
a closed-form expression of the distance measure. This Md can also be semi-Riemannian [9] with
an indefinite metric. This semi-Riemannian representation, under the names ‚Äúpseudo-Euclidean
space‚Äù, ‚ÄúMinkowski space‚Äù, or more conveniently, ‚Äúspace-time‚Äù, was shown [3, 7, 10‚Äì12] to be a
powerful representation for non-metric datasets. In these works, an embedding is obtained through
a spectral decomposition of a ‚Äúpseudo-Gram‚Äù matrix, which is computed based on some input data.
On the other hand, manifold learning methods [4, 13, 14] are capable of learning a p. s. d. kernel Gram matrix, that encapsulates useful information into a narrow band of its eigen-spectrum.
‚àó

Corresponding author

1

Usually, local neighborhood information is more strongly preserved as compared to non-local information [4, 15], so that the input information is unfolded in a non-linear manner to achieve the
desired compactness.
The present work advocates the space-time representation. Section 2 introduces the basic concepts.
Section 3 gives several simple propositions that describe the representation power of space-time. As
novel contributions, section 4 applies the space-time representation to manifold learning. Section 5
shows that using the same number of parameters, more information can be preserved by such embeddings as compared to Euclidean embeddings. This leads to new data visualization techniques.
Section 6 concludes and discusses possible extensions.

2

Space-time

The fundamental measurements in geometry are established by the concept of a metric [6]. Intuitively, it is a locally- or globally-defined inner product. The metric of a Euclidean space <d is
everywhere identity. The inner product between any two vectors y1 and y2 is hy1 , y2 i = y1T Id y2 ,
where Id is the d √ó d identity matrix. A space-time <ds ,dt is a (ds + dt )-dimensional real vector
space, where ds ‚â• 0, dt ‚â• 0, and the metric is


Id s
0
M=
.
(1)
0 ‚àíIdt
This metric is not trivial. It is semi-Riemannian with a background in physics [9]. A point in <ds ,dt
is called an event, denoted by y = (y 1 , . . . , y ds , y ds +1 , . . . , y ds +dt )T . The first ds dimensions
are space-like, where the measurements are exactly the same as in a Euclidean space. The last dt
dimensions are time-like, which cause counter-intuitions. In accordance to the metric M in eq. (1),
‚àÄy1 , y2 ‚àà <ds ,dt ,

hy1 , y2 i =

ds
X
l=1

y1l y2l ‚àí

dX
s +dt

y1l y2l .

(2)

l=ds +1

In analogy to using inner products to define distances, the following definition gives a dissimilarity
measure between two events in <ds ,dt .

Definition 1. The space-time interval, or shortly interval, between any two events y1 and y2 is
dX
ds
s +dt
X
l
l 2
(y1l ‚àí y2l )2 .
(y1 ‚àí y2 ) ‚àí
c(y1 , y2 ) = hy1 , y1 i + hy2 , y2 i ‚àí 2hy1 , y2 i =
l=1

(3)

l=ds +1

The space-time interval c(y1 , y2 ) can be positive, zero or negative. With respect to a reference point
y0 ‚àà <ds ,dt , the set {y : c(y, y0 ) = 0} is called a light cone. Figure 1a shows a light cone in
<2,1 . Within the light cone, c(y, y0 ) < 0, i. e., negative interval occurs; outside the light cone,
c(y, y0 ) > 0. The following counter-intuitions help to establish the concept of space-time.
A low-dimensional <ds ,dt can accommodate an arbitrarily large number of events sharing a common nearest neighbor. In <2,1 , let A = (0, 0, 1), and put {B1 , B2 , . . . , } evenly on the circle
{(y 1 , y 2 , 0) : (y 1 )2 + (y 2 )2 = 1} at time 0. Then, A is the unique nearest neighbor of B1 , B2 , . . . .

A low-dimensional <ds ,dt can represent uniform pair-wise similarities between an arbitrarily large
number of points. In <1,1 , the similarities within {Ai : Ai = (i, i)}ni=1 are uniform.

In <ds ,dt , the triangle inequality is not necessarily satisfied. In <2,1 , let A = (‚àí1, 0, 0), B =
(0, 0, 1), C = (1, 0, 0). Then c(A, C) > c(A, B) + c(B, C). The trick is that, as B‚Äôs absolute
time value increases, its intervals with all events at time 0 are shrinking. Correspondingly, similarity
measures in <ds ,dt can be non-transitive. The fact that B is similar to A and C independently does
not necessarily mean that A and C are similar.
A neighborhood of y0 ‚àà <2,1 is {(y 1 , y 2 , y 3 ) : (y 1 ‚àíy01 )2 +(y 2 ‚àíy02 )2 ‚àí(y 3 ‚àíy03 )2 ‚â§ }, where  ‚àà
<. This hyperboloid has infinite ‚Äúvolume‚Äù, no matter how small  is. Comparatively, a neighborhood
in <d is much narrower, with an exponentially shrinking volume as its radius decreases.
2

time

time
c
spa

e

‚àí1 5
c = = ‚àí0.
c = 0.5
c
c=1

y0

‚àÜn
pÃÇ3,0

space

0

g(Kn3,0 )

space

p?

lightcone

pÃÇ2,1

(a)

(b)

g(Kn2,1 )

(c)

Figure 1: (a) A space-time; (b) A space-time ‚Äúcompass‚Äù in <1,1 . The colored lines show equalinterval contours with respect to the origin; (c) All possible embeddings in <2,1 (resp. <3 ) are
mapped to a sub-manifold of ‚àÜn , as shown by the red (resp. blue) line. Dimensionality reduction
projects the input p? onto these sub-manifolds, e. g. by minimizing the KL divergence.

3

The representation capability of space-time

This section formally discusses some basic properties of <ds ,dt in relation to dimensionality reduction. We first build a tool to shift between two different representations of an embedding: a matrix
of c(yi , yj ) and a matrix of hyi , yj i. From straightforward derivations, we have

Lemma
Pn 1. Cn = {Cn√ón : ‚àÄi, Cii = 0; ‚àÄi 6= j, Cij = Cji } and Kn = {Kn√ón :
‚àÄi, j=1 Kij = 0; ‚àÄi 6= j, Kij = Kji } are two families of real symmetric matrices. dim(Cn ) =
dim(Kn ) = n(n ‚àí 1)/2. A linear mapping from Cn to Kn and its inverse are given by
1
1
1
K(C) = ‚àí (In ‚àí eeT )C(In ‚àí eeT ), C(K) = diag(K)eT + ediag(K)T ‚àí 2K, (4)
2
n
n
where e = (1, ¬∑ ¬∑ ¬∑ , 1)T , and diag(K) means the diagonal entries of K as a column vector.
Cn and Kn are the sets of interval matrices and ‚Äúpseudo-Gram‚Äù matrices, respectively [3, 12]. In
particular, a p. s. d. K ‚àà Kn means a Gram matrix, and the corresponding C(K) means a square
distance matrix. The double centering mapping K(C) is widely used to generate a (pseudo-)Gram
matrix from a dissimilarity matrix.

Proposition 2. ‚àÄC ? ‚àà Cn , ‚àÉ n events in <ds ,dt , s. t. ds + dt ‚â§ n ‚àí 1 and their intervals are C ? .
Prank(K ? ) ? ? ? T
Proof. ‚àÄC ? ‚àà Cn , K ? = K(C ? ) has the eigen-decomposition K ? =
vl (vl )
Œªl p
l=1
where rank(K ? ) ‚â§ n ‚àí 1 and {vl? } are orthonormal. For each l = 1, ¬∑ ¬∑ ¬∑ , rank(K ? ), |Œª?l |vl?
gives the coordinates in one dimension, which is space-like if Œª?l > 0 or time-like if Œª?l < 0.
Remark 2.1. <ds ,dt (ds + dt ‚â§ n ‚àí 1) can represent any interval matrix C ? ‚àà Cn , or equivalently,
any K ? ‚àà Kn . Comparatively, <d (d ‚â§ n ‚àí 1) can only represent {K ‚àà Kn : K  0}.
A pair-wise distance matrix in <d is invariant to rotations. In other words, the direction information
of a point cloud is completely discarded. In <ds ,dt , some direction information is kept to distinguish
between space-like and time-like dimensions. As shown in fig. 1b, one can tell the direction in <1,1
by moving a point along the curve {(y 1 )2 + (y 2 )2 = 1} and measuring its interval w. r. t. the origin.
Local
embedding techniques often use similarity measures in a statistical
n
o simplex ‚àÜn =
P
p = (pij ) : 1 ‚â§ i ‚â§ n; 1 ‚â§ j ‚â§ n; i < j; ‚àÄi, ‚àÄj, pij > 0; i,j:i<j pij = 1 . This ‚àÜn has one
less dimension than Cn and Kn so that dim(‚àÜn ) = n(n ‚àí 1)/2 ‚àí 1. A mapping from Kn (Cn ) to
‚àÜn is given by
pij ‚àù f (Cij (K)),
(5)

where f (¬∑) is a positive-valued strictly monotonically decreasing function, so that a large probability
mass is assigned to a pair of events with a small interval. Proposition 2 trivially extends to
Proposition 3. ‚àÄp? ‚àà ‚àÜn , ‚àÉ n events in <ds ,dt , s. t. ds + dt ‚â§ n ‚àí 1 and their similarities are p? .
Remark 3.1. <ds ,dt (ds + dt ‚â§ n ‚àí 1) can represent any n √ó n symmetric positive similarities.
3

?
Typically in
The pre-image in
 eq. (5) we have f (x)
 = exp (‚àíx).
	 Cn of any given p ‚àà
 ‚àÜn is
?
the curve C ? + 2Œ¥ eeT ‚àí In : ‚àÄi 6= j, Cij
= ‚àí ln p?ij ; Œ¥ ‚àà < , where 2Œ¥ eeT ‚àí In means
?
a uniform
on the off-diagonal
(4), the corresponding curve in
 increment
 entries of
	 C . By eq.
1
?
?
T
?
ee
:
Œ¥
‚àà
<
,
where
K
(0)
= K ? = K(C ? ). Because
Kn is K (Œ¥)
=
K
+
Œ¥
I
‚àí
n
n

1
T
?
In ‚àí n ee shares with K a common eigenvector e with zero eigenvalue, and the rest eigenrank(K ? )

?
values are all 1, there exist orthonormal vectors {vl? }n‚àí1
l=1 and real numbers {Œªl }l=1
?

P
P
rank(K
)
n‚àí1
K ? = l=1
Œª?l vl? (vl? )T , and In ‚àí n1 eeT = l=1 vl? (vl? )T . Therefore
rank(K ? )

?

K (Œ¥) =

X

(Œª?l + Œ¥)vl? (vl? )T +

n‚àí1
X

Œ¥vl? (vl? )T .

, s. t.

(6)

l=rank(K ? )+1

l=1

Depending on Œ¥, K ? (Œ¥) can be negative definite, positive definite, or somewhere in between. This
is summarized in the following theorem.
Theorem 4. If f (x) = exp(‚àíx) in eq. (5), the pre-image in Kn of ‚àÄp? ‚àà ‚àÜn is a continuous curve
{K ? (Œ¥) : Œ¥ ‚àà <}. ‚àÉŒ¥0 , Œ¥1 ‚àà <, s. t. ‚àÄŒ¥ < Œ¥0 , K ? (Œ¥) ‚â∫ 0, ‚àÄŒ¥ > Œ¥1 , K ? (Œ¥)  0, and the number
of positive eigenvalues of K ? (Œ¥) increases monotonically with Œ¥.
With enough dimensions, any p? ‚àà ‚àÜn can be perfectly represented in a space-only, or timeonly, or space-time-mixed <ds ,dt . There is no particular reason to favor a space-only model,
because the objective of dimensionality reduction is to get a compact model with a small number of dimensions, regardless of whether they are space-like or time-like. Formally, Knds ,dt =
{K + ‚àí K ‚àí : rank(K + ) ‚â§ ds ; rank(K ‚àí ) ‚â§ dt ; K +  0; K ‚àí  0} is a low-rank subset of
Kn . In the domain Kn , dimensionality reduction based on the input p? finds some KÃÇ ds ,dt ‚àà
Knds ,dt , which is close to the curve K ? (Œ¥).
In the probability domain ‚àÜn , the image of Knds ,dt under some mapping g : Kn ‚Üí ‚àÜn is
g(Knds ,dt ). As shown in fig. 1c, dimensionality reduction finds some pÃÇds ,dt ‚àà g(Knds ,dt ), so
that pÃÇds ,dt is the closest point to p? w. r. t. some information theoretic measure. The proximity
of p? to pÃÇds ,dt , i. e. its proximity to g(Knds ,dt ), measures the quality of the model <ds ,dt as the
embedding target space, when the model scale or the number of dimensions is given.
We will investigate the latter approach, which depends on the choice of ds , dt , the mapping g, and
some proximity measure on ‚àÜn . We will show that, with the same number of dimensions ds + dt ,
the region g(Knds ,dt ) with space-time-mixed dimensions is naturally close to certain input p? .

4

Space-time local embeddings

We project a given similarity matrix p? ‚àà ‚àÜn to some KÃÇ ‚àà Knds ,dt , or equivalently, to a set of
events Y = {yi }ni=1 ‚äÇ <ds ,dt , so that ‚àÄi, ‚àÄj, hyi , yj i = KÃÇij as in eq. (2), and the similarities
among these events resemble p? . As discussed in section 3, a mapping g : Kn ‚Üí ‚àÜn helps transfer
Knds ,dt into a sub-manifold of ‚àÜn , so that the projection can be done inside ‚àÜn . This mapping
expressed in the event coordinates is given by

exp kyit ‚àí yjt k2
pij (Y ) ‚àù
,
(7)
1 + kyis ‚àí yjs k2

where y s = (y 1 , . . . , y ds )T , y t = (y ds +1 , . . . , y ds +dt )T , and k ¬∑ k denotes the 2-norm. For any pair
of events yi and yj , pij (Y ) increases when their space coordinates move close, and/or when their
time coordinates move away. This agrees with the basic intuitions of space-time. For time-like dimensions, the heat kernel is used to make pij (Y ) sensitive to time variations. This helps to suppress
events with large absolute time values, which make the embedding less interpretable. For space-like
dimensions, the Student-t kernel, as suggested by t-SNE [13], is used, so that there could be more
‚Äúvolume‚Äù to accommodate the often high-dimensional input data. Based on our experience, this
hybrid parametrization of pij (Y ) can better model real data as compared to alternative parametrizations. Similar to SNE [4] and t-SNE [13], an optimal embedding can be obtained by minimizing the
Kullback-Leibler (KL) divergence from the input p? to the output p(Y ), given by
X
p?ij
.
(8)
KL(Y ) =
p?ij ln
pij (Y )
i,j:i<j
4

According to some straightforward derivations, its gradients are
X


‚àÇKL
= ‚àí2
p?ij ‚àí pij (Y ) yit ‚àí yjt ,
t
‚àÇyi

(9)

j:j6=i

X


1
‚àÇKL
p?ij ‚àí pij (Y ) yis ‚àí yjs ,
=2
s
s
s
2
‚àÇyi
1 + kyi ‚àí yj k

(10)

j:j6=i

where ‚àÄi, ‚àÄj, p?ij = p?ji and pij (Y ) = pji (Y ). As an intuitive interpretation of a gradient descent
process w. r. t. eqs. (9) and (10), we have that if pij (Y ) < p?ij , i. e. yi and yj are put too far
from each other, then yis and yjs are attracting, and yit and yjt are repelling, so that their space-time
interval becomes shorter; if pij (Y ) > p?ij , then yi and yj are repelling in space and attracting in
time.
During gradient descent, {yis } are updated by the delta-bar-delta scheme as used in t-SNE [13],
where each scalar parameter has its own adaptive learning rate initialized to Œ≥ s > 0; {yit } are
updated based on one global adaptive learning rate initialized to Œ≥ t > 0. The learning of time
should be more cautious, because pij (Y ) is more sensitive to time variations by eq. (7). Therefore,
the ratio Œ≥ t /Œ≥ s should be very small, e.g. 1/100.

5

Empirical results

Aiming at potential applications in data visualization and social network analysis, we compare
SNE [4], t-SNE [13], and the method proposed in section 4 denoted as SNEST . They are based
on the same optimizer but correspond to different sub-manifolds of ‚àÜn , as presented by the curves
in fig. 1c. Given different embeddings of the same dataset using the same number of dimensions,
we perform model selection based on the KL divergence as explained in the end of section 3.
We generated a toy dataset SCHOOL, representing a school with two classes. Each class has 20
students standing evenly on a circle, where each student is communicating with his (her) 4 nearest
neighbours, and one teacher, who is communicating with all the students in the same class and the
teacher in the other class. The input p? is distributed evenly on the pairs (i, j) who are socially
connected.
NIPS22 contains a 4197 √ó 3624 author-document matrix from NIPS 1988 to 2009 [2]. After
discarding the authors who have only one NIPS paper, we get 1418 authors who co-authored
2121 papers. The co-authorship matrix is CA1418√ó1418 , where CAij denotes the number of papers thatPauthor i co-authored
with author j. The input similarity p? is computed so that p?ij ‚àù
P
CAij (1/ j CAij + 1/ i CAij ), where the number of co-authored papers is normalized by each
author‚Äôs total number of papers. NIPS17 is built in the same way using only the first 17 volumes.
GrQc is an arXiv co-authorship graph [16] with 5242 nodes and 14496 edges. After removing
one isolated node, a matrix CA5241√ó5241 gives the numbers of co-authored papers between any two
authors who submitted to the general relativity and quantum cosmology
category
P
P from January 1993
to April 2003. The input similarity p? satisfies p?ij ‚àù CAij (1/ j CAij + 1/ i CAij ).
W5000 is the semantic similarities among 5000 English words in WS5000√ó5000 [2, 17]. Each WSij
is an asymmetric non-negative similarityPfrom word i to P
word j. The input is normalized into a
probability vector p? so that p?ij ‚àù WSij / j WSij + WSji / i WSji . W1000 is built in the same way
using a subset of 1000 words.
Table 1 shows the KL divergence in eq. (8). In most cases, SNEST for a fixed number of free parameters has the lowest KL. On NIPS22, GrQc, W1000 and W5000, the embedding by SNEST in <2,1
is even better than SNE and t-SNE in <4 , meaning that the embedding by SNEST is both compact
and faithful. This is in contrast to the mixture approach for visualization [2], which multiplies the
number of parameters to get a faithful representation.
Fixing the free parameters to two dimensions, t-SNE in <2 has the best overall performance, and
SNEST in <1,1 is worse. We also discovered that, using d dimensions, <d‚àí1,1 usually performs
better than alternative choices such as <d‚àí2,2 , which are not shown due to space limitation. A timelike dimension allows adaptation to non-metric data. The investigated similarities, however, are
5

Table 1: KL divergence of different embeddings. After repeated runs on different configurations for
each embedding, the minimal KL that we have achieved within 5000 epochs is shown. The bold
numbers show the winners among SNE, t-SNE and SNEST using the same number of parameters.

SNE ‚Üí <2
SNE ‚Üí <3
SNE ‚Üí <4
t-SNE ‚Üí <2
t-SNE ‚Üí <3
t-SNE ‚Üí <4
SNEST ‚Üí <1,1
SNEST ‚Üí <2,1
SNEST ‚Üí <3,1

SCHOOL
0.52
0.36
0.19
0.61
0.58
0.58
0.43
0.31
0.29

NIPS17
1.88
0.85
0.35
0.88
0.85
0.84
0.91
0.60
0.54

NIPS22
2.98
1.79
1.01
1.29
1.23
1.22
1.62
0.97
0.93

GrQc
3.19
1.82
1.03
1.24
1.14
1.11
2.34
1.00
0.88
2

W1000
3.67
3.20
2.76
2.15
2.00
1.96
2.59
1.92
1.79

W5000
4.93
4.42
3.93
3.00
2.79
2.74
3.64
2.57
2.39

exp(kyit ‚àí ytj k2)/(1 + kyis ‚àí ysj k2)

100

kyit ‚àí ytj k

time

1

50

0.1

10

100

1

teachers

0

(a)

50

100 150
kyis ‚àí ysj k

0
200

(b)

Figure 2: (a) The embedding of SCHOOL by SNEST in <2,1 . The black (resp. colored) dots denote
the students (resp. teachers). The paper coordinates (resp. color) mean the space (resp. time)
exp(ky t ‚àíy t k2 )
coordinates. The links mean social connections. (b) The contour of 1+kysi‚àíysjk2 in eq. (7) as a
i

j

function of kyis ‚àí yjs k (x-axis) and kyit ‚àí yjt k (y-axis). The unit of the displayed levels is 10‚àí3 .
mainly space-like, in the sense that a random pair of people or words are more likely to be dissimilar
(space-like) rather than similar (time-like). According to our experience, on such datasets, good
performance is often achieved with mainly space-like dimensions mixed with a small number of
time-dimensions, e.g. <2,1 or <3,1 as suggested by table 1.

To interpret the embeddings, fig. 2a presents the embedding of SCHOOL in <2,1 , where the space
and time are represented by paper coordinates and three colors levels, respectively. Each class is
embedded as a circle. The center of each class, the teacher, is lifted to a different time, so as to be
near to all students in the same class. One teacher being blue, while the other being red, creates a
‚Äúhyper-link‚Äù between the teachers, because their large time difference makes them nearby in <2,1 .
Figures 3 and 4 show the embeddings of NIPS22 and W5000 in <2,1 . Similar to the (t-)SNE
visualizations [2, 4, 13], it is easy to find close authors or words embedded nearby. The learned
p(Y ), however, is not equivalent to the visual proximity, because of the counter-intuitive time dimension. How much does the visual proximity reflect the underlying p(Y )? From the histogram
of the time coordinates, we see that the time values are in the narrow range [‚àí1.5, 1.5], while the
range of the space coordinates is at least 100 times larger. Figure 2b shows the similarity function
on the right-hand-side of eq. (7) over an interesting range of kyis ‚àí yjs k and kyit ‚àí yjt k. In this range,
large similarity values are very sensitive to space variations, and their red level curves are almost
vertical, meaning that the similarity information is largely carried by space coordinates. Therefore,
the visualization of neighborhoods is relatively accurate: visually nearby points are indeed similar;
proximity in a neighborhood is informative regarding p(Y ). On the other hand, small similarity values are less sensitive to space variations, and their blue level curves span a large distance in space,
meaning that the visual distance between dissimilar points is less informative regarding p(Y ). For
6

>1.0

Das
Atiya
Frasconi

Sminchisescu

Mozer
Grimes

Bengio

Li

Kim
Yu

Achan

Gupta

150
Ballard

Gerstner

Rosenfeld

0.5

Touretzky

Black

RaoCohn

Caruana
Mitchell

Thrun

Courville
Gordon

Wainwright

Bradley
Tesauro

Lafferty

SahaniPillow

Willsky

Tresp

Montague

Welling

Dayan

Smyth

Malik
Hofmann

Kulis
Rahimi
Grauman
Pouget
Roth
Blei
Teh
Hinton
Simoncelli
Baldi
Mjolsness
Riesenhuber
Winther
Lee
Saad Ghahramani
Moody
Zemel
Griffiths Bartlett Seung
Opper
Leen
Barber
Marchand
Jordan
Scholkopf
Amari
Kakade
Wang
Muller
Atkeson Bishop TishbySaul BengioSeeger
Weston
Scott
Doya WilliamsFrey
Crammer Shawe-Taylor
Ratsch
Zador
JaakkolaSinger
Ruppin
Waibel
0
Yuille
Bach Poggio
Maass
Garrigues
Horn WeinshallDarrell SchraudolphChapelle Williamson
Hochreiter
Vapnik
DeWeese
Lewicki
Koller FukumizuPlatt Warmuth
Freeman
Bialek
Lee
XingGrettonSmolaSimard
Stevens
Weiss
Gray
Herbrich
Ng
Blair
Pearlmutter
Viola Mohri Denker
Mel
Moore
Kearns
Koch Bower Schuurmans
Hastie
Movellan
Singh LeCun
Lee
Liu
WangJin
Zhang
Sejnowski
Lippmann
Barto
DeFreitas
Murray
Sutton
Hasler
Attias
Goldstein
Minch
Roweis

Nowlan

RasmussenTenenbaum

Buhmann

Sollich

Cristianini

---time-->

250

0

Graepel

Morgan
Beck
Johnson
Rumelhart

‚àí150

Cauwenberghs

Meir

Kawato

Cottrell

Smith

Zhang

-0.5

250

Sun

Obermayer

histogram of time coordinates

Giles

200
150
100

‚àí250

50

Cowan

‚àí250

‚àí150

‚àí1.5

0

150

0.0

250

1.5

<-1.0

Figure 3: An embedding of NIPS22 in <2,1 . ‚ÄúMajor authors‚Äù with at least 10 NIPS papers or with
a time value in the range (‚àí‚àû, ‚àí1] ‚à™ [1, ‚àû) are shown by their names. Other authors are shown
by small dots. The paper coordinates are in space-like dimensions. The positions of the displayed
names are adjusted up to a tiny radius to avoid text overlap. The color of each name represents the
time dimension. The font size is proportional to the absolute time value.

example, a visual distance of 165 with a time difference of 1 has roughly the same similarity as a
visual distance of 100 with no time difference. This is a matter of embedding dissimilar samples far
or very far and does not affect much the visual perception, which naturally requires less accuracy on
such samples. However, perception errors could still occur in these plots, although they are increasingly unlikely as the observation radius turns small. In viewing such visualizations, one must count
in the time represented by the colors and font sizes, and remember that a point with a large absolute
time value should be weighted higher in similarity judgment.
Consider the learning of yi by eq. (9), if the input p?ij is larger than what can be faithfully modeled
in a space-only model, then j will push i to a different time. Therefore, the absolute value of time
is a significance measurement. By fig. 2a, the connection hubs, and points with remote connections,
are more likely to be at a different time. Emphasizing the embedding points with large absolute time
values helps the user to focus on important points. One can easily identify well-known authors and
popular words in figs. 3 and 4. This type of information is not discovered by traditional embeddings.

6

Conclusions and Discussions

We advocate the use of space-time representation for non-metric data. While previous works on
such embeddings [3, 12] compute an indefinite kernel by simple transformations of the input data,
we learn a low-rank indefinite kernel by manifold learning, trying to better preserve the neigh7

>0.8

HER

SPIDER

SHUTTER

150

OUTDOORS

SQUID
GLOVES

CLAM

PARROT

PANTYHOSE

CURTAINS

TURTLE

PRONOUN

COWGIRL

LOBSTER

CRANE

SHARK

FEATHER

SINKER

FISH BOAT

BIRD

FRONTIER

BOOT

DOLPHIN

CREW
LAKE
SPACE
GULLY MARINE
TELESCOPE
PLANETS
NEST BIRDSDUCKS RIVER DRIFT
SUBMARINE

EAGLE
PENGUIN

FLY

CUE
WATERFALL

BUTTERFLY

HARBOR
TROPICALSUNSET

ROCKS

WATERDISASTER
THIRSTY

ROACH

BASSFLUTE
PARADE

BOARD

SUNSHINE

TUBE

PIANO

MARINES

TERMINAL
ACCIDENT

HIKER

FLEET

RADIATOR

MUSIC

GENERAL

FLAP
POUR
TRAILER
KEYSSTICKER
BATTERY
BUS
WANDERCHASE
SINGER SPEAKER
DUNK BLOW
AURA
PLUG
CONSOLE
RANKCOMMANDER
FUSE
TRAIN
LIGHTNING
POEM
DANCER
BALLOON
MAROON
RUNNER VALVE
OZONE
TRUCK
STEAMBREATH
HAUL
CLEAR PADDY
EXPRESS
BREEZEWAY
HOOP
SOAPCLEANER FUMES
UNLOAD
SALOON
HANDLE
VEER
HIKING
FOG
BUMPS
TAR
SPIT BOILFEVER CUPBOARD
RUNNING
COUNTER
MENTHOL
FIREMAN
CLEANING
CYLINDER
MARBLE
SLURP
DRAGON
MAIN SIGNAL
SPONTANEOUS
COOL MILD OIL
CORNERPASSAGE
VODKALEMONADE
SCUM
DUST
EMERGENCY
CAMPINGGLIDE LEVELELECTRICIAN WIDE CROOKED ORIENT EXPLORER
DRYER
DRAFT WINECOCA-COLA
SHAKE HEAT
FIREPLACE
BEYOND
COVEREDDESCENT
PILE
SMEAR
REACTION
MEASURE
CLUMSY STAIRS
ATTIC
FUZZ
TRAVEL
TOOTHPASTEPALE
BARREL THIRSTSNAP
MOTION
SWING
DIRECTION
MINER
LIFT
DRILL
CUBE
TOURIST

RAIN

SWAMPSPRAYDRAIN

SLUG

DAMP

LIZARD

SHADOW

DRUNK

HOLE

COLD

BOWL

BLENDER

HOCKEY
GRIND

SNOW
BUFFALO
STICK
MILKSHEEP

TEAPOT

SILVERWARE

KITCHENORANGE JUICE
MOLASSES
CATTLE
FRY

EGGOATMEAL

TURKEY
INTAKE

LUNCH

HOSTESS

0

CHINESE

HOT DOGS
COLESLAW

ITALIAN

MIXED

TOPPING

CRUSH

LEMON

SCAR

COMMUNIST
MAKE UP

RED

RELIEF

DISGUST

SQUEEZE

BODY

MOLE

FRAIL

DANGEROUS

TWELVE

FOUL

DRUGSDOCTOR
MAN
MAIDEN

BABY

ESSENCE

histogram of time coordinates

IMPATIENCE

SUPERMAN

SPOIL

GHOST

PUBERTY

PARTY

HORMONES

ECSTASY

BUZZ
INVENTOR
1000 EGYPT GRACE

‚àí100
PRIME

CROSS

PROVE

PERSUADE

LOVER

INTIMATE

INTEGRITY

REPENTANCE NEPHEW

‚àí1.5

0.0

‚àí150

1.5

ROYAL

‚àí100

MONARCH

OATH FAVOR

ENGAGE

GROOM

PLEASE GIVING

RARE

COMMON SOME

COURT

NOTHING

BET

EXTRA

-0.4

REDUCE

STOLEN

ADD
VOID QUALITY
ADDITION
GET USE BORROW
TILL
ACCOUNT
CHARITY

FORBID
POSSESSION

KEEP

BEG

COMPULSION

RITUAL
THRESHOLD

NORM

UNEVEN

DISTINCT

CRIME CHANCE
FOR

CAPTURE

REPLACE
FORBIDDEN

ADMIT

LIMIT

DARECLAIMS

LACK

ASSOCIATE
PARENT
INDEPENDENT
RESPONSIBILITY

EMPIRE

ANARCHY

CHANGE
ANOTHER MAJORITY
RESTRICTION
DIFFERENCE
MAXIMUM
REBEL LEGAL

FAKEINTERESTSALESMAN

FAITHFULAFFAIR

SISTER
BOND
RIGHTEOUSNESS FAMILY
VALENTINESTRANGER
JEWISH
SUNDAY
GREEK
OPPONENT
LOVERS

NUN

NOT

MORAL
HONOR
TRAITOR
DISBELIEVE

RESPECT

UNDERSTANDING

REVIVAL

500

TABOO

ADORE

FAIR
PERFECT

FAVORITE

ENDLESS
DECISION
OFTEN
EVERYDAY
ABUNDANCE

CONGRESS

PROOF

ACCUMULATE

INNOCENCE

OUTSTANDING

DECENCY

DISOWN

PARENTS

BLAME

MISCHIEF

HELPFUL

FUSS
WORRY
ANNOYING

IDOL JUNIOR
SUPERSTITION

IDENTITY
MEMORY DISPERSE

FILL

HANDICAP

VOTE

ORDER

TEN

PERCENT

PERSONALITY
TEND

WORTHLESS

REGRET

MATH SPY

ALONETOGETHER
PERSON

CRITICISM

REPEAT

AWARD

FACTORY

LEADER

CARD

POSSIBLE

UNSURE

DEPLETION

MANAGEMENT
BRIEFCASE
AUTHORITY
HELPER MASTER

SUPPORT

POLICEMAN
UNION
CULTURE
IMPORTANT
DIVISION

EGO

FEELING

GODANGEL
DAMN

PROVERB

STUBBORN

CONDEMN

DISMAY

FRUSTRATE

SNEAKY
POPULAR

TREAT

CHAMPION

DILIGENCE

SUGGEST

FIGURE

NEGOTIATION

NONSENSE

KIDS

DELIGHT

STAFF

AFFECT

DOBE
PROFESSIONAL

REASON

EFFORT
SKILL CAPACITY
SECRETARY
EXCEL
PROFESSION
VIOLATION
CHALLENGE
POTENTIAL
DEFEAT

SPECIFIC

CRISIS

RESISTANCE

CONSEQUENCE

ACCOMPLISHED

HISTORY

EVALUATE
PROJECT

RENOUNCE

CREATIVITY

THINK
CONFUSE
SHYEXPERIENCE

OBNOXIOUS

ENTERTAIN

SENSITIVE

TEMPER

CODE

SCREAM

AWAREHIDDENSTIMULUS

CONFUSION THEORY
OPINION

SERIOUS

ATTRACT

SCIENTIFIC

COMPOUND

EINSTEIN

SPADE

HARDY

CHEERLEADER

WASP

NERVE

OUTRAGEOUS

GROWTH

SPELL
MESSAGE
CONTEXT
DELIVER

GRADUATION
LEARN

0

DEFENSE

WRAP

FOREIGN

ELABORATE

SINCE

WHO

LABEL
WRITER

GONE COURSEMEANING

THERAPY

DARING

CONTEMPORARY

TRAUMAELDERS

OUTLINE

PLAN

IMPRESSION

HOROSCOPE
ABSENCE ORIGINATE

SCRIBBLE

CLAMP

WARN

SYMBOL

ISSUE

THESIS

HIGHLIGHT

ATTENTION

OPENING

SNEAK

PROTECT TELEPHONE
PRESENTATION

BUSY

DOMINATE

FRAY

RECENT
DATE INSTANCE
TELEVISION
LEAD
ELIMINATION EVENT
AD

TALE

FREE

STALL

GIVE UP

GOING
COMPUTER

DISCOVER

BOXER

ATTEND

ESCAPE

IMITATE

PERFORMANCE

CAST

SALUTE

ERA

BRAKE

RETREAT
DELAY
DIRECT
RETURN

SERIES

SCREEN ENGINEER

PROCESS

PROTECTION

DISCIPLINE

PANIC

MEDICINE BOY
CUTEDECORATEVIOLENT
ELEGANT

EYEBALL

UNIFORM

NERVES
INTENSITY

FREEDOM

EVICT

SCENERY

MISSILE
TIRED

EXTREME

MADE

GROW

NEUTRAL

SMALL

TONE

WAKE

PLAIN

STIFF

EUROPE

MICROSCOPE

COMFORTPEACEFUL
HYPNOTIZE

GRAVE

BREAST

HEALTH

LOFT

PLUSH

FIT

PERISH BIRTH

SMELL MAFIA DRUG

MUSTY

GUN

COMPONENTS

RAPE

STROKE

BACTERIA

HANDKERCHIEF
MOUTH

SENSE

RESTORE

JAPAN
ANNIHILATE

DOWNTOWN

LONG

EXERCISE

RECLINERPOISE

LIMP

SPLIT

HOBBY

FOLD

STYLE

SWABS

SUSPENSE
CAPTION

RANGE

FOOTBALL

COVER
MUSCLE

TIP

FORT

SPIKE

CLOTHES

BLOOD

PINCH

ZIT
LICK HEADACHE

TART

AX

HAIRCUT

LEAN

CHEEK

PAINTER
KEEPER

PLACE
APARTMENT

WRESTLING
EMERALD

SEWLACE

DETERIORATE

WASTED SABER
PINK BEARD

COSTUME

MINT

GARLIC

PROVISIONINTESTINELIVER

PANTS

SWOON

BROWN NECK

CLAY

SLIVER

RACK

INDIANBLOCK

HOTEL

BASE
BUILDING

BAT DICE

HIT

BOUND

STRIPE

PAT

FLOWER HAIR SHOT

SQUASH

VASELINE

BISCUIT

WOLF

BACK

IMAGEBLOCKADEARTS CHART

POUND

CHUCK

WASTE

ELF

APPLE

PIZZA

‚àí50

NATURAL

PASTRYSTRAWBERRYFLOWERS

DILL

PARSLEY

PLASTIC

RAT

STRAY

TAIL

PIGFIELD

CRACKER FATTENING
PIE

GOODS TASTY

PREDATOR LEO

BAG

CHISEL
SHED

HANGHAND
TARGET
ANIMALCOAT
IRON
ANKLE

LANDSCAPEHEDGE

SAUSAGE

PROTEIN

RIBS CRUNCH
BRITTLE RABBIT

SOUTHERN
POTATO

WOOD

STAIN

ELEPHANT

MONKEY
TREEDEER
BEAR

CHICKENCORN
HAY

HUNGER
COOKED

SUPERMARKET

MOVE

DIAMETER

SAUCER

MIXTURE

ROAD

DOOR

BEER

50

RECKLESS

CAR

ROBE

---time-->

BITE

LAVA

EXPLODE AARDVARK
GOO

ANISETTE

‚àí150

0.4

CART
MOTORCYCLE CONTROLS
BICYCLE
HELICOPTER

RUBBER

CRATER

WEEK

AFTERNOON

NOMAD RAM

SITTING

RIDER

COAST

CORAL

SWIMMER

WORM SEALOVERFLOW
GRASSHOPPER

100

DOWNSTAIRS

STABLE
BOOTS
HORSE
ENVIRONMENTUNICORN

AHOY

TRADE

MONEY
QUARTER
DUE

CENTS

RECEIPT

RATE

EXCISE
PROFIT
VALUE
SALES

SHOP

INFLATION
ECONOMIC
HANDBAG

EXTRAVAGANT PRECIOUS

BROKE

BLACKMAIL
LUXURYSTINGY
BUM
GHETTO
WELFARE

WELCOME

MEET

‚àí50

0

50

100

150

<-0.8

Figure 4: An embedding of W5000 in <2,1 . Only a subset is shown for a clear visualization. The
position of each word represents its space coordinates up to tiny adjustments to avoid overlap. The
color of each word shows its time value. The font size represents the absolute time value.
bours [4]. We discovered that, using the same number of dimensions, certain input information is
better preserved in space-time than Euclidean space. We built a space-time visualizer of non-metric
data, which automatically discovers important points.
To enhance the proposed visualization, an interactive interface can allow the user select one reference point, and show the true similarity values, e.g., by aligning other points so that the visual
distances correspond to the similarities. Proper constraints or regularization could be proposed, so
that the time values are discrete or sparse, and the resulting embedding can be more easily interpreted.
The proposed learning is on a sub-manifold Knds ,dt ‚äÇ Kn , or a corresponding sub-manifold
of ‚àÜn .
	
Another interesting sub-manifold of Kn could be K ‚àí ttT : K  0; t ‚àà <n , which extends the
p. s. d. cone to any matrix in Kn with a compact negative eigen-spectrum. It is possible to construct
a sub-manifold of Kn so that the embedder can learn whether a dimension is space-like or time-like.
As another axis of future investigation, given the large family of manifold learners, there can be many
ways to project the input information onto these sub-manifolds. The proposed method SNEST is
based on the KL divergence in ‚àÜn . Some immediate extensions can be based on other dissimilarity
measures in Kn or ‚àÜn . This could also be useful for faithful representations of graph datasets with
indefinite weights.
Acknowledgments
This work has been supported be the Department of Computer Science, University of Geneva, in
collaboration with Swiss National Science Foundation Project MAAYA (Grant number 144238).
8

References
[1] K. Zeger and A. Gersho. How many points in Euclidean space can have a common nearest
neighbor? In International Symposium on Information Theory, page 109, 1994.
[2] L. van der Maaten and G. E. Hinton. Visualizing non-metric similarities in multiple maps.
Machine Learning, 87(1):33‚Äì55, 2012.
[3] J. Laub and K. R. MuÃàller. Feature discovery in non-metric pairwise data. JMLR, 5(Jul):801‚Äì
818, 2004.
[4] G. E. Hinton and S. T. Roweis. Stochastic neighbor embedding. In NIPS 15, pages 833‚Äì840.
MIT Press, 2003.
[5] J. Cook, I. Sutskever, A. Mnih, and G. E. Hinton. Visualizing similarity data with a mixture of
maps. In AISTATS‚Äô07, pages 67‚Äì74, 2007.
[6] J. Jost. Riemannian Geometry and Geometric Analysis. Universitext. Springer, 6th edition,
2011.
[7] R. C. Wilson, E. R. Hancock, E. Pekalska, and R. P. W. Duin. Spherical embeddings for
non-Euclidean dissimilarities. In CVPR‚Äô10, pages 1903‚Äì1910, 2010.
[8] D. Lunga and O. Ersoy. Spherical stochastic neighbor embedding of hyperspectral data. Geoscience and Remote Sensing, IEEE Transactions on, 51(2):857‚Äì871, 2013.
[9] B. O‚ÄôNeill. Semi-Riemannian Geometry With Applications to Relativity. Number 103 in Series:
Pure and Applied Mathematics. Academic Press, 1983.
[10] L. Goldfarb. A unified approach to pattern recognition. Pattern Recognition, 17(5):575‚Äì582,
1984.
[11] E. Pekalska and R. P. W. Duin. The Dissimilarity Representation for Pattern Recognition:
Foundations and Applications. World Scientific, 2005.
[12] J. Laub, J. Macke, K. R. MuÃàller, and F. A. Wichmann. Inducing metric violations in human
similarity judgements. In NIPS 19, pages 777‚Äì784. MIT Press, 2007.
[13] L. van der Maaten and G. E. Hinton. Visualizing data using t-SNE. JMLR, 9(Nov):2579‚Äì2605,
2008.
[14] N. D. Lawrence. Spectral dimensionality reduction via maximum entropy. In AISTATS‚Äô11,
JMLR W&CP 15, pages 51‚Äì59, 2011.
[15] K. Q. Weinberger, F. Sha, and L. K. Saul. Learning a kernel matrix for nonlinear dimensionality
reduction. In ICML‚Äô04, pages 839‚Äì846, 2004.
[16] J. Leskovec, J. Kleinberg, and C. Faloutsos. Graph evolution: Densification and shrinking
diameters. ACM Transactions on Knowledge Discovery from Data, 1(1), 2007.
[17] D. L. Nelson, C. L. McEvoy, and T. A Schreiber. The university of South Florida
word association, rhyme, and word fragment norms. 1998. http://www.usf.edu/
FreeAssociation.

9

