Convolutional Spike-triggered Covariance Analysis
for Neural Subunit Models
Anqi Wu1

Il Memming Park2

Jonathan W. Pillow1

Princeton Neuroscience Institute, Princeton University
{anqiw, pillow}@princeton.edu
Department of Neurobiology and Behavior, Stony Brook University
memming.park@stonybrook.edu
1

2

Abstract
Subunit models provide a powerful yet parsimonious description of neural responses to complex stimuli. They are defined by a cascade of two linear-nonlinear
(LN) stages, with the first stage defined by a linear convolution with one or more
filters and common point nonlinearity, and the second by pooling weights and an
output nonlinearity. Recent interest in such models has surged due to their biological plausibility and accuracy for characterizing early sensory responses. However,
fitting poses a difficult computational challenge due to the expense of evaluating
the log-likelihood and the ubiquity of local optima. Here we address this problem
by providing a theoretical connection between spike-triggered covariance analysis and nonlinear subunit models. Specifically, we show that a “convolutional”
decomposition of a spike-triggered average (STA) and covariance (STC) matrix
provides an asymptotically efficient estimator for class of quadratic subunit models. We establish theoretical conditions for identifiability of the subunit and pooling weights, and show that our estimator performs well even in cases of model
mismatch. Finally, we analyze neural data from macaque primary visual cortex
and show that our moment-based estimator outperforms a highly regularized generalized quadratic model (GQM), and achieves nearly the same prediction performance as the full maximum-likelihood estimator, yet at substantially lower cost.

1

Introduction

A central problem in systems neuroscience is to build flexible and accurate models of the sensory
encoding process. Neurons are often characterized as responding to a small number of features in the
high-dimensional space of natural stimuli. This motivates the idea of using dimensionality reduction
methods to identify the features that affect the neural response [1–9]. However, many neurons in
the early visual pathway pool signals from a small population of upstream neurons, each of which
integrates and nolinearly transforms the light from a small region of visual space. For such neurons,
stimulus selectivity is often not accurately described with a small number of filters [10]. A more
accurate description can be obtained by assuming that such neurons pool inputs from an earlier stage
of shifted, identical nonlinear “subunits” [11–13].
Recent interest in subunit models has surged due to their biological plausibility and accuracy for
characterizing early sensory responses. In the visual system, linear pooling of shifted rectified linear
filters was first proposed to describe sensory processing in the cat retina [14, 15], and more recent
work has proposed similar models for responses in other early sensory areas [16–18]. Moreover,
recent research in machine learning and computer vision has focused on hierarchical stacks of such
subunit models, often referred to as Convolutional Neural Networks (CNN) [19–21].
The subunit models we consider here describe neural responses in terms of an LN-LN cascade, that
is, a cascade of two linear-nonlinear (LN) processing stages, each of which involves linear projection
and a nonlinear transformation. The first LN stage is convolutional, meaning it is formed from one or
1

more banks of identical, spatially shifted subunit
filters, with outputs transformed by a shared subunit nonlinearity. The second LN stage consists
of a set of weights for linearly pooling the nonlinear subunits, an output nonlinearity for mapping
the output into the neuron’s response range, and
finally, an noise source for capturing the stochasticity of neural responses (typically assumed to be
Gaussian, Bernoulli or Poisson). Vintch et al proposed one variant of this type of subunit model, and
showed that it could account parsimoniously for the
multi-dimensional input-output properties revealed
by spike-triggered analysis of V1 responses [12, 13].

stimulus
1st LN stage

subunit
filter

subunit
nonliearity
W4

pooling
weights
W1

W3
W2

output
nonlinearity

W5

W6

W7

2nd LN stage

However, fitting such models remains a challengPoisson
ing problem. Simple LN models with Gaussian or
spiking
Poisson noise can be fit very efficiently with spiketriggered-moment based estimators [6–8], but there
response
is no equivalent theory for LN-LN or subunit models. This paper aims to fill that gap. We show that a
convolutional decomposition of the spike-triggered Figure 1: Schematic of subunit LN-LNP casaverage (STA) and covariance (STC) provides an cade model. For simplicity, we show only 1
asymptotically efficient estimator for a Poisson sub- subunit type.
unit model under certain technical conditions: the
stimulus is Gaussian, the subunit nonlinearity is well
described by a second-order polynomial, and the final nonlinearity is exponential. In this case, the
subunit model represents a special case of a canonical Poisson generalized quadratic model (GQM),
which allows us to apply the expected log-likelihood trick [7, 8] to reduce the log-likelihood to a
form involving only the moments of the spike-triggered stimulus distribution. Estimating the subunit
model from these moments, an approach we refer to as convolutional STC, has fixed computational
cost that does not scale with the dataset size after a single pass through the data to compute sufficient
statistics. We also establish theoretical conditions under which the model parameters are identifiable. Finally, we show that convolutional STC is robust to modest degrees of model mismatch, and
is nearly as accurate as the full maximum likelihood estimator when applied to neural data from V1
simple and complex cells.

2

Subunit Model

We begin with a general definition of the Poisson convolutional subunit model (Fig. 1). The model
is specified by:
subunit outputs:
spike rate:

smi = f (km · xi )
⇣XX
⌘
=g
wmi smi
m

spike count:

(1)
(2)

i

y| ⇠ Poiss( ),

(3)

where km is the filter for the m’th type of subunit, xi is the vectorized stimulus segment in the i’th
position of the shifted filter during convolution, and f is the nonlinearity governing subunit outputs.
For the second stage, wmi is a linear pooling weight from the m’th subunit at position i, and g is the
neuron’s output nonlinearity. Spike count y is conditionally Poisson with rate .
Fitting subunit models with arbitrary g and f poses significant computational challenges. However,
if we set g to exponential and f takes the form of second-order polynomial, the model reduces to
⇣ X
⌘
X
= exp 12
wmi (km · xi )2 + wmi (km · xi ) + a
(4)
⇣
⌘
1 >
= exp
b>
+a
(5)
[w,k] x
2 x C[w,k] x +
where

C[w,k] =

X

>
Km
diag(wm )Km ,

m

b[w,k] =

X
m

2

>
Km
wm ,

(6)

and Km is a Toeplitz matrix consisting of shifted copies of km satisfying Km x
[x1 , x2 , x3 , . . .]> km .

=

In essence, these restrictions on the two nonlinearities reduce the subunit model to a (canonicalform) Poisson generalized quadratic model (GQM) [7, 8, 22], that is, a model in which the Poisson
spike rate takes the form of an exponentiated quadratic function of the stimulus. We will pursue
the implications of this mapping below. We assume that k is a spatial filter vector without time
expansion. If we have a spatio-temporal stimulus-response, k should be a spatial-temporal filter, but
the subunit convolution (across filter position i) involves only the spatial dimension(s). From (eqs. 4
and 5) it can be seen that the subunit model contains fewer parameters than a full GLM, making it a
more parsimonious description for neurons with multi-dimensional stimulus selectivity.

3

Estimators for Subunit Model

With the above definitions and formulations, we now present three estimators for the model parameters {w, k}. To simplify the notation, we omit the subscript in C[w,k] and b[w,k] , but their
dependence on the model parameters is assumed throughout.
Maximum Log-Likelihood Estimator
The maximum log-likelihood estimator (MLE) has excellent asymptotic properties, though it comes
with the high computational cost. The log-likelihood function can be written:
X
X
LMLE (✓) =
yi log i
(7)
i
i

=
=

i

X

X
>
>
yi ( 21 x>
exp( 12 x>
i Cxi + b xi + a)
i Cxi + b xi + a)
"
#
X
>
>
1 >
Tr[C⇤] + b µ + ansp
exp( 2 xi Cxi + b xi + a)

(8)
(9)

i

P
P
where µ = i yi xi is the spike-triggered
average (STA) and ⇤ = i yi xi x>
i is the spike-triggered
P
covariance (STC) and nsp = i yi is the total number of spikes. We denote the MLE as ✓MLE .
Moment-based Estimator with Expected Log-Likelihood Fitting

If the stimuli are drawn from x ⇠ N (0, ), a zero-mean Gaussian with covariance , then the
expression in square brackets divided by N in (eq. 9) will converge to its expectation, given by
1
⇥
⇤
>
E exp( 12 x>
C| 2 exp 12 b> ( 1 C) 1 b + a
(10)
i Cxi + b xi + a) = |I

Substituting this expectation into (9) yields a quantity called expected log-likelihood, with the objective function as,
LELL (✓) = Tr[C⇤] + b> µ + ansp

N |I

C|

1
2

exp

1 >
2b (

1

C)

1

(11)

b+a

where N is the number of time bins. We refer to ✓MELE = arg max✓ LELL (✓) as the MELE (maximum expected log-likelihood estimator) [7, 8, 22].
Moment-based Estimator with Least Squares Fitting
Maximizing (11) w.r.t {C, b, a} yields analytical expected maximum likelihood estimates [7]:
Cmele =

1

⇤

1

, bmele = ⇤

1

µ, amele = log(

nsp
N |

⇤

1
1 2
| )

1 >
2µ

1

⇤

1

µ

(12)

With these analytical estimates, it is straightforward and to optimize w and k by directly minimizing
squared error:
LLS (✓) = ||Cmele

K > diag(w)K||22 + ||bmele

K > w||22

(13)

which corresponds to an optimal “convolutional” decomposition of the moment-based estimates.
This formulation shows that the eigenvectors of Cmele are spanned by shifted copies of k. We
denote this estimate ✓LS .
All three estimators, ✓MLE , ✓MELE and ✓LS should provide consistent estimates for the subunit model
parameters due to consistency of ML and MELE estimates. However, the moment-based estimates
3

(MELE and LS) are computationally much simpler, and scale much better to large datasets, due
to the fact that they depend on the data only via the spike-triggered moments. In fact their only
dependence on the dataset size is the cost of computing the STA and STC in one pass through the
data. As for efficiency, ✓LS has the drawback of being sensitive to noise in the Cmele estimate, which
has far more free parameters than in the two vectors w and k (for a 1-subunit model). Therefore,
accurate estimation of Cmele should be a precondition for good performance of ✓LS , and we expect
✓MELE to perform better for small datasets.

4

Identifiability

The equality C = C[w,k] = K > diag(w)K is a core assumption to bridge the theoretical connection between a subunit model and the spike-triggered moments (STA & STC). In case we care
about recovering the underlying biological structure, we maybe interested to know when the solution
is unique and naively interpretable. Here we address the identifiability of the convolution decomposition of C for k and w estimation. Specifically, we briefly study the uniqueness of the form
C = K > diag(w)K for a single subunit and multiple subunits respectively. We provide the proof
for the single subunit case in the main text, and the proof for multiple subunits sharing the same
pooling weight w in the supplement.
Note that failure of identifiability only indicates that there are possible symmetries in the solution
space so that there are multiple equivalent optima, which is a question of theoretical interest, but it
holds no implications for practical performance.
4.1

Identifiability for Single Subunit Model

We will frequently make use of frequency domain representation. Let B 2 Rd⇥d denote the discrete
Fourier transform (DFT) matrix with j-th column is,
h
i>
2⇡
2⇡
2⇡
2⇡
bj = 1, e d (j 1) , e d 2(j 1) , e d 3(j 1) , . . . , e d (d 1)(j 1) .
(14)
e be a d-dimensional vector resulting from a discrete Fourier transform, that is, k
e = Bk k where
Let k
e 2 Rd be a Fourier representation of w.
Bk is a d ⇥ dk DFT matrix, and similarly w
We assume that k and w have full support in the frequency domain.
e or w
e is zero.
Assumption 1. No element in k

Theorem. Suppose Assumption 1 holds, the convolution decomposition C = K > diag(w)K is
uniquely identifiable up to shift and scale, where C 2 Rd⇥d and d = dk + dw 1.
e to be a unit vector to deal with the obvious scale invariance. First note
Proof. We fix k (and thus k)
that we can rewrite the convolution operator K using DFT matrices as,
K = B H diag(Bk k)Bw

(15)

where B 2 Rd⇥d is the DFT matrix and (·)H denotes conjugate transpose operation. Thus,
C

=

e H Bw diag(w)B H diag(k)B
e
B H diag(k)
w

H
f := Bw diag(w)Bw
Note that W
is a circulant matrix,
0
w
e1
w
ed
e2
w
e1
B w
B .
..
f := circulant(w)
.
e =B
W
.
B .
@w
e
w
e
d 1

w
ed

···
···
..
.

d 2

w
ed

Hence, we can rewrite (16) in the frequency domain as,

1

···
···

e HW
e =W
e = BCB H = diag(k)
f diag(k)
f
C

w
e3
w
e4
..
.
w
e1
w
e2

1
w
e2
w
e3 C
.. C
C
. C
w
e A

(16)

(17)

d

w
e1

ek
e H )>
(k

(18)

Since B is invertible, the uniqueness of the original C decomposition is equivalent to the uniqueness
e decomposition. The newly defined decomposition is
of C
e
C

=

f
W

4

ek
e H )> .
(k

(19)

e and {Ve , g
e and {g, g
f , k}
e}, where both {k, k}
e}
Suppose there are two distinct decompositions {W
H >
H
>
e
e
e
f
e
f
e
e ) . Since both W and V have no zero,
are unit vectors, such that C = W (kk ) = V (e
gg
f ./Ve )> 2 Rd⇥d , then we have
define the element-wise ratio R := (W
ek
eH = g
eg
eH
R k
(20)
Note that rank(R

ek
eH ) = rank(e
eH ) = 1.
k
gg

R is also a circulant matrix which can be diagonalized by DFT [23]: R = B diag (r1 , . . . , rd )B H .
Pd
We can express R as R = i=1 ri bi bH
i . Using the identity for Hadamard product that for any
vector a and b, (aaH ) (bbH ) = (a b)(a b)H , we get
R

ek
eH =
k

d
X

ri (bi bH
i )

i=1

By Lemma 1 (in the appendix), {b1

e b2
k,

ek
eH ) =
(k

d
X

ri (bi

i=1

e . . . , bd
k,

e i
k)(b

e H
k)

(21)

e is a linearly independent set.
k}

ek
eH ) = 1, ri can be non-zero at most a single i.
Therefore, to satisfy the rank constraint rank(R k
Without loss of generality, let ri 6= 0 and all other r· to be zero, then we have,
ek
eH = g
ek
eH diag(bi )H = g
eg
eH =) ri diag(bi )k
eg
eH
ri (bi bH
k
(22)
i )
⇣
⌘
e and g
e is the Fourier
e are unit vectors, ri = 1. By recognizing that diag(bi )k
Because bi , k
transform of i 1 positions shifted k, denoted as ki 1 , we have, ki 1 (ki 1 )> = gg> . Therefore,
f thus, vi 1 = w. that is, v must also
g = ki 1 . Moreover, from (20) and (22), (bi bH
Ve = W
i )
be a shifted version of w.
If restricting k and g to be unit vectors, then any solution v and g would satisfy w = vi
g = ki 1 . Therefore, the two decompositions are identical up to scale and shift.
4.2

1

and

Identifiability for Multiple Subunits Model

Multiple subunits model (with m > 1 subunits) is far more complicated to analyze due to large
degree of hidden invariances. In this study, we only provide the analysis under a specific condition
when all subunits share a common pooling weight w.
Assumption 2. All models share a common w.
We make a few additional assumptions. We would like to consider a tight parameterization where
no combination of subunits can take over another subunit’s task.
Assumption 3. K := [k1 , k2 , k3 , . . . , km ] spans an m-dimensional subspace where ki is the subunit filter for i-th subunit and K 2 Rdk ⇥m . In addition, K has orthogonal columns.
We denote K with p positions shifted along the column as Kp := [kp1 , kp2 , kp3 , . . . , kpm ]. Also, note
that trivially, m  dk < dk + dw 1 < d since dw > 1.

To allow arbitrary scale corresponding to each unit vector ki , we introduce coefficient ↵i to the i-th
subunit, thus extending (19) to,
!>
m
m
X
X
H >
H
e
e
e
e
e K
e H )>
f
f
f (KA
C=
W (↵i ki k ) = W
↵i k i k
=W
(23)
i

i=1

i

i=1

e 2 Rd⇥m is the DFT of K.
where A 2 Rm⇥m is a diagonal matrix of ↵i and K
m⇥m
i
Assumption 4. @⌦ 2 R
such that K ⌦ = P Ki , 8i, where P 2 Rdk ⇥dk is the permutation
i
j
matrix from K to K by shifting rows, namely Kj = P Ki , 8i, j, and ⌦ is a linear projection
coefficient matrix satisfying Kj = Ki ⌦.
Assumption 5. A has all positive or all negative values on the diagonal.
Given these assumptions, we establish the proposition for multiple subunits model.
e K
e H )> is
f (KA
Proposition. Under Assumptions (1-5), the convolutional decomposition C = W
uniquely identifiable up to shift and scale.
The proof for the proposition and illustrations of Assumption 4-5 are in the supplement.
5

b)

0.4

true parameters
MELE
smoothMELE

0.4
0.2
0

0
0

10

20

-0.4
0

30

10

20

exponential

MSE

quadratic
sigmoid

subunit nonlinearity

c)

run time (sec)

a)

7

3.5

0 3
10

30

output nonlinearity

0.7

0.6

0.64

0.56

0.38

0.39

0.14

0.37

0.07

0.17

0.88

0.74

0.7

0.71

0.45

0.39

0.4

0.43

4

10

5

10

0.05

0.09

smoothLS

5

10

soft-rectifier

0.76

sample size

4

10

sample size

1.13

0.03 3
10

smoothLS
smoothMELE
smoothMLE

0.14

smoothMELE

smoothMLE

Figure 2: a) True parameters and MELE and smoothMELE estimations. b) Speed performance for
smoothLS, smoothMELE and smoothMLE. The slightly decreasing running time along with a larger
size is resulted from a more and more fully supported subspace, which makes optimization require
fewer iterations. c) Accuracy performance for all combinations of subunit and output nonlinearities
for smoothLS, smoothMELE and smoothMLE. Top left is the subunit model matching the data;
others are model mismatch.

5

Experiments

5.1 Initialization
All three estimators are non-convex and contain many local optima, thus the selection of model
initialization would affect the optimization substantially. Similar to [12] using ‘convolutional STC’
for initialization, we also use a simple moment based method with some assumptions. For simplicity,
we assume all subunit models sharing the same w with different scaling factors as in eq. (23). Our
initializer is generated from a shallow bilinear regression. Firstly, initialize w with a wide Gaussian
e K
e H from element-wise division of Cmele by W
f . Secondly, use SVD
profile, then estimate KA
H
e K
e into an orthogonal base set K
e and a positive diagonal matrix A, where K
e
to decompose KA
and A contain information about ki ’s and ↵’s respectively, hypothesizing that k’s are orthogonal to
each other and ↵’s are all positive (Assumptions 3 and 5). Based on the ki ’s and ↵i ’s we estimated
from the rough Gaussian profile of w, now we fix those and re-estimate w with the same elementf . This bilinear iterative procedure proceeds only a few times in order to avoid
wise division for W
overfitting to Cmele which is a coarse estimate of C.
5.2

Smoothing prior

Neural receptive fields are generally smooth, thus a prior smoothing out high frequency fluctuations
would improve the performance of estimators, unless the data likelihood provides sufficient evidence
for jaggedness. We apply automatic smoothness determination (ASD [24]) to both w and k, each
with an associated balancing hyper parameter w and k . Assuming w ⇠ N (0, Cw ) with
✓
◆
k k2
Cw = exp
⇢w
(24)
2
2 w
2
where
is the vector of differences between neighboring locations in w. ⇢w and w
are variance
and length scale of Cw that belong to the hyper parameter set. k also has the same ASD prior with
hyper parameters ⇢k and k2 . For multiple subunits, each wi and ki would have its own ASD prior.

6

smoothLS(#2)
smoothMELE(#1)
smoothMELE(#2)

goodness-of-fit
(nats/spk)

low−rank,
smooth GQM
smoothLS(#1)

0

smoothMLE(#1)
smoothMLE(#2)

−1

0

−0.06

−2

−0.12

−0.18
4

−3

10

4

10

5

speed
250
200
150
100
50

10

5

training size

running time (sec)

performance

low−rank, smooth,
expected GQM

10

10

4

5

10

training size

Figure 3: Goodness-of-model fits from various estimators and their running speeds (without GQM
comparisons). Black curves are regularized GQM (with and without expected log-likelihood trick);
blue is smooth LS; green is smooth MELE; red is smooth MLE. All the subunit estimators have
results for 1 subunit and 2 subunits. The inset figure in performance is the enlarged view for large
goodness-of-fit values. The right figure is the speed result showing that MLE-based methods require
exponentially increasing running time when increasing the training size, but our moment-based ones
have quite consistent speed.
Fig. 2a shows the true w and k and the estimations from MELE and smoothMELE (MELE with
smoothing prior). From now on, we use smoothing prior by default.
5.3

Simulations

To illustrate the performance of our moment-based estimators, we generated Gaussian stimuli from
an LNP neuron with exponentiated-quadratic nonlinearity and 1 subunit model with 8-element filter
k and 33-element pooling weights w. Mean firing rate is 0.91 spk/s. In our estimation, each time
bin stimulus with 40 dimensions is treated as one sample to generate spike response. Fig. 2 b and c
show the speed and accuracy performance of three estimators LS, MELE and MLE (with smoothing
prior). LS and MELE are comparable with baseline MLE in terms of accuracy but are exponentially
faster.
Although LNP with exponential nonlinearity has been widely adapted in neuroscience for its simplicity, the actual nonlinearity of neural systems is often sub-exponential, such as soft-rectifier nonlinearity. But exponential is favored as a convenient approximation of soft-rectifier within a small
regime around the origin. Also generally, LNP neuron leans towards sigmoid subunit nonlinearity
rather than quadratic. Quadratic could well approximate a sigmoid within a small nonlinear regime
before the linear regime of the sigmoid. Therefore, in order to check the generalization performance
of LS and MELE on mismatch models, we stimulated data from a neuron with sigmoid subunit nonlinearity or soft-rectifier output nonlinearity as shown in Fig. 2c. All the full MLEs formulated with
no model mismatch provide a baseline for inspecting the performance of the ELL methods. Despite
the model-mismatch, our estimators (LS and MELE) are on par with MLE when the subunit nonlinearity is quadratic, but the performance is notably worse for the sigmoid nonlinearity. Even so, in
real applications, we will explore fits with different subunit nonlinearities using full MLE, where the
exponential and quadratic assumption is thus primarily useful for a reasonable and extremely fast
initializer. Moreover, the running time for moment-based estimators is always exponentially faster.
5.4

Application to neural data

In order to show the predictive performance more comprehensively in real neural dataset, we applied
LS, MELE and MLE estimators to data from a population of 57 V1 simple and complex cells (data
published in [11]). The stimulus consisted of oriented binary white noise (“flickering bar”) aligned
with the cell’s preferred orientation. The size of receptive field was chosen to be # of bars d ⇥10
time bins, yielding a 10d-dimensional stimulus space. The time bin size is 10 ms and the number of
bars (d) is 16 in our experiment.
We compared moment-based estimators and MLE with smoothed low-rank expected GQM and
smoothed low-rank GQM [7, 8]. Models are trained on stimuli with size varying from 6.25 ⇥ 103
to 105 and tested on 5 ⇥ 104 samples. Each subunit filter has a length of 5. All hyper parameters
are chosen by cross validation. Fig. 3 shows that GQM is weakly better than LS but its running
time is far more than LS (data not shown). Both MELE and MLE (but not LS) outfight GQM and
7

a)

subunit #1

subunit #1

subunit #2

subunit #2

0
0.2
-0.1
0.1

-0.2
0

STA

b)

10

20

excitatory STC filters

0
0

10

20

suppressive STC filters

V1
responses

subunit
model

Figure 4: Estimating visual receptive fields from a complex cell (544l029.p21). a) k and w by
fitting smoothMELE(#2). Subunit #1 is suppressive (negative w) and #2 is excitatory (positive
w). Form the y-axis we can tell from w that both imply that middle subunits contribute more than
the ends. b) Qualitative analysis. Each image corresponds to a normalized 24 dimensions spatial
pixels (horizontal) by 10 time bins (vertical) filter. Top row: STA/STC from true data; Bottom row:
simulated response from 2-subunit MELE model given true stimuli and applied the same subspace
analysis.

expected GQM with both 1 subunit and 2 subunits. Especially the improvement is the greatest with
1 subunit, which results from the average over all simple and complex cells. Generally, the more
“complex” the cell is, the higher probability that multiple subunits would fit better. Outstandingly,
MELE outperforms others with best goodness-of-fit and flat speed curve. The goodness-of-fit is
defined to be the log-likelihood on the test set divided by spike count.
For qualitative analysis, we ran smoothMELE(#2) for a complex cell and learned the optimal subunit filters and pooling weights (Fig. 4a), and then simulated V1 response by fitting 2-subunit MELE
generative model given the optimal parameters. STA/STC analysis is applied to both neural data and
simulated V1 response data. The quality of the filters trained on 105 stimuli are qualitatively close to
that obtained by STA/STC (Fig. 4b). Subunit models can recover STA, the first six excitatory STC
filters and the last four suppressive ones but with a considerably parsimonious parameter space.

6

Conclusion

We proposed an asymptotically efficient estimator for quadratic convolutional subunit models, which
forges an important theoretical link between spike-triggered covariance analysis and nonlinear subunit models. We have shown that the proposed method works well even when the assumptions
about model specification (nonlinearity and input distribution) were violated. Our approach reduces
the difficulty of fitting subunit models because computational cost does not depend on dataset size
(beyond the cost of a single pass through the data to compute the spike-triggered moments). We
also proved conditions for identifiability of the convolutional decomposition, which reveals that for
most cases the parameters are indeed identifiable. We applied our estimators to the neural data from
macaque primary visual cortex, and showed that they outperform a highly regularized form of the
GQM and achieve similar performance to the subunit model MLE at substantially lower computational cost.

Acknowledgments
This work was supported by the Sloan Foundation (JP), McKnight Foundation (JP), Simons Global
Brain Award (JP), NSF CAREER Award IIS-1150186 (JP), and a grant from the NIH (NIMH grant
MH099611 JP). We thank N. Rust and T. Movshon for V1 data.
8

References
[1] R. R. de Ruyter van Steveninck and W. Bialek. Real-time performance of a movement-senstivive neuron
in the blowfly visual system: coding and information transmission in short spike sequences. Proc. R. Soc.
Lond. B, 234:379–414, 1988.
[2] J. Touryan, B. Lau, and Y. Dan. Isolation of relevant visual features from random stimuli for cortical
complex cells. Journal of Neuroscience, 22:10811–10818, 2002.
[3] B. Aguera y Arcas and A. L. Fairhall. What causes a neuron to spike? Neural Computation, 15(8):1789–
1807, 2003.
[4] Tatyana Sharpee, Nicole C. Rust, and William Bialek. Analyzing neural responses to natural signals:
maximally informative dimensions. Neural Comput, 16(2):223–250, Feb 2004.
[5] O. Schwartz, J. W. Pillow, N. C. Rust, and E. P. Simoncelli. Spike-triggered neural characterization.
Journal of Vision, 6(4):484–507, 7 2006.
[6] J. W. Pillow and E. P. Simoncelli. Dimensionality reduction in neural models: An information-theoretic
generalization of spike-triggered average and covariance analysis. Journal of Vision, 6(4):414–428, 4
2006.
[7] Il Memming Park and Jonathan W. Pillow. Bayesian spike-triggered covariance analysis. In J. ShaweTaylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 1692–1700, 2011.
[8] Il M. Park, Evan W. Archer, Nicholas Priebe, and Jonathan W. Pillow. Spectral methods for neural characterization using generalized quadratic models. In Advances in Neural Information Processing Systems
26, pages 2454–2462, 2013.
[9] Ross S. Williamson, Maneesh Sahani, and Jonathan W. Pillow. The equivalence of information-theoretic
and likelihood-based methods for neural dimensionality reduction. PLoS Comput Biol, 11(4):e1004141,
04 2015.
[10] Kanaka Rajan, Olivier Marre, and Gašper Tkačik. Learning quadratic receptive fields from neural responses to natural stimuli. Neural Computation, 25(7):1661–1692, 2013/06/19 2013.
[11] Nicole C. Rust, Odelia Schwartz, J. Anthony Movshon, and Eero P. Simoncelli. Spatiotemporal elements
of macaque v1 receptive fields. Neuron, 46(6):945–956, Jun 2005.
[12] B Vintch, A Zaharia, J A Movshon, and E P Simoncelli. Efficient and direct estimation of a neural
subunit model for sensory coding. In Adv. Neural Information Processing Systems (NIPS*12), volume 25,
Cambridge, MA, 2012. MIT Press. To be presented at Neural Information Processing Systems 25, Dec
2012.
[13] Brett Vintch, Andrew Zaharia, J Movshon, and Eero P Simoncelli. A convolutional subunit model for
neuronal responses in macaque v1. J. Neursoci, page in press, 2015.
[14] HB Barlow and W Ro Levick. The mechanism of directionally selective units in rabbit’s retina. The
Journal of physiology, 178(3):477, 1965.
[15] S. Hochstein and R. Shapley. Linear and nonlinear spatial subunits in y cat retinal ganglion cells. J.
Physiol., 262:265–284, 1976.
[16] Jonathan B Demb, Kareem Zaghloul, Loren Haarsma, and Peter Sterling. Bipolar cells contribute to
nonlinear spatial summation in the brisk-transient (y) ganglion cell in mammalian retina. The Journal of
neuroscience, 21(19):7447–7454, 2001.
[17] Joanna D Crook, Beth B Peterson, Orin S Packer, Farrel R Robinson, John B Troy, and Dennis M Dacey.
Y-cell receptive field and collicular projection of parasol ganglion cells in macaque monkey retina. The
Journal of neuroscience, 28(44):11277–11291, 2008.
[18] PX Joris, CE Schreiner, and A Rees. Neural processing of amplitude-modulated sounds. Physiological
reviews, 84(2):541–577, 2004.
[19] Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern
recognition unaffected by shift in position. Biological cybernetics, 36(4):193–202, 1980.
[20] T. Serre, L. Wolf, S. Bileschi, M. Riesenhuber, and T. Poggio. Robust object recognition with cortex-like
mechanisms. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(3):411–426, 2007.
[21] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
[22] AlexandroD. Ramirez and Liam Paninski. Fast inference in generalized linear models via expected loglikelihoods. Journal of Computational Neuroscience, pages 1–20, 2013.
[23] Philip J Davis. Circulant matrices. American Mathematical Soc., 1979.
[24] M. Sahani and J. Linden. Evidence optimization techniques for estimating stimulus-response functions.
NIPS, 15, 2003.

9

