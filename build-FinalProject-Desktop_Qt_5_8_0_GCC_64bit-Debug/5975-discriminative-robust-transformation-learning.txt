Discriminative Robust Transformation Learning

Jiaji Huang

Qiang Qiu

Guillermo Sapiro

Robert Calderbank

Department of Electrical Engineering, Duke University
Durham, NC 27708
{jiaji.huang,qiang.qiu,guillermo.sapiro,robert.calderbank}@duke.edu

Abstract
This paper proposes a framework for learning features that are robust to data variation, which is particularly important when only a limited number of training
samples are available. The framework makes it possible to tradeoff the discriminative value of learned features against the generalization error of the learning
algorithm. Robustness is achieved by encouraging the transform that maps data
to features to be a local isometry. This geometric property is shown to improve
(K, )-robustness, thereby providing theoretical justification for reductions in generalization error observed in experiments. The proposed optimization framework
is used to train standard learning algorithms such as deep neural networks. Experimental results obtained on benchmark datasets, such as labeled faces in the wild,
demonstrate the value of being able to balance discrimination and robustness.

1

Introduction

Learning features that are able to discriminate is a classical problem in data analysis. The basic idea
is to reduce the variance within a class while increasing it between classes. One way to implement
this is by regularizing a certain measure of the variance, while assuming some prior knowledge
about the data. For example, Linear Discriminant Analysis (LDA) [4] measures sample covariance
and implicitly assumes that each class is Gaussian distributed. The Low Rank Transform (LRT) [10],
instead uses nuclear norm to measure the variance and assumes that each class is near a low-rank
subspace. A different approach is to regularize the pairwise distances between data points. Examples
include the seminal work on metric learning [17] and its extensions [5, 6, 16].
While great attention has been paid to designing objectives to encourage discrimination, less effort
has been made in understanding and encouraging robustness to data variation, which is especially
important when a limited number of training samples are available. One exception is [19], which
promotes robustness by regularizing the traditional metric learning objective using prior knowledge
from an auxiliary unlabeled dataset.
In this paper we develop a general framework for balancing discrimination and robustness. Robustness is achieved by encouraging the learned data-to-features transform to be locally an isometry
within each class. We theoretically justify this approach using (K, )-robustness [1, 18] and give an
example of the proposed formulation, incorporating it in deep neural networks. Experiments validate the capability to trade-off discrimination against robustness. Our main contributions are the
following: 1) prove that locally near isometry leads to robustness; 2) propose a practical framework
that allows to robustify a wide class of learned transforms, both linear and nonlinear; 3) provide
an explicit realization of the proposed framework, achieving competitive results on difficult face
verification tasks.
The paper is organized as follows. Section 2 motivates the proposed study and proposes a general
formulation for learning a Discriminative Robust Transform (DRT). Section 3 provides a theoretical
justification for the framework by making an explicit connection to robustness. Section 4 gives a
1

specific example of DRT, denoted as Euc-DRT. Section 5 provides experimental validation of EucDRT, and section 6 presents conclusions. 1

2

Problem Formulation

Consider an L-way classification problem. The training set is denoted by T = {(xi , yi )}, where
xi ‚àà Rn is the data and yi ‚àà {1, . . . , L} is the class label. We want to learn a feature transform
fŒ± (¬∑) such that a datum x becomes more discriminative when it is transformed to feature fŒ± (x).
The transform fŒ± is parametrized by a vector Œ±, a framework that includes linear transforms and
neural networks where the entries of Œ± are the learned network parameters.
2.1

Motivation

The transform fŒ± promotes discriminability by reducing intra-class variance and enlarging interclass variance. This aim is expressed in the design of objective functions [5, 10] or the structure
of the transform fŒ± [7, 11]. However the robustness of the learned transform is an important issue
that is often overlooked. When training samples are scarce, statistical learning theory [15] predicts
overfitting to the training data. The result of overfitting is that discrimination achieved on test data
will be significantly worse than that on training data. Our aim in this paper is the design of robust
transforms fŒ± for which the training-to-testing degradation is small [18].
We formally measure robustness of the learned transform fŒ± in terms of (K, )-robustness [1].
Given a distance metric œÅ, a learning algorithm is said to be (K, )-robust if the input data space
can be partitioned into K disjoint sets Sk , k = 1, ..., K, such that for all training sets T , the learned
parameter Œ±T determines a loss for which the value on pairs of training samples taken from different
sets Sj and Sk is very close to the value of any pair of data samples taken from Sj and Sk .
(K, )-robustness is illustrated in Fig. 1, where S1 and S2 are both of diameter Œ≥ and
|e ‚àí e0 | = |œÅ(fŒ± (x1 ), fŒ± (x2 )) ‚àí œÅ(fŒ± (x01 ), fŒ± (x02 ))|.
If the transform fŒ± preserves all distances within S1 and S2 , then |e ‚àí e0 | cannot deviate much from
|d ‚àí d0 | ‚â§ 2Œ≥.

Figure 1: (K, )-robustness: Here d = œÅ(x1 , x2 ), d0 = œÅ(x01 , x02 ), e = œÅ(fŒ± (x1 ), fŒ± (x2 )), and
e0 = œÅ(fŒ± (x01 ), fŒ± (x02 )). The difference |e ‚àí e0 | cannot deviate too much from |d ‚àí d0 |.
2.2

Formulation and Discussion

Motivated bythe above reasoning, we now present our proposed framework. First we define a pair
1
if yi = yj
label `i,j ,
. Given a metric œÅ, we use the following hinge loss to encourage
‚àí1 otherwise
high inter-class distance and small intra-class distance.
1 X
max {0, `i,j [œÅ (fŒ± (xi ), fŒ± (xj )) ‚àí t(`i,j )]} ,
(1)
|P|
i,j‚ààP

Here P = {(i, j|i 6= j)} is the set of all data pairs. t(`i,j ) ‚â• 0 is a function of `i,j and t(1) < t(‚àí1).
Similar to metric learning [17], this loss function connects pairwise distance to discrimination. However traditional metric learning typically assumes squared Euclidean distance and here the metric œÅ
can be arbitrary.
For robustness, as discussed above, we may want fŒ± (¬∑) to be distance-preserving within each small
local region. In particular, we define the set of all local neighborhoods as
N B , {(i, j)|`i,j = 1, œÅ(xi , xj ) ‚â§ Œ≥} .
1
A note on the notations: matrices (vectors) are denoted in upper (lower) case bold letters. Scalars are
denoted in plain letters.

2

Therefore, we minimize the following objective function
X
1
|œÅ(fŒ± (xi ), fŒ± (xj )) ‚àí œÅ(xi , xj )| .
|N B|

(2)

(i,j)‚ààN B

Note that we do not need to have the same metric in both the input and the feature space, they do not
even have in general the same dimension. With a slight abuse of notation we use the same symbol
to denote both metrics.
To achieve discrimination and robustness simultaneously, we formulate the objective function as a
weighted linear combination of the two extreme cases in (1) and (2)
Œª X
1‚àíŒª X
max {0, `i,j [œÅ (fŒ± (xi ), fŒ± (xj )) ‚àí t(`i,j )]}+
|œÅ(fŒ± (xi ), fŒ± (xj )) ‚àí œÅ(xi , xj )|
|P|
|N B|
i,j‚ààP

(i,j)‚ààN B

(3)
where Œª ‚àà [0, 1]. The formulation (3) balances discrimination and robustness. When Œª = 1 it seeks
discrimination, and as Œª decreases it starts to encourage robustness. We shall refer to a transform
that is learned by solving (3) as a Discriminative Robust Transform (DRT). The DRT framework
provides opportunity to select both the distance measure and the transform family.

3

Theoretical Analysis

In this section, we provide a theoretical explanation for robustness. In particular, we show that if the
solution to (1) yields a transform fŒ± that is locally a near isometry, then fŒ± is robust.
3.1

Theoretical Framework

Let X denote the original data, let Y = {1, ..., L} denote the set of class labels, and let Z = X √ó Y.
The training samples are pairs zi = (xi , yi ), i = 1, . . . , n drawn from some unknown distribution
D defined on Z. The indicator function is defined as `i,j = 1 if yi = yj and ‚àí1 otherwise. Let
fŒ± be a transform that maps a low-level feature x to a more discriminative feature fŒ± (x), and let F
denote the space of transformed features.
For simplicity we consider an arbitrary metric œÅ defined on both X and F (the general case of
different metrics is a straightforward extension), and a loss function g(œÅ(fŒ± (xi ), fŒ± (xj )), `i,j ) that
encourages œÅ(fŒ± (xi ), fŒ± (xj )) to be small (big) if `i,j = 1 (‚àí1). We shall require the Lipschtiz
constant of g(¬∑, 1) and g(¬∑, ‚àí1) to be upper bounded by A > 0. Note that the loss function in Eq. (1)
has a Lipschtiz constant of 1. We abbreviate
g(œÅ(fŒ± (xi ), fŒ± (xj )), `i,j ) , hŒ± (zi , zj ).
The empirical loss on the training set is a function of Œ± given by
Pn
2
Remp (Œ±) , n(n‚àí1)
i,j=1 hŒ± (zi , zj ),

(4)

i6=j

and the expected loss on the test data is given by
R(Œ±) , Ez01 ,z02 ‚àºD [hŒ± (z01 , z02 )] .
The algorithm operates on pairs of training samples and finds parameters
Œ±T , arg min Remp (Œ±),

(5)
(6)

Œ±

that minimize the empirical loss on the training set T . The difference Remp ‚àí R between expected
loss on the test data and empirical loss on the training data is the generalization error of the algorithm.
3.2

(K, )-robustness and Covering Number

We work with the following definition of (K, )-robustness [1].
Definition 1. A learning algorithm is (K, )-robust if Z = X √óY can be partitioned into K disjoint
sets Zk , k = 1, . . . , K such that for all training sets T ‚àà Z n , the learned parameter Œ±T determines
a loss function where the value on pairs of training samples taken from sets Zp and Zq is ‚Äúvery
close‚Äù to the value of any pair of data samples taken from Zp and Zq . Formally,
assume zi , zj ‚àà T , with zi ‚àà Zp and zj ‚àà Zq , if z0i ‚àà Zp and z0j ‚àà Zq , then


hŒ± (zi , zj ) ‚àí hŒ± (z0i , z0j ) ‚â§ .
T
T

3

Remark 1. (K, )-robustness means that the loss incurred by a testing pair (z0i , z0j ) in Zp √ó Zq is
very close to the loss incurred by any training pair (zi , zj ) in Zp √ó Zq . It is shown in [1] that the
generalization error of (K, )-robust algorithms is bounded as
r !
K
R(Œ±T ) ‚àí Remp (Œ±T ) ‚â§  + O
.
(7)
n
Therefore the smaller , the smaller is the generalization error, and the more robust is the learning
algorithm.
Given a metric space, the covering number specifies how many balls of a given radius are needed to
cover the space. The more complex the metric space, the more balls are needed to cover it. Covering
number is formally defined as follows.
Definition 2 (Covering number). Given a metric space (S, œÅ), we say that a subset SÃÇ of S is a
Œ≥-cover of S, if for every element s ‚àà S, there exists sÃÇ ‚àà SÃÇ such that œÅ(s, sÃÇ) ‚â§ Œ≥. The Œ≥-covering
number of S is
NŒ≥ (S, œÅ) = min{|SÃÇ| : SÃÇ is a Œ≥-cover of S}.
Remark 2. The covering number is a measure of the geometric complexity of (S, œÅ). A set S with
covering number NŒ≥/2 (S, œÅ) can be partitioned into NŒ≥/2 (S, œÅ) disjoint subsets, such that any two
points within the same subset are separated by no more than Œ≥.
Lemma 1. The metric space Z = X √ó Y can be partitioned into LNŒ≥/2 (X , œÅ) subsets, denoted
as Z1 , . . . , ZLNŒ≥/2 (X ,œÅ) , such that any two points z1 , (x1 , y1 ), z2 , (x2 , y2 ) in the same subset
satisfy y1 = y2 and œÅ(x1 , x2 ) ‚â§ Œ≥.
Proof. Assuming the metric space (X , œÅ) is compact, we can partition X into NŒ≥/2 (X , œÅ) subsets,
each with diameter at most Œ≥. Since Y is a finite set of size L, we can partition Z = X √ó Y into
LNŒ≥/2 (X , œÅ) subsets with the property that two samples (x1 , y1 ), (x2 , y2 ) in the same subset satisfy
y1 = y2 and œÅ(x1 , x2 ) ‚â§ Œ≥.
It follows from Lemma 1 that we may partition X into subsets X1 , . . . , XLNŒ≥/2 (X ,œÅ) , such that pairs
of points x1 , x2 from the same subset have the same label and satisfy œÅ(xi , xj ) ‚â§ Œ≥. Before we
connect local geometry to robustness we need one more definition. We say that a learned transform
fŒ± is a Œ¥-isometry if the metric is distorted by at most Œ¥:
Definition 3 (Œ¥-isometry). Let A, B be metric spaces with metrics œÅA and œÅB . A map f : A 7‚Üí B is
a Œ¥-isometry if for any a1 , a2 ‚àà A, |œÅA (f (a1 ), f (a2 )) ‚àí œÅB (a1 , a2 )| ‚â§ Œ¥.
Theorem 1. Let fŒ± be a transform derived via Eq. (6) and let X1 , . . . , XLNŒ≥/2 (X ,œÅ) be a cover of
X as described above. If fŒ± is a Œ¥-isometry, then it is (LNŒ≥/2 (X , œÅ), 2A(Œ≥ + Œ¥))-robust.
Proof sketch. Consider training samples zi , zj and testing samples z0i , z0j such that zi , z0i ‚àà Zp and
zj , z0j ‚àà Zq for some p, q ‚àà {1, . . . , LNŒ≥/2 (X , œÅ)}. Then by Lemma 1,
œÅ(xi , x0i ) ‚â§ Œ≥ and œÅ(xj , x0j ) ‚â§ Œ≥,

yi = yi0 and yj = yj0 ,

and xi , x0i ‚àà Xp and xj , x0j ‚àà Xq . By definition of Œ¥-isometry,
|œÅ(fŒ±T (xi ), fŒ±T (x0i )) ‚àí œÅ(xi , x0i )| ‚â§ Œ¥ and |œÅ(fŒ±T (xj ), fŒ±T (x0j )) ‚àí œÅ(xj , x0j )| ‚â§ Œ¥.
Rearranging the terms gives
œÅ(fŒ±T (xi ), fŒ±T (x0i )) ‚â§ œÅ(xi , x0i ) + Œ¥ ‚â§ Œ≥ + Œ¥ and œÅ(fŒ±T (xj ), fŒ±T (x0j )) ‚â§ œÅ(xj , x0j ) + Œ¥ ‚â§ Œ≥ + Œ¥.

Figure 2: Proof without words.
4

In order to bound the generalization error, we need to bound the difference between
œÅ(fŒ±T (xi ), fŒ±T (xj )) and œÅ(fŒ±T (x0i ), fŒ±T (x0j )). The details can be found in [9]; here we appeal to the proof schematic in Fig. 2. We need to bound |e ‚àí e0 | and it cannot exceed twice the
diameter of a local region in the transformed domain.
Robustness of the learning algorithm depends on the granularity of the cover and the degree to
which the learned transform fŒ± distorts distances between pairs of points in the same covering
subset. The subsets in the cover constitute regions where the local geometry makes it possible to
bound generalization error. It now
from [1] that the generalization error satisfies R(Œ±T ) ‚àí
qfollows


K
Remp (Œ±T ) ‚â§ 2A(Œ≥ + Œ¥) + O
n . The DRT proposed here is a particular example of a local
isometry, and Theorem 1 explains why the generalization error is smaller than that of pure metric
learning.

The transform described in [9] partitions the metric space X into exactly L subsets, one for each
class. The experiments reported in Section 5 demonstrate that the performance improvements derived from working with a finer partition can be worth the cost of learning finer grained local regions.

4

An Illustrative Realization of DRT

Having justified robustness, we now provide a realization of the proposed general DRT where the
metric œÅ is Euclidean distance. We use Gaussian random variables to initialize Œ±, then, on the
randomly transformed data, we set t(1) (t(‚àí1)) to be the average intra-class (inter-class) pairwise
distance. In all our experiments, the solution satisfied the condition t(1) < t(‚àí1) required in Eq. (1).
We calculate the diameter Œ≥ of the local regions N B indirectly, using the Œ∫-nearest neighbors of each
training sample to define a local neighborhood. We leave the question of how best to initialize the
indicator t and the diameter Œ≥ for future research.
We denote this particular example as Euc-DRT and use gradient descent to solve for Œ±. Denoting
the objective by J, we define yi , fŒ± (xi ), Œ¥i,j , fŒ± (xi ) ‚àí fŒ± (xj ), and œÅ0i.j , kxi ‚àí xj k. Then
X
X 1‚àíŒª
‚àÇJ
Œª
Œ¥i,j
Œ¥i,j
=
¬∑ `i,j ¬∑
+
¬∑ sgn(kŒ¥i,j k ‚àí œÅ0i,j ) ¬∑
. (8)
‚àÇyi
|P|
kŒ¥i,j k
|N B|
kŒ¥i,j k
(i,j)‚ààP
`i,j (kŒ¥i,j k‚àít(`i,j ))>0

(i,j)‚ààN B

In general, fŒ± defines a D-layer neural network (when D = 1 it defines a linear transform). Let Œ±(d)
(D)
be the linear weights at the d-th layer, and let x(d) be the output of the d-th layer, so that yi = xi .
Then the gradients are computed as,
(d+1)
(d)
X ‚àÇJ
X ‚àÇJ
‚àÇJ
‚àÇyi
‚àÇxi
‚àÇxi
‚àÇJ
¬∑
¬∑
¬∑
=
,
and
=
for 1 ‚â§ d ‚â§ D ‚àí1. (9)
(d+1)
(d)
‚àÇyi ‚àÇŒ±(D)
‚àÇŒ±(D)
‚àÇŒ±(d)
‚àÇŒ±(d)
‚àÇx
i
i ‚àÇx
i

i

Algorithm 1 provides a summary, and we note that the extension to stochastic training using minbatches is straightforward.

5

Experimental Results

In this section we report on experiments that confirm robustness of Euc-DRT. Recall that empirical
loss is given by Eq. (4) where Œ± is learned as Œ±T from the training set T , and |T | = N . The
generalization error is R ‚àí Remp where the expected loss R is estimated using a large test set.
5.1

Toy Example

This illustrative example is motivated by the discussion in Section 2.1. We first generate a 2D
dataset consisting of two noisy half-moons, then use a random 100 √ó 2 matrix to embed the data
in a 100-dimensional space. We learn a linear transform fŒ± that maps the 100 dimensional data to
2 dimensional features, and we use Œ∫ = 5 nearest neighbors to construct the set N B. We consider
Œª = 1, 0.5, 0.25, representing the most discriminative, balanced, and more robust scenarios.
When Œª = 1 the transformed training samples are rather discriminative (Fig. 3a), but when the
transform is applied to testing data, the two classes are more mixed (Fig. 3d). When Œª = 0.5, the
5

Algorithm 1 Gradient descent solver for Euc-DRT
Input: Œª ‚àà [0, 1], training pairs {(xi , xj , `i,j )}, a pre-defined D-layer network (D = 1 as linear
transform), stepsize Œ∑, neighborhood size Œ∫.
Output: Œ±
1: Randomly initialize Œ±, compute yi = fŒ± (xi ).
2: On the yi , compute the average intra and inter-class pairwise distances, assign to t(1), t(‚àí1)
3: For each training datum, find its Œ∫ nearest neighbor and define the set N B.
4: while stable objective not achieved do
5:
Compute yi = fŒ± (xi ) by a forward pass.
6:
Compute objective J.
‚àÇJ
7:
Compute ‚àÇy
as Eq. (8).
i
8:
for l = D down to 1 do
‚àÇJ
9:
Compute ‚àÇŒ±
(d) as Eq. (9).
‚àÇJ
(d)
(d)
10:
Œ± ‚Üê Œ± ‚àí Œ∑ ‚àÇŒ±
(d) .
11:
end for
12: end while

30

30

30

20

20

20

10

10

10

0

0

0

-10

-10

-10

-20

-20

-20

-30

-30
-20

0

20

-30
-20

0

20

-20

0

20

(a) Œª = 1 Transformed training (b) Œª = 0.5 transformed training (c) Œª = 0.25 Transformed trainsamples. (discriminative case)
samples. (balanced case)
ing samples. (robust case)
30

30

30

20

20

20

10

10

10

0

0

0

-10

-10

-10

-20

-20

-20

-30

-30
-20

0

20

-30
-20

0

20

-20

0

20

(d) Œª = 1 Transformed testing (e) Œª = 0.5 transformed testing (f) Œª = 0.25 Transformed testing
samples. (discriminative case)
samples. (balanced case)
samples. (robust case)

Figure 3: Original and transformed training/testing samples embedded in 2-dimensional space with
different colors representing different classes.

transformed training data are more dispersed within each class (Fig. 3b), hence less easily separated
than when Œª = 1. However Fig. 3e shows that it is easier to separate the two classes on the test data.
When Œª = 0.25, robustness is preferred to discriminative power as shown in Figs. 3c and 3f.
Tab. 1 quantifies empirical loss Remp , generalization error, and classification performance (by 1-nn)
for Œª = 1, 0.5 and 0.25. As Œª decreases, Remp increases, indicating loss of discrimination on the
training set. However, generalization error decreases, implying more robustness. We conclude that
by varying Œª, we can balance discrimination and robustness.
5.2

MNIST Classfication Using a Very Small Training Set

The transform fŒ± learned in the previous section was linear, and we now apply a more sophisticated
convolutional neural network to the MNIST dataset. The network structure is similar to LeNet, and is
6

Table 1: Varying Œª on a toy dataset.
Œª
Remp
generalization error
1-nn accuracy
(original data 93.35%)

1
1.5983
10.5855

0.5
1.6025
9.5071

0.25
1.9439
8.8040

92.20%

98.30%

91.55%

Table 3: Implementation details of the neural network for
MNIST classification.
name
conv1
pool1

Table 2: Classification error on MNIST.
Training/class
original pixels
LeNet
DML
Euc-DRT

30
81.91%
87.51%
92.32%
94.14%

50
86.18%
89.89%
94.45%
95.20%

70
86.86%
91.24%
95.67%
96.05%

conv2

100
88.49%
92.75%
96.19%
96.21%

pool2
conv3

parameters
size: 5 √ó 5 √ó 1 √ó 20
stride: 1, pad: 0
size: 2 √ó 2
size: 5 √ó 5 √ó 20 √ó 50
stride: 1, pad: 0
size: 2 √ó 2
size: 4 √ó 4 √ó 50 √ó 128
stride: 1, pad: 0

made up of alternating convolutional layers and pooling layers, with parameters detailed in Table 3.
We map the original 784-dimensional pixel values (28x28 image) to 128-dimensional features.
While state-of-art results often use the full training set (6,000 training samples per class), here we are
interested in small training sets. We use only 30 training samples per class, and we use Œ∫ = 7 nearest
neighbors to define local regions in Euc-DRT. We vary Œª and study empirical error, generalization
error, and classification accuracy (1-nn). We observe in Fig. 4 that when Œª decreases, the empirical
error also decreases, but that the generalization error actually increases. By balancing between these
two factors, a peak classification accuracy is achieved at Œª = 0.25. Next, we use 30, 50, 70, 100

4.5

0.12

4

0.08
0.06

3
2.5

0.04

2

0.02
0

94

3.5

R-R emp

R emp

0.1

94.5

1-nn accuracy(%)

0.14

0

0.25

0.5

0.75

1

1.5

0

0.25

0.5

0.75

1

93.5
93
92.5
92

0

0.25

0.5

Œª

Œª

Œª

(a)

(b)

(c)

0.75

1

Figure 4: MNIST test: with only 30 training samples per class. We vary Œª and assess (a) Remp ; (b)
generalization error; and (c) 1-nn classification accuracy. Peak accuracy is achieved at Œª = 0.25.
training samples per class and compare the performance of Euc-DRT with LeNet and Deep Metric
Learning (DML) [7]. DML minimizes a hinge loss on the squared Euclidean distances. It shares the
same spirit with our Euc-DRT using Œª = 1. All methods use the same network structure, Tab. 3, to
map to the features. For classification, LeNet uses a linear softmax classifier on top of the ‚Äúconv3‚Äù
layer and minimizes the standard cross-entropy loss during training. DML and Euc-DRT both use
a 1-nn classifier on the learned features. Classification accuracies are reported in Tab. 2. In Tab. 2,
we see that all the learned features improve upon the original ones. DML is very discriminative
and achieves higher accuracy than LeNet. However, when the training set is very small, robustness
becomes more important and Euc-DRT significantly outperforms DML.
5.3

Face Verification on LFW

We now present face verification on the more challenging Labeled Faces in the Wild (LFW) benchmark, where our experiments will show that there is an advantage to balancing disciminability and
robustness. Our goal is not to reproduce the success of deep learning in face verification [7, 14],
but to stress the importance of robust training and to compare the proposed Euc-DRT objective
with popular alternatives. Note also that it is difficult to compare with deep learning methods when
training sets are proprietary [12‚Äì14].
7

We adopt the experimental framework used in [2], and train a deep network on the WDRef dataset,
where each face is described using a high dimensional LBP feature [3] (available at 2 ) that is reduced to a 5000-dimensional feature using PCA. The WDRef dataset is significantly smaller than
the proprietary datasets typical of deep learning, such as the 4.4 million labeled faces from 4030
individuals in [14], or the 202,599 labeled faces from 10,177 individuals in [12]. It contains 2,995
subjects with about 20 samples per subject.
We compare the Euc-DRT objective with DeepFace (DF) [14] and Deep Metric Learning (DML) [7],
two state-of-the-art deep learning objectives. For a fair comparison, we employ the same network
structure and train on the same input data. DeepFace feeds the output of the last network layer to an
L-way soft-max to generate a probability distribution over L classes, then minimizes a cross entropy
loss. The Euc-DRT feature fŒ± is implemented as a two-layer fully connected network with tanh as
the squash function. Weight decay (conventional Frobenius norm regularization) is employed in
both DF and DML, and results are only reported for the best weight decay factor. After a network
is trained on WDRef, it is tested on the LFW benchmark. Verification simply consists of comparing
the cosine distance between a given pair of faces to a threshold.
Fig. 5 displays ROC curves and Table 4 reports area under the ROC curve (AUC) and verification
accuracy. High-Dim LBP refers to verification using the initial LBP features. DeepFace (DF) optimizes for a classification objective by minimizing a softmax loss, and it successfully separates
samples from different classes. However the constraint that assigns similar representations to the
same class is weak, and this is reflected in the true positive rate displayed in Fig. 5. In Deep Metric
Learning (DML) this same constraint is strong, but robustness is a concern when the training set
is small. The proposed Euc-DRT improves upon both DF and DML by balancing disciminability
and robustness. It is less conservative than DF for better discriminability, and more responsive to
local geometry than DML for smaller generalization error. Face verification accuracy for Euc-DRT
was obtained by varying the regularization parameter Œª between 0.4 and 1 (as shown in Fig 6), then
reporting the peak accuracy observed at Œª = 0.9.
92.4

verification accuracy (%)

1
0.9
0.8
0.7
HD-LBP
deepFace
DML
Euc-DRT

0.6
0.5

0

0.5

1

Table 4: Verification accuracy and
AUCs on LFW

92.2
92

Method

91.8
91.6
91.4
0.4

0.6

0.8

1

Œª

Figure 5: Comparison of Figure 6: Verification accuROCs for all methods
racy of Euc-DRT as Œª varies

6

HD-LBP
deepFace
DML
Euc-DRT

Accuracy
(%)
74.73
88.72
90.28
92.33

AUC
(√ó10‚àí2 )
82.22¬±1.00
95.50¬± 0.29
96.74¬±0.33
97.77¬± 0.25

Conclusion

We have proposed an optimization framework within which it is possible to tradeoff the discriminative value of learned features with robustness of the learning algorithm. Improvements to generalization error predicted by theory are observed in experiments on benchmark datasets. Future
work will investigate how to initialize and tune the optimization, also how the Euc-DRT algorithm
compares with other methods that reduce generalization error.

7

Acknowledgement

The work of Huang and Calderbank was supported by AFOSR under FA 9550-13-1-0076 and by
NGA under HM017713-1-0006. The work of Qiu and Sapiro is partially supported by NSF and
DoD.

2

http://home.ustc.edu.cn/chendong/

8

References
[1] A. Bellet and A. Habrard. Robustness and generalization for metric learning. Neurocomputing,
151(14):259‚Äì267, 2015.
[2] D. Chen, X. Cao, L. Wang, F. Wen, and J. Sun. Bayesian face revisited: A joint formulation.
In European Conference on Computer Vision (ECCV), 2012.
[3] D. Chen, X. Cao, F. Wen, and J. Sun. Blessing of dimensionality: High-dimensional feature
and its efficient compression for face verification. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2013.
[4] K. Fukunaga. Introduction to Statistical Pattern Recognition. San Diego: Academic Press,
1990.
[5] A. Globerson and S. Roweis. Metric learning by collapsing classes. In Advances in Neural
Information Processing Systems (NIPS), 2005.
[6] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In Advances in Neural Information Processing Systems (NIPS), 2004.
[7] J. Hu, J. Lu, and Y. Tan. Discriminative deep metric learning for face verification in the wild.
In Computer Vision and Pattern Recognition (CVPR), pages 1875‚Äì1882, 2014.
[8] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A
database for studying face recognition in unconstrained environments. Technical Report 0749, University of Massachusetts, Amherst, October 2007.
[9] J. Huang, Q. Qiu, R. Calderbank, and G. Sapiro. Geometry-aware deep transform. In International Conference on Computer Vision, 2015.
[10] G. Sapiro Q. Qiu. Learning transformations for clustering and classification. Journal of Machine Learning Research (JMLR), pages 187‚Äì225, 2015.
[11] C. Sumit, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to face verification. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), volume 1, pages 539‚Äì546, 2005.
[12] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning face representation by joint
identification-verification. In Advances in Neural Information Processing Systems (NIPS),
pages 1988‚Äì1996, 2014.
[13] Y. Sun, X. Wang, and X. Tang. Deep learning face representation from predicting 10,000
classes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
1891‚Äì1898, 2014.
[14] Y. Taigman, M. Yang, M. A. Ranzato, and L. Wolf. Deepface: Closing the gap to humanlevel performance in face verification. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 1701‚Äì1708, 2014.
[15] V. N. Vapnik. An overview of statistical learning theory. IEEE Transactions on Neural Networks, 10(5):988‚Äì999, 1999.
[16] K. Q. Weinberger and L. K. Saul. Distance metric learning for large margin nearest neighbor
classification. Journal of Machine Learning Research, 10:207‚Äì244, 2009.
[17] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Distance metric learning, with application
to clustering with side-information. In Advances in Neural Information Processing Systems
(NIPS), 2002.
[18] H. Xu and S. Mannor. Robustness and generalization. Machine Learning, 86(3):391‚Äì423,
2012.
[19] Z. Zha, T. Mei, M. Wang, Z. Wang, and X. Hua. Robust distance metric learning with auxiliary
knowledge. In International Joint Conference on Artificial Intelligence (IJCAI), 2009.

9

