On the consistency theory of high dimensional
variable screening

Xiangyu Wang
Dept. of Statistical Science
Duke University, USA
xw56@stat.duke.edu

Chenlei Leng
Dept. of Statistics
University of Warwick, UK
C.Leng@warwick.ac.uk

David B. Dunson
Dept. of Statistical Science
Duke University, USA
dunson@stat.duke.edu

Abstract
Variable screening is a fast dimension reduction technique for assisting high dimensional feature selection. As a preselection method, it selects a moderate size
subset of candidate variables for further refining via feature selection to produce
the final model. The performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables
without discarding the important ones. When the data dimension p is substantially
larger than the sample size n, variable screening becomes crucial as 1) Faster feature selection algorithms are needed; 2) Conditions guaranteeing selection consistency might fail to hold. This article studies a class of linear screening methods
and establishes consistency theory for this special class. In particular, we prove
the restricted diagonally dominant (RDD) condition is a necessary and sufficient
condition for strong screening consistency. As concrete examples, we show two
screening methods SIS and HOLP are both strong screening consistent (subject
to additional constraints) with large probability if n > O((ρs+σ/τ )2 log p) under
random designs. In addition, we relate the RDD condition to the irrepresentable
condition, and highlight limitations of SIS.

1

Introduction

The rapidly growing data dimension has brought new challenges to statistical variable selection, a
crucial technique for identifying important variables to facilitate interpretation and improve prediction accuracy. Recent decades have witnessed an explosion of research in variable selection and
related fields such as compressed sensing [1, 2], with a core focus on regularized methods [3–7].
Regularized methods can consistently recover the support of coefficients, i.e., the non-zero signals,
via optimizing regularized loss functions under certain conditions [8–10]. However, in the big data
era when p far exceeds n, such regularized methods might fail due to two reasons. First, the conditions that guarantee variable selection consistency for convex regularized methods such as lasso
might fail to hold when p >> n; Second, the computational expense of both convex and non-convex
regularized methods increases dramatically with large p.
Bearing these concerns in mind, [11] propose the concept of “variable screening”, a fast technique
that reduces data dimensionality from p to a size comparable to n, with all predictors having nonzero coefficients preserved. They propose a marginal correlation based fast screening technique
“Sure Independence Screening” (SIS) that can preserve signals with large probability. However,
this method relies on a strong assumption that the marginal correlations between the response and
the important predictors are high [11], which is easily violated in the practice. [12] extends the
marginal correlation to the Spearman’s rank correlation, which is shown to gain certain robustness
but is still limited by the same strong assumption. [13] and [14] take a different approach to attack
the screening problem. They both adopt variants of a forward selection type algorithm that includes
one variable at a time for constructing a candidate variable set for further refining. These methods
1

eliminate the strong marginal assumption in [11] and have been shown to achieve better empirical
performance. However, such improvement is limited by the extra computational burden caused
by their iterative framework, which is reported to be high when p is large [15]. To ameliorate
concerns in both screening performance and computational efficiency, [15] develop a new type of
screening method termed “High-dimensional ordinary least-square projection” (HOLP ). This new
screener relaxes the strong marginal assumption required by SIS and can be computed efficiently
(complexity is O(n2 p)), thus scalable to ultra-high dimensionality.
This article focuses on linear models for tractability. As computation is one vital concern for designing a good screening method, we primarily focus on a class of linear screeners that can be efficiently
computed, and study their theoretical properties. The main contributions of this article lie in three
aspects.
1. We define the notion of strong screening consistency to provide a unified framework for
analyzing screening methods. In particular, we show a necessary and sufficient condition for a screening method to be strong screening consistent is that the screening matrix
is restricted diagonally dominant (RDD). This condition gives insights into the design of
screening matrices, while providing a framework to assess the effectiveness of screening
methods.
2. We relate RDD to other existing conditions. The irrepresentable condition (IC) [8] is necessary and sufficient for sign consistency of lasso [3]. In contrast to IC that is specific to the
design matrix, RDD involves another ancillary matrix that can be chosen arbitrarily. Such
flexibility allows RDD to hold even when IC fails if the ancillary matrix is carefully chosen
(as in HOLP ). When the ancillary matrix is chosen as the design matrix, certain equivalence is shown between RDD and IC, revealing the difficulty for SIS to achieve screening
consistency. We also comment on the relationship between RDD and the restricted eigenvalue condition (REC) [6] which is commonly seen in the high dimensional literature. We
illustrate via a simple example that RDD might not be necessarily stronger than REC.
3. We study the behavior of SIS andHOLP under random designs, and prove that a sample
size of n = O (ρs + σ/τ )2 log p is sufficient for SIS and HOLP to be screening consistent, where s is the sparsity, ρ measures the diversity of signals and τ /σ evaluates the
signal-to-noise ratio. This is to be compared to the sign consistency results in [9] where the
design matrix is fixed and assumed to follow the IC.
The article is organized as follows. In Section 1, we set up the basic problem and describe the
framework of variable screening. In Section 2, we provide a deterministic necessary and sufficient
condition for consistent screening. Its relationship with the irrepresentable condition is discussed
in Section 3. In Section 4, we prove the consistency of SIS and HOLP under random designs by
showing the RDD condition is satisfied with large probability, although the requirement on SIS is
much more restictive.

2

Linear screening

Consider the usual linear regression
Y = Xβ + ,
where Y is the n × 1 response vector, X is the n × p design matrix and  is the noise. The regression
task is to learn the coefficient vector β. In the high dimensional setting where p >> n, a sparsity
assumption is often imposed on β so that only a small portion of the coordinates are non-zero. Such
an assumption splits the task of learning β into two phases. The first is to recover the support of
β, i.e., the location of non-zero coefficients; The second is to estimate the value of these non-zero
signals. This article mainly focuses on the first phase.
As pointed out in the introduction, when the dimensionality is too high, using regularization methods
methods raises concerns both computationally and theoretically. To reduce the dimensionality, [11]
suggest a variable screening framework by finding a submodel
Md = {i : |β̂i | is among the largest d coordinates of |β̂|} or
2

Mγ = {i : |β̂i | > γ}.

Let Q = {1, 2, · · · , p} and define S as the true model with s = |S| being its cardinarlity. The
hope is that the submodel size |Md | or |Mγ | will be smaller or comparable to n, while S ⊆ Md
or S ⊆ Mγ . To achieve this goal two steps are usually involved in the screening analysis. The
first is to show there exists some γ such that mini∈S |β̂i | > γ and the second step is to bound the
size of |Mγ | such that |Mγ | = O(n). To unify these steps for a more comprehensive theoretical
framework, we put forward a slightly stronger definition of screening consistency in this article.
Definition 2.1. (Strong screening consistency) An estimator β̂ (of β) is strong screening consistent
if it satisfies that
min |β̂i | > max |β̂i |

(1)

i6∈S

i∈S

and
sign(β̂i ) = sign(βi ), ∀i ∈ S.
(2)
Remark 2.1. This definition does not differ much from the usual screening property studied in the
(n−s)
literature, which requires mini∈S |β̂i | > maxi6∈S |β̂i |, where max(k) denotes the k th largest item.
The key of strong screening consistency is the property (1) that requires the estimator to preserve
consistent ordering of the zero and non-zero coefficients. It is weaker than variable selection consistency in [8]. The requirement in (2) can be seen as a relaxation of the sign consistency defined in [8],
as no requirement for β̂i , i 6∈ S is needed. As shown later, such relaxation tremendously reduces the
restriction on the design matrix, and allows screening methods to work for a broader choice of X.
The focus of this article is to study the theoretical properties of a special class of screeners that take
the linear form as
β̂ = AY
for some p×n ancillary matrix A. Examples include sure independence screening (SIS) where A =
X T /n and high-dimensional ordinary least-square projection (HOLP ) where A = X T (XX T )−1 .
We choose to study the class of linear estimators because linear screening is computationally efficient and theoretically tractable. We note that the usual ordinary least-squares estimator is also a
special case of linear estimators although it is not well defined for p > n.

3

Deterministic guarantees

In this section, we derive the necessary and sufficient condition that guarantees β̂ = AY to be strong
screening consistent. The design matrix X and the error  are treated as fixed in this section and we
will investigate random designs later. We consider the set of sparse coefficient vectors defined by


maxi∈supp(β) |βi |
p
B(s, ρ) = β ∈ R : |supp(β)| ≤ s,
≤ρ .
mini∈supp(β) |βi |
The set B(s, ρ) contains vectors having at most s non-zero coordinates with the ratio of the largest
and smallest coordinate bounded by ρ. Before proceeding to the main result of this section, we
introduce some terminology that helps to establish the theory.
Definition 3.1. (restricted diagonally dominant matrix) A p × p symmetric matrix Φ is restricted
diagonally dominant with sparsity s if for any I ⊆ Q, |I| ≤ s − 1 and i ∈ Q \ I
X

X
Φii > C0 max
|Φij + Φkj |,
|Φij − Φkj | + |Φik | ∀k 6= i, k ∈ Q \ I,
j∈I

j∈I

where C0 ≥ 1 is a constant.
Notice this definition implies that for i ∈ Q \ I
X

X
X
Φii ≥ C0
|Φij + Φkj | +
|Φij − Φkj | /2 ≥ C0
|Φij |,
j∈I

j∈I

(3)

j∈I

which is related to the usual diagonally dominant matrix. The restricted diagonally dominant matrix provides a necessary and sufficient condition for any linear estimators β̂ = AY to be strong
screening consistent. More precisely, we have the following result.
3

Theorem 1. For the noiseless case where  = 0, a linear estimator β̂ = AY is strong screening
consistent for every β ∈ B(s, ρ), if and only if the screening matrix Φ = AX is restricted diagonally
dominant with sparsity s and C0 ≥ ρ.
Proof. Assume Φ is restricted diagonally dominant with sparsity s and C0 ≥ ρ. Recall β̂ = Φβ.
Suppose S is the index set of non-zero predictors. For any i ∈ S, k 6∈ S, if we let I = S \ {i}, then
we have




X βj
X βj
X βj
|β̂i | = |βi | Φii +
Φij = |βi | Φii +
(Φij + Φkj ) + Φki −
Φkj − Φki
βi
βi
βi
j∈I
j∈I
j∈I
X



βj
|βi | X
> −|βi |
Φkj + Φki = −
βj Φkj + βi Φki = −sign(βi ) · β̂k ,
βi
βi
j∈I

j∈I

and




X βj
X βj
X βj
|β̂i | = |βi | Φii +
Φij = |βi | Φii +
(Φij − Φkj ) − Φki +
Φkj + Φki
βi
βi
βi
j∈I
j∈I
j∈I
X

βj
> |βi |
Φkj + Φki = sign(βi ) · β̂k .
βi
j∈I

Therefore, whatever value sign(βi ) is, it always holds that |β̂i | > |β̂k | and thus mini∈S |β̂i | >
maxk6∈S |β̂k |.
To prove the sign consistency for non-zero coefficients, we notice that for i ∈ S,


X
X βj
Φij > 0.
β̂i βi = Φii βi2 +
Φij βj βi = βi2 Φii +
βi
j∈I

j∈I

The proof of necessity is left to the supplementary materials.

The noiseless case is a good starting point to analyze β̂. Intuitively, in order to preserve the correct
order of the coefficients in β̂ = AXβ, one needs AX to be close to a diagonally dominant matrix,
so that β̂i , i ∈ MS will take advantage of the large diagonal terms in AX to dominate β̂i , i 6∈ MS
that is just linear combinations of off-diagonal terms.
When noise is considered, the condition in Theorem 1 needs to be changed slightly to accommodate
extra discrepancies. In addition, the smallest non-zero coefficient has to be lower bounded to ensure
a certain level of signal-to-noise ratio. Thus, we augment our previous definition of B(s, ρ) to have
a signal strength control
Bτ (s, ρ) = {β ∈ B(s, ρ)|

min
i∈supp(β)

|βi | ≥ τ }.

Then we can obtain the following modified Theorem.
Theorem 2. With noise, the linear estimator β̂ = AY is strong screening consistent for every
β ∈ Bτ (s, ρ) if Φ = AX − 2τ −1 kAk∞ Ip is restricted diagonally dominant with sparsity s and
C0 ≥ ρ.
The proof of Theorem 2 is essentially the same as Theorem 1 and is thus left to the supplementary
materials. The condition in Theorem 2 can be further tailored to a necessary and sufficient version
with extra manipulation on the noise term. Nevertheless, this might not be useful in practice due to
the randomness in noise. In addition, the current version of Theorem 2 is already tight in the sense
that there exists some noise vector  such that the condition in Theorem 2 is also necessary for strong
screening consistency.
Theorems 1 and 2 establish ground rules for verifying consistency of a given screener and provide
practical guidance for screening design. In Section 4, we consider some concrete examples of ancillary matrix A and prove that conditions in Theorems 1 and 2 are satisfied by the corresponding
screeners with large probability under random designs.
4

4

Relationship with other conditions

For some special cases such sure independence screening (”SIS”), the restricted diagonally dominant
(RDD) condition is related to the strong irrepresentable condition (IC) proposed in [8]. Assume each
column of X is standardized to have mean zero. Letting C = X T X/n and β be a given coefficient
vector, the IC is expressed as
−1
kCS c ,S CS,S
· sign(βS )k∞ ≤ 1 − θ
(4)
for some θ > 0, where CA,B represents the sub-matrix of C with row indices in A and column
indices in B. The authors enumerate several scenarios of C such that IC is satisfied. We verify some
of these scenarios for screening matrix Φ.
Corollary 1. If Φii = 1, ∀i and |Φij | < c/(2s), ∀i 6= j for some 0 ≤ c < 1 as defined in Corollary
1 and 2 in [8], then Φ is a restricted diagonally dominant matrix with sparsity s and C0 ≥ 1/c.
If |Φij | < r|i−j| , ∀i, j for some 0 < r < 1 as defined in Corollary 3 in [8], then Φ is a restricted
diagonally dominant matrix with sparsity s and C0 ≥ (1 − r)2 /(4r).
A more explicit but nontrivial relationship between IC and RDD is illustrated below when |S| = 2.
Theorem 3. Assume Φii = 1, ∀i and |Φij | < r, ∀i 6= j. If Φ is restricted diagonally dominant with
sparsity 2 and C0 ≥ ρ, then Φ satisfies
ρ−1
kΦS c ,S Φ−1
S,S · sign(βS )k∞ ≤
1−r
for all β ∈ B(2, ρ). On the other hand, if Φ satisfies the IC for all β ∈ B(2, ρ) for some θ, then Φ is
a restricted diagonally dominant matrix with sparsity 2 and
1 1−r
.
C0 ≥
1−θ1+r
Theorem 3 demonstrates certain equivalence between IC and RDD. However, it does not mean
that RDD is also a strong requirement. Notice that IC is directly imposed on the covariance matrix
X T X/n. This makes IC a strong assumption that is easily violated; for example, when the predictors
are highly correlated. In contrast to IC, RDD is imposed on matrix AX where there is flexibility in
choosing A. Only when A is chose to be X/n, RDD is equivalently strong as IC, as shown in next
theorem. For other choices of A, such as HOLP defined in next section, the estimator satisfies RDD
even when predictors are highly correlated. Therefore, RDD is considered as weak requirement.
For ”SIS”, the screening matrix Φ = X T X/n coincides with the covariance matrix, making RDD
and IC effectively equivalent. The following theorem formalizes this.
Theorem 4. Let A = X T /n and standardize columns of X to have sample variance one. Assume
X satisfies the sparse Riesz condition [16], i.e,
min λmin (XπT Xπ /n) ≥ µ,
π⊆Q, |π|≤s

for some
√ µ > 0. Now if AX is restricted diagonally dominant with sparsity s + 1 and C0 ≥ ρ with
ρ > s/µ, then X satisfies the IC for any β ∈ B(s, ρ).
√
In other words, under the condition ρ > s/µ, the strong screening consistency of SIS for B(s +
1, ρ) implies the model selection consistency of lasso for B(s, ρ).
Theorem 4 illustrates the difficulty of SIS. The necessary condition that guarantees good screening
performance of SIS also guarantees the model selection consistency of lasso. However, such a
strong necessary condition does not mean that SIS should be avoided in practice given its substantial
advantages in terms of simplicity and computational efficiency. The strong screening consistency
defined in this article is stronger than conditions commonly used in justifying screening procedures
as in [11].
Another common assumption in the high dimensional literature is the restricted eigenvalue condition
(REC). Compared to REC, RDD is not necessarily stronger due to its flexibility in choosing the
ancillary matrix A. [17, 18] prove that the REC is satisfied when the design matrix is sub-Gaussian.
However, REC might not be guaranteed when the row of X follows heavy-tailed distribution. In
contrast, as the example shown in next section and in [15], by choosing A = X T (XX T )−1 , the
resulting estimator satisfies RDD even when the rows of X follow heavy-tailed distributions.
5

5

Screening under random designs

In this section, we consider linear screening under random designs when X and  are Gaussian.
The theory developed in this section can be easily extended to a broader family of distributions, for
example, where  follows a sub-Gaussian distribution [19] and X follows an elliptical distribution
[11, 15]. We focus on the Gaussian case for conciseness. Let  ∼ N (0, σ 2 ) and X ∼ N (0, Σ).
We prove the screening consistency of SIS and HOLP by verifying the condition in Theorem 2.
Recall the ancillary matrices for SIS and HOLP are defined respectively as
AHOLP = X T (XX T )−1 .

ASIS = X/n,

For simplicity, we assume Σii = 1 for i = 1, 2, · · · , p. To verify the RDD condition, it is essential
to quantify the magnitude of the entries of AX and A.
Lemma 1. Let Φ = ASIS X, then for any t > 0 and i 6= j ∈ Q, we have



 2

t n
tn
P |Φii − Σii | ≥ t ≤ 2 exp − min
,
,
8e2 K 2eK
and




 2
tn
t n
,
,
P |Φij − Σij | ≥ t ≤ 6 exp − min
72e2 K 6eK
where K = kX 2 (1) − 1kψ1 is a constant, X 2 (1) is a chi-square random variable with one degree
of freedom and the norm k · kψ1 is defined in [19].
Lemma 1 states that the screening matrix Φ = ASIS X for SIS will eventually converge to the covariance matrix Σ in l∞ when n tends to infinity and log p = o(n). Thus, the screening performance
of SIS strongly relies on the structure of Σ. In particular, the (asymptotically) necessary and sufficient condition for SIS being strong screening consistent is Σ satisfying the RDD condition. For
the noise term, we have the following lemma.
Lemma 2. Let η = ASIS . For any t > 0 and i ∈ Q, we have

 2

tn
t n
,
P (|ηi | ≥ σt) ≤ 6 exp − min
,
72e2 K 6eK
where K is defined the same as in Lemma 1.
The proof of Lemma 2 is essentially the same as the proof of off-diagonal terms in Lemma 1 and
is thus omitted. As indicated before, the necessary and sufficient condition for SIS to be strong
screening consistent is that Σ follows RDD. As RDD is usually hard to verify, we consider a stronger
sufficient condition inspired by Corollary 1.
Theorem 5. Let r = maxi6=j |Σij |. If r <

n > 144K

1
2ρs ,

then for any δ > 0, if the sample size satisfies

1 + 2ρs + 2σ/τ
1 − 2ρsr

2
log(3p/δ),

(5)

where K is defined in Lemma 1, then with probability at least 1 − δ, Φ = ASIS X −
2τ −1 kASIS k∞ Ip is restricted diagonally dominant with sparsity s and C0 ≥ ρ. In other words,
SIS is screening consistent for any β ∈ Bτ (s, ρ).
Proof. Taking union bound on the results from Lemma 1 and 2, we have for any t > 0 and p > 2,



 2

n
t
t
P min Φii ≤ 1 − t or max |Φij | ≥ r + t or kηk∞ ≥ σt ≤ 7p2 exp −
min
,
.
i∈Q
i6=j
K
72e2 6e
In other words, for any δ > 0, when n ≥ K log(7p2 /δ), with probability at least 1 − δ, we have
r
r
√
√
K log(7p2 /δ)
K log(7p2 /δ)
min Φii ≥ 1 − 6 2e
, max |Φij | ≤ r + 6 2e
,
i∈Q
i6=j
n
n
6

√
max |ηi | ≤ 6 2eσ
i∈Q

r

K log(7p2 /δ)
.
n

A sufficient condition for Φ to be restricted diagonally dominant is that
min Φii > 2ρs max |Φij | + 2τ −1 max |ηi |.
i

i

i6=j

Plugging in the values we have
r
r
r
√
√
√
K log(7p2 /δ)
K log(7p2 /δ)
K log(7p2 /δ)
−1
> 2ρs(r + 6 2e
) + 12 2eτ σ
.
1 − 6 2e
n
n
n
Solving the above inequality (notice that 7p2 /δ < 9p2 /δ 2 and ρ > 1) completes the proof.
The requirement that maxi6=j |Σij | < 1/(ρsr) or the necessary and sufficient condition that Σ is
RDD strictly constrains the correlation structure of X, causing the difficulty for SIS to be strong
screening consistent. For HOLP we instead have the following result.
Lemma 3. Let Φ = AHOLP X. Assume p > c0 n for some c0 > 1, then for any C > 0 there exists
some 0 < c1 < 1 < c2 and c3 > 0 such that for any t > 0 and any i ∈ Q, j 6= i, we have




n
n
P |Φii | < c1 κ−1
≤ 2e−Cn , P |Φii | > c2 κ
≤ 2e−Cn
p
p
and


√ 
2
n
P |Φij | > c4 κt
≤ 5e−Cn + 2e−t /2 ,
p

√
c (c −c )
where c4 = √ 2 0 1 .
c3 (c0 −1)

Proof. The proof of Lemma 3 relies heavily on previous results for the Stiefel Manifold provided in
the supplementary materials. We only sketch the basic idea here and leave the complete proof to the
supplementary materials. Defining H = X T (XX T )−1/2 , then we have Φ = HH T and H follows
the Matrix Angular Central Gaussian (MACG) with covariance Σ. The diagonal terms of HH T
can be bounded similarly via the Johnson-Lindenstrauss lemma, by using the fact that HH T =
Σ1/2 U (U T ΣU )−1 U Σ, where U is a p × n random projection matrix. Now for off-diagonal terms,
we decompose the Stiefel manifold as H = (G(H2 )H1 H2 ), where H1 is a (p − n + 1) × 1 vector,
H2 is a p × (n − 1) matrix and G(H2 ) is chosen so that (G(H2 ) H2 ) ∈ O(p), and show that H1
follows Angular Central Gaussian (ACG) distribution with covariance G(H2 )T ΣG(H2 ) conditional
(d)

on H2 . It can be shown that e2 HH T e1 = e2 G(H2 )H1 |eT1 H2 = 0. Let t21 = eT1 HH T e1 , then
eT1 H2 = 0 is equivalent to eT1 G(H2 )H1 = t1 , and we obtain the desired coupling distribution as
(d)

eT2 HH T e1 = eT2 G(H2 )H1 |eT1 G(H2 )H1 = t1 . Using the normal representation of ACG(Σ), i.e.,
if x = (x1 , · · · , xp ) ∼ N (0, Σ), then x/kxk ∼ ACG(Σ), we can write G(H2 )H1 in terms of
normal variables and then bound all terms using concentration inequalities.
Lemma 3 quantifies the entries of the screening matrix for HOLP . As illustrated in the lemma,
regardless of the covariance Σ, diagonal terms of Φ are always O( np ) and the off-diagonal terms are
√

O( pn ). Thus, with n ≥ O(s2 ), Φ is likely to satisfy the RDD condition with large probability. For
the noise vector we have the following result.
Lemma 4. Let η = AHOLP . Assume p > c0 n for some c0 > 1, then for any C > 0 there exist the
same c1 , c2 , c3 as in Lemma 3 such that for any t > 0 and i ∈ Q,

√ 
√
2
2σ c2 κt n
P |ηi | ≥
< 4e−Cn + 2e−t /2 ,
−1 p
1 − c0
if n ≥ 8C/(c0 − 1)2 .
The proof is almost identical to Lemma 2 and is provided in the supplementary materials. The
following theorem results after combining Lemma 3 and 4.
7

Theorem 6. Assume p > c0 n for some c0 > 1. For any δ > 0, if the sample size satisfies


8C
n > max 2C 0 κ4 (ρs + σ/τ )2 log(3p/δ),
,
(c0 − 1)2
where C 0 = max{

4c24
4c2
}
,
2
c21 c21 (1−c−1
0 )

(6)

and c1 , c2 , c3 , c4 , C are the same constants defined in Lemma 3,

then with probability at least 1 − δ, Φ = AHOLP X − 2τ −1 kAHOLP k∞ Ip is restricted diagonally
dominant with sparsity s and C0 ≥ ρ. This implies HOLP is screening consistent for any β ∈
Bτ (s, ρ).
Proof. Notice that if
min |Φii | > 2sρ max |Φij | + 2τ −1 kX T (XX T )−1 k∞ ,
i

ij

(7)

then the proof is complete√
because Φ − 2τ −1 kX T (XX T )−1 k∞ is already a restricted diagonally
dominant matrix. Let t = Cn/ν. The above equation then requires
√
√
√
√
2c4 Cκsρ n
2σ c2 Cκt n
2c4 Cκsρ
2σ c2 Cκ  n
−1 n
−1
c1 κ
−
−
= c1 κ −
−
> 0,
p
ν
p
ν
(1 − c−1
(1 − c−1
0 )τ ν p
0 )τ ν p
which implies that

√
√
2c4 Cκ2 ρs
2σ c2 Cκ2
= C1 κ2 ρs + C2 κ2 τ −1 σ > 1,
ν>
+
c1
c1 (1 − c−1
)τ
0
√

√

c2 C
where C1 = 2c4c1 C , C2 = c 2(1−c
−1 . Therefore, taking union bounds on all matrix entries, we
1
0 )
have



	
2
1
P (7) does not hold
< (p + 5p2 )e−Cn + 2p2 e−Cn/ν < (7 + )p2 e−Cn/ν ,
n

where the second inequality is due to the fact that p > n and ν > 1. Now for any δ > 0, (7) holds
with probability at least 1 − δ if


ν2
n≥
log(7 + 1/n) + 2 log p − log δ ,
C
√
2
which is satisfied provided (noticing 8 < 3) n ≥ 2νC log 3p
δ . Now pushing ν to the limit gives (6),
the precise condition we need.
There are several interesting observations on equation (5) and (6). First, (ρs + σ/τ )2 appears in
both expressions. We note that ρs evaluates the sparsity and the diversity of the signal β while σ/τ
is closely related to the signal-to-noise ratio. Furthermore, HOLP relaxes the correlation constraint
r < 1/(2ρs) or the covariance constraint (Σ is RDD) with the conditional number constraint. Thus
for any Σ, as long as the sample size is large enough, strong screening consistency is assured.
Finally, HOLP provides an example to satisfy the RDD condition in answer to the question raised
in Section 4.

6

Concluding remarks

This article studies and establishes a necessary and sufficient condition in the form of restricted
diagonally dominant screening matrices for strong screening consistency of a linear screener. We
verify the condition for both SIS and HOLP under random designs. In addition, we show a
close relationship between RDD and the IC, highlighting the difficulty of using SIS in screening for
arbitrarily correlated predictors. For future work, it is of interest to see how linear screening can be
adapted to compressed sensing [20] and how techniques such as preconditioning [21] can improve
the performance of marginal screening and variable selection.
Acknowledgments This research was partly support by grant NIH R01-ES017436 from the National Institute of Environmental Health Sciences.
8

References
[1] David L Donoho. Compressed sensing.
52(4):1289–1306, 2006.

IEEE Transactions on Information Theory,

[2] Richard Baraniuk. Compressive sensing. IEEE Signal Processing Magazine, 24(4), 2007.
[3] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society. Series B (Statistical Methodology), 58(1):267–288, 1996.
[4] Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its
oracle properties. Journal of the American Statistical Association, 96(456):1348–1360, 2001.
[5] Emmanuel Candes and Terence Tao. The dantzig selector: statistical estimation when p is
much larger than n. The Annals of Statistics, 35(6):2313–2351, 2007.
[6] Peter J Bickel, Ya’acov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso and
dantzig selector. The Annals of Statistics, 37(4):1705–1732, 2009.
[7] Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The
Annals of Statistics, 38(2):894–942, 2010.
[8] Peng Zhao and Bin Yu. On model selection consistency of lasso. The Journal of Machine
Learning Research, 7:2541–2563, 2006.
[9] Martin J Wainwright. Sharp thresholds for high-dimensional and noisy recovery of sparsity
using l1-constrained quadratic programming. IEEE Transactions on Information Theory, 2009.
[10] Jason D Lee, Yuekai Sun, and Jonathan E Taylor. On model selection consistency of mestimators with geometrically decomposable penalties. Advances in Neural Processing Information Systems, 2013.
[11] Jianqing Fan and Jinchi Lv. Sure independence screening for ultrahigh dimensional feature
space. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(5):849–
911, 2008.
[12] Gaorong Li, Heng Peng, Jun Zhang, Lixing Zhu, et al. Robust rank correlation based screening.
The Annals of Statistics, 40(3):1846–1877, 2012.
[13] Hansheng Wang. Forward regression for ultra-high dimensional variable screening. Journal
of the American Statistical Association, 104(488):1512–1524, 2009.
[14] Haeran Cho and Piotr Fryzlewicz. High dimensional variable selection via tilting. Journal of
the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593–622, 2012.
[15] Xiangyu Wang and Chenlei Leng. High-dimensional ordinary least-squares projection for
screening variables. https://stat.duke.edu/˜xw56/holp-paper.pdf, 2015.
[16] Cun-Hui Zhang and Jian Huang. The sparsity and bias of the lasso selection in highdimensional linear regression. The Annals of Statistics, 36(4):1567–1594, 2008.
[17] Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties for
correlated gaussian designs. The Journal of Machine Learning Research, 11:2241–2259, 2010.
[18] Shuheng Zhou. Restricted eigenvalue conditions on subgaussian random matrices. arXiv
preprint arXiv:0912.4045, 2009.
[19] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv
preprint arXiv:1011.3027, 2010.
[20] Lingzhou Xue and Hui Zou. Sure independence screening and compressed random sensing.
Biometrika, 98(2):371–380, 2011.
[21] Jinzhu Jia and Karl Rohe. Preconditioning to comply with the irrepresentable condition. arXiv
preprint arXiv:1208.5584, 2012.

9

