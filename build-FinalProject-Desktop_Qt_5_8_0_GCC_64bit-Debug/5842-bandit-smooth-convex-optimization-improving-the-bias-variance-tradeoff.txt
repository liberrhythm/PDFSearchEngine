Bandit Smooth Convex Optimization:
Improving the Bias-Variance Tradeoff
Ofer Dekel
Microsoft Research
Redmond, WA
oferd@microsoft.com

Ronen Eldan
Weizmann Institute
Rehovot, Israel
roneneldan@gmail.com

Tomer Koren
Technion
Haifa, Israel
tomerk@technion.ac.il

Abstract
Bandit convex optimization is one of the fundamental problems in the field of
online learning. The best algorithm for the general bandit convex optimizae 5/6 ), while the best known lower bound
tion problem guarantees a regret of O(T
1/2
is ⌦(T ). Many attempts have been made to bridge the huge gap between these
bounds. A particularly interesting special case of this problem assumes that the
loss functions are smooth. In this case, the best known algorithm guarantees a ree 2/3 ). We present an efficient algorithm for the bandit smooth convex
gret of O(T
e 5/8 ). Our result rules out
optimization problem that guarantees a regret of O(T
an ⌦(T 2/3 ) lower bound and takes a significant step towards the resolution of this
open problem.

1

Introduction

Bandit convex optimization [11, 5] is the following online learning problem. First, an adversary
privately chooses a sequence of bounded and convex loss functions f1 , . . . , fT defined over a convex domain K in d-dimensional Euclidean space. Then, a randomized decision maker iteratively
chooses a sequence of points x1 , . . . , xT , where each xt 2 K. On iteration t, after choosing the
point xt , the decision maker incurs a loss of ft (xt ) and receives bandit feedback: he observes the
value of his loss but he does not receive any other information about the function ft . The decision
maker uses the feedback to make better choices on subsequent rounds. His goal is to minimize regret, which is the difference between his loss and the loss incurred by the best fixed point in K. If
the regret grows sublinearly with T , it indicates that the decision maker’s performance improves as
the length of the sequence increases, and therefore we say that he is learning.
Finding an optimal algorithm for bandit convex optimization is an elusive open problem. The first
algorithm for this problem was presented in Flaxman et al. [11] and guarantees a regret of R(T ) =
e 5/6 ) for any sequence of loss functions (here and throughout, the asymptotic O
e notation hides
O(T
a polynomial dependence on the dimension d as well as logarithmic factors). Despite the ongoing
effort to improve on this rate, it remains the state of the art. On the other hand, Dani et al. [9]
proves that for any algorithm there exists a worst-case sequence of loss functions for which R(T ) =
⌦(T 1/2 ), and the gap between the upper and lower bounds is huge.
While no progress has been made on the general form of the problem, some progress has been made
in interesting special cases. Specifically, if the bounded convex loss functions are also assumed to be
e 3/4 ). If the loss funcLipschitz, Flaxman et al. [11] improves their regret guarantee to R(T ) = O(T
tions are smooth (namely, their gradients are Lipschitz), Saha and Tewari [15] present an algorithm
e 2/3 ). Similarly, if the loss functions are bounded, Lipschitz, and
with a guaranteed regret of O(T
e 2/3 ) [3]. If even stronger assumptions are made, an
strongly convex, the guaranteed regret is O(T
1/2
e
optimal regret rate of ⇥(T ) can be guaranteed; namely, when the loss functions are both smooth
1

and strongly-convex [12], when they are Lipschitz and linear [2], and when Lipschitz loss functions
are not generated adversarially but drawn i.i.d. from a fixed and unknown distribution [4].
Recently, Bubeck et al. [8] made progress that did not rely on additional assumptions, such as
Lipschitz, smoothness, or strong convexity, but instead considered the general problem in the onee 1/2 ) regret
dimensional case. That result proves that there exists an algorithm with optimal ⇥(T
for arbitrary univariate convex functions ft : [0, 1] 7! [0, 1]. Subsequently, and after the current
paper was written, Bubeck and Eldan [7] generalized this result to bandit convex optimization in
general Euclidean spaces (albeit requiring a Lipschitz assumption). However, the proofs in both
papers are non-constructive and do not give any hint on how to construct a concrete algorithm, nor
any indication that an efficient algorithm exists.
The current state of the bandit convex optimization problem has given rise to two competing conjectures. Some believe that there exists an efficient algorithm that matches the current lower bound.
Meanwhile, others are trying to prove larger lower bounds, in the spirit of [10], even under the assumption that the loss functions are smooth; if the ⌦(T 1/2 ) lower bound is loose, a natural guess
e 2/3 ).1 In this paper, we take an important step towards the
of the true regret rate would be ⇥(T
e 5/8 ) against
resolution of this problem by presenting an algorithm that guarantees a regret of ⇥(T
any sequence of bounded, convex, smooth loss functions. Compare this result to the previous statee 2/3 ) (noting that 2/3 = 0.666... and 5/8 = 0.625). This result rules out
of-the-art result of ⇥(T
the possibility of proving a lower bound of ⌦(T 2/3 ) with smooth functions. While there remains
a sizable gap with the T 1/2 lower bound, our result brings us closer to finding the elusive optimal
algorithm for bandit convex optimization, at least in the case of smooth functions.
Our algorithm is a variation on the algorithms presented in [11, 1, 15], with one new idea. These
algorithms all follow the same template: on each round, the algorithm computes an estimate of
rft (xt ), the gradient of the current loss function at the current point, by applying a random perturbation to xt . The sequence of gradient estimates is then plugged into a first-order online optimization
technique. The technical challenge in the analysis of these algorithms is to bound the bias and the
variance of these gradient estimates. Our idea is take a window of consecutive gradient estimates
and average them, producing a new gradient estimate with lower variance and higher bias. Overall,
the new bias-variance tradeoff works in our favor and allows us to improve the regret upper-bound.
Averaging uncorrelated random vectors to reduce variance is a well-known technique, but applying
it in the context of bandit convex optimization algorithm is easier said than done and requires us to
overcome a number of technical difficulties. For example, the gradient estimates in our window are
taken at different points, which introduces a new type of bias. Another example is the difficulty that
arrises when the sequence xs , . . . , xt travels adjacent to the boundary of the convex set K (imagine
transitioning from one face of a hypercube to another); the random perturbation applied to xs and
xt could be supported on orthogonal directions, yet we average the resulting gradient estimates
and expect to get a meaningful low-variance gradient estimate. While the basic idea is simple, our
non-trivial technical analysis is not, and may be of independent interest.

2

Preliminaries

We begin by defining smooth bandit convex optimization more formally, and recalling several basic
results from previous work on the problem (Flaxman et al. [11], Abernethy et al. [2], Saha and Tewari
[15]) that we use in our analysis. We also review the necessary background on self-concordant
barrier functions.
2.1

Smooth Bandit Convex Optimization

In the bandit convex optimization problem, an adversary first chooses a sequence of convex functions
f1 , . . . , fT : K 7! [0, 1], where K is a closed and convex domain in Rd . Then, on each round
t = 1, . . . , T , a randomized decision maker has to choose a point xt 2 K, and after committing
to his decision he incurs a loss of ft (xt ), and observes this loss as feedback. The P
decision maker’s
T
expected loss (where expectation is taken with respect to his random choices) is E[ t=1 ft (xt )] and
1
In fact, we are aware of at least two separate research groups that invested time trying to prove such
an ⌦(T 2/3 ) lower bound.

2

his regret is

"

R(T ) = E

T
X

ft (xt )

t=1

#

min

x2K

T
X

ft (x) .

t=1

Throughout, we use the notation Et [·] to indicate expectations conditioned on all randomness up to
and including round t 1.
We make the following assumptions. First, we assume that each of the functions f1 , . . . , fT is LLipschitz with respect to the Euclidean norm k·k2 , namely that |ft (x) ft (y)|  Lkx yk2 for all
x, y 2 K. We further assume that ft is H-smooth with respect to k·k2 , which is to say that
8 x, y 2 K ,
krft (x) rft (y)k2  H kx yk2 .
In particular, this implies that ft is continuously differentiable over K. Finally, we assume that the
Euclidean diameter of the decision domain K is bounded by D > 0.
2.2

First Order Algorithms with Estimated Gradients

The online convex optimization problem becomes much easier in the full information setting, where
the decision maker’s feedback includes the vector gt = rft (xt ), the gradient (or subgradient) of
ft at the point xt . In this setting, the decision maker can use a first-order online algorithm, such as
the projected online gradient descent algorithm [17] or dual averaging [13] (sometimes known as
follow the regularized leader [16]), and guarantee a regret of O(T 1/2 ). The dual averaging approach
sets xt to be the solution to the following optimization problem,
( t 1
)
X
xt = arg min x ·
↵s,t gs + R(x) ,
(1)
x2K

s=1

where R is a suitably chosen regularizer, and for all t = 1, . . . , T and s = 1, . . . , t we define a
non-negative weight ↵s,t . Typically, all of the weights (↵s,t ) are set to a constant value ⌘, called the
learning rate parameter.

However, since we are not in the full information setting and the decision maker does not observe gt ,
the algorithms mentioned above cannot be used directly. The key observation of Flaxman et al. [11],
which is later reused in all of the follow-up work, is that gt can be estimated by randomly perturbing
the point xt . Specifically, on round t, the algorithm chooses the point
yt = xt + At ut ,
(2)
instead of the original point xt , where > 0 is a parameter that controls the magnitude of the
perturbation, At is a positive definite d ⇥ d matrix, and ut is drawn from the uniform distribution
on the unit sphere. In Flaxman et al. [11], At is simply set to the identity matrix whereas in Saha
and Tewari [15], At is more carefully tailored to the point xt (see details below). In any case, care
should be taken to ensure that the perturbed point yt remains in the convex set K. The observed
value ft (yt ) is then used to compute the gradient estimate
d
ĝ t = ft (yt )At 1 ut ,
(3)
and this estimate is fed to the first-order optimization algorithm. While ĝ t is not an unbiased estimator of rft (xt ), it is an unbiased estimator for the gradient of a different function, fˆt , defined
by
fˆt (x) = Et [ft (x + At v)] ,
(4)
where v 2 Rd is uniformly drawn from the unit ball. The function fˆt (x) is a smoothed version of
ft , which plays a key role in our analysis and in many of the previous results on this topic. The main
property of fˆt is summarized in the following lemma.
Lemma 1 (Flaxman et al. [11], Saha and Tewari [15, Lemma 5]). For any differentiable function
f : Rd 7! R, positive definite matrix A, x 2 Rd , and 2 (0, 1], define ĝ = (d/ )f (x+ Au)·A 1 u,
where u is uniform on the unit sphere. Also, let fˆ(x) = E[f (x + Av)] where v is uniform on the
unit ball. Then E[ĝ] = rfˆ(x).

The difference between rft (xt ) and rfˆt (xt ) is the bias of the gradient estimator ĝ t . The analysis
in Flaxman et al. [11], Abernethy et al. [2], Saha and Tewari [15] focuses on bounding the bias and
the variance of ĝ t and their effect on the first-order optimization algorithm.
3

2.3

Self-Concordant Barriers

Following [2, 1, 15], our algorithm and analysis rely on the properties of self-concordant barrier
functions. Intuitively, a barrier is a function defined on the interior of the convex body K, which is
rather flat in most of the interior of K and explodes to 1 as we approach its boundary. Additionally, a self-concordant barrier has some technical properties that are useful in our setting. Before
giving the formal definition of a self-concordant barrier, we define the local norm defined by a
self-concordant barrier.
Definition 2 (Local Norm Induced by a Self-Concordant Barrier [14]). Let R : int(K) 7! R be a
self-concordant barrier.pThe local norm induced by R at the pointpx 2 int(K) is denoted by kzkx
and defined as kzkx = zT r2 R(x)z. Its dual norm is kzkx,⇤ = zT (r2 R(x)) 1 z.
In words, the local norm at x is the Mahalanobis norm defined by the Hessian of R at the point x,
namely, r2 R(x). We now give a formal definition of a self-concordant barrier.
Definition 3 (Self-Concordant Barrier [14]). Let K ✓ Rd be a convex body. A function R :
int(K) 7! R is a #-self-concordant barrier for K if (i) R is three times continuously differentiable,
(ii) R(x) ! 1 as x ! @K, and (iii) for all x 2 int(K) and y 2 Rd , R satisfies
p
|r3 R(x)[y, y, y]|  2kyk3x and |rR(x) · y|  #kykx .

This definition is given for completeness, and is not directly used in our analysis. Instead, we rely
on some useful properties of self-concordant barriers. First and foremost, there exists a O(d)-selfconcordant barrier for any convex body [14, 6]. Efficiently-computable self-concordant barriers
are only known for specific classes of convex bodies, such as polytopes, yet we make the standard
assumption that we have an efficiently computable #-self-concordant barrier for the set K.
Another key feature of a self-concordant barrier is the set of Dikin ellipsoids that it defines. The
Dikin ellipsoid at x 2 int(K) is simply the unit ball with respect to the local norm at x. A key
feature of the Dikin ellipsoid is that it is entirely contained in the convex body K, for any x [see 14,
Theorem 2.1.1]. Another technical property of a self-concordant barriers is that its Hessian changes
slowly with respect to its local norm.
Theorem 4 (Nesterov and Nemirovskii [14, Theorem 2.1.1]). Let K be a convex body with selfconcordant barrier R. For any x 2 int(K) and z 2 Rd such that kzkx < 1, it holds that
(1

kzkx )2 r2 R(x)

r2 R(x + z)

(1

kzkx )

2

r2 R(x) .

While the self-concordant barrier explodes to infinity at the boundary of K, it is quite flat at points
that are far from the boundary. To make this statement formal, we define an operation that multiplicatively shrinks the set K toward the minimizer of R (called the analytic center of K). Let
y = arg min R(x) and assume without loss of generality that R(y) = 0. For any ✏ 2 (0, 1) let Ky,✏
denote the set {y + (1 ✏)(x y) : x 2 K}. The next theorem states that the barrier is flat in Ky,✏
and explodes to 1 in the thin shell between Ky,✏ and K.
Theorem 5 (Nesterov and Nemirovskii [14, Propositions 2.3.2-3]). Let K be a convex body with
#-self-concordant barrier R, let y = arg min R(x), and assume that R(y) = 0. For any ✏ 2 (0, 1],
it holds that
1
8 x 2 Ky,✏
R(x)  # log .
✏
Our assumptions on the loss functions, as the Lipschitz assumption or the smoothness assumption,
are stated in terms of the standard Euclidean norm (which we denote by k · k2 ). Therefore, we will
need to relate the Euclidean norm to the local norms defined by the self-concordant barrier. This is
accomplished by the following lemma (whose proof appears in the supplementary material).
Lemma 6. Let K be a convex body with self-concordant barrier R and let D be the (Euclidean)
diameter of K. For any x 2 K, it holds that D 1 kzkx,⇤  kzk2  Dkzkx for all z 2 Rd .
2.4

Self-Concordant Barrier as a Regularizer

Looking back at the dual averaging strategy defined in Eq. (1), we can now fill in some of the details
that were left unspecified: [1, 15] set the regularization R in Eq. (1) to be a #-self-concordant barrier
for the set K. We use the following useful lemma from Abernethy and Rakhlin [1] in our analysis.
4

Algorithm 1: Bandit Smooth Convex Optimization
Parameters: perturbation parameter 2 (0, 1], dual averaging weights (↵s,t ), self-concordant
barrier R : int(K) 7! R
Initialize: y1 2 K arbitrarily
for t = 1, . . . , T
At
(r2 R(xt )) 1/2
draw ut uniformly from the unit sphere
yt
xt + At ut
choose yt , receive feedback ft (yt )
ĝ t
(d/ )ft (yt ) · At 1 ut
Pt
xt+1
arg minx2K {x · s=1 ↵s,t ĝ s + R(x)}
Lemma 7 (Abernethy and Rakhlin [1]). Let K be a convex body with #-self-concordant barrier
R, let g1 , . . . , gT be vectors in Rd , and let ⌘ > 0 be such that ⌘kgt kxt ,⇤  14 for all t. Define
Pt 1
xt = arg minx2K {x · s=1 ⌘gs + R(x)}. Then,
(i) for all t it holds that kxt xt+1 kxt  2⌘kgt kxt ,⇤ ;
PT
PT
(ii) for any x? 2 K it holds that t=1 gt · (xt x? )  ⌘1 R(x? ) + 2⌘ t=1 kgt k2xt ,⇤ .

Algorithms for bandit convex optimization that use a self-concordant regularizer also use the same
self-concordant barrier to obtain gradient estimates. Namely, these algorithms perturb the dual averaging solution xt as in Eq. (3), with the perturbation matrix At set to (r2 R(xt )) 1/2 , the root of
the inverse Hessian of R at the point xt . In other words, the distribution of yt is supported on the
Dikin ellipsoid centered at xt , scaled by . Since 2 (0, 1], this form of perturbation guarantees
that yt 2 K. Moreover, if yt is generated in this way and used to construct the gradient estimator
ĝ t , then the local norm of ĝ t is bounded, as specified in the following lemma.
Lemma 8 (Saha and Tewari [15, Lemma 5]). Let K ✓ Rd be a convex body with self-concordant
barrier R. For any differentiable function f : K 7! [0, 1], 2 (0, 1], and x 2 int(K), define
ĝ = (d/ )f (y) · A 1 u, where A = (r2 R(x)) 1/2 , y = x + Au, and u is drawn uniformly from
the unit sphere. Then kĝkx  d/ .

3

Main Result

Our algorithm for the bandit smooth convex optimization problem is a variant of the algorithm in
Saha and Tewari [15], and appears in Algorithm 1. Following Abernethy and Rakhlin [1], Saha and
Tewari [15], we use a self-concordant function as the dual averaging regularizer and we use its Dikin
ellipsoids to perturb the points xt . The difference between our algorithm and previous ones is the
introduction of dual averaging weights (↵s,t ), for t = 1, . . . , T and s = 1, . . . , t, which allow us to
vary the weight of each gradient in the dual averaging objective function.
In addition to the parameters , ⌘, and ✏, we introduce a new buffering parameter k, which takes
non-negative integer values. We set the dual averaging weights in Algorithm 1 to be
(
⌘
if s  t k
↵s,t =
(5)
t s+1
⌘
if s > t k ,
k+1
where ⌘ > 0 is a global learning rate parameter. This choice of (↵s,t ) effectively decreases the
influence of the feedback received on the most recent k rounds. If k = 0, all of the (↵s,t ) become
equal to ⌘ and Algorithm 1 reduces to the algorithm in Saha and Tewari [15]. The surprising result
is that there exists a different setting of k > 0 that gives a better regret bound.
We introduce a slight abuse of notation, which helps us simplify the presentation of our regret bound.
We will eventually achieve the desired regret bound by setting the parameters ⌘, , and k to be some
functions of T . Therefore, from now on, we treat the notation ⌘, , and k as an abbreviation for the
functional forms ⌘(T ), (T ), and k(T ) respectively. The benefit is that we can now use asymptotic
notation (e.g., O(⌘k)) to sweep meaningless low-order terms under the rug.
5

We prove the following regret bound for this algorithm.
Theorem 9. Let f1 , . . . , fT be a sequence of loss functions where each ft : K 7! [0, 1] is differentiable, convex, H-smooth and L-Lipschitz, and where K ✓ Rd is a convex body of diameter
D > 0 with #-self-concordant barrier R. For any , ⌘ 2 (0, 1] and k 2 {0, 1, . . . , T } assume that
Algorithm 1 is run with these parameters and with the weights defined in Eq. (5) (using k and ⌘) to
generate the sequences x1 , . . . , xT and y1 , . . . , yT . If 12k⌘d  and for any ✏ 2 (0, 1) it holds that
p
✓
◆
# log 1✏
64d2 ⌘T
12(HD2 + DL)d⌘ kT
T✏
R(T )  HD2 2 T +
+ 2
+
+O
+ T ⌘k .
⌘
(k + 1)
Specifically, if wepset = d1/4 T
that R(T ) = O( d T 5/8 log T ).

3/16

,⌘ =d

1/2

T

5/8

, k = d1/2 T 1/8 , and ✏ = T

100

, we get

e 2/3 ) bound in Saha and Tewari [15] up
Note that if we set k = 0 in our theorem, we recover the O(T
to a small numerical constant (namely, the dependence on L, H, D, #, d, and T is the same).

4

Analysis

PT
Using the notation x? = arg minx2K t=1 ft (x), the decision maker’s regret becomes R(T ) =
⇥ PT
⇤
E
ft (x? ) . Following Flaxman et al. [11], Saha and Tewari [15], we rewrite the
t=1 ft (yt )
regret as
" T
#
" T
#
" T
#
X
X
X
?
?
?
ˆ
ˆ
ˆ
ˆ
R(T ) = E
ft (yt ) f t (xt ) + E
f t (xt ) f t (x ) + E
f t (x ) ft (x ) . (6)
|

t=1

{z
a

}

|

t=1

{z
b

}

t=1

|

{z
c

}

This decomposition essentially adds a layer of hallucination to the analysis: we pretend that the
loss functions are fˆ1 , . . . , fˆT instead of f1 , . . . , fT and we also pretend that we chose the points
x1 , . . . , xT rather than y1 , . . . , yT . We then analyze the regret in this pretend world (this regret is
the expression in Eq. (6b)). Finally, we tie our analysis back to the real world by bounding the
difference between that which we analyzed and the regret of the actual problem (this difference is
the sum of Eq. (6a) and Eq. (6c)). The advantage of our pretend world over the real world is that we
have unbiased gradient estimates ĝ 1 , . . . , ĝ T that can plug into the dual averaging algorithm.
The algorithm in Saha and Tewari [15] sets all of the dual averaging weights (↵s,t ) equal to the
constant learning rate ⌘ > 0. It decomposes the regret as in Eq. (6) and their main technical result is
the following bound for the individual terms:
Theorem 10 (Saha and Tewari [15]). Let f1 , . . . , fT be a sequence of loss functions where each
ft : K 7! [0, 1] is differentiable, convex, H-smooth and L-Lipschitz, and where K ✓ Rd is a convex
body of diameter D > 0 and #-self-concordant barrier R. Assume that Algorithm 1 is run with
perturbation parameter 2 (0, 1] and generates the sequences x1 , . . . , xT and y1 , . . . , yT . Then
for any ✏ 2 (0, 1) it holds that (6a) + (6c)  (HD2 2 + ✏L)T . If, additionally, the dual averaging
weights (↵s,t ) are all set to the constant learning rate ⌘ then (6b)  # log(1/✏)⌘ 1 + d2 2 ⌘T .
e 2/3 ) by choosing
The analysis in Saha and Tewari [15] goes on to obtain a regret bound of O(T
optimal values for the parameters ⌘, and ✏ and plugging those values into Theorem 10. Our
analysis uses the first part of Theorem 10 to bound (6a) + (6c) and shows that our careful choice of
the dual averaging weights (↵s,t ) results in the following improved bound on (6b).
We begin our analysis by defining a moving average of the functions fˆ1 , . . . , fˆT , as follows:
k

8 t = 1, . . . , T ,

f¯t (x) =

1 Xˆ
f t i (x) ,
k + 1 i=0

(7)

where, for soundness, we let f¯s ⌘ 0 for s  0. Also, define a moving average of gradient estimates:
k

8 t = 1, . . . , T ,

ḡ t =

6

1 X
ĝ t
k + 1 i=0

i

,

again, with ĝ s = 0 for s  0. In Section 4 below, we show how each ḡ t can be used as a biased
estimate of rf¯t (xt ). Also note that the choice of the dual averaging weights (↵s,t ) in Eq. (5) is such
Pt
Pt
that s=1 ↵s,t ĝ s = ⌘ s=1 ḡ s for all t. Therefore, the last step in Algorithm 1 basically performs
dual averaging with the gradient estimates ḡ 1 , . . . , ḡ T uniformly weighted by ⌘.
We use the functions f¯t to rewrite Eq. (6b) as
" T
#
" T
#
" T
#
X
X
X
E
fˆt (xt ) f¯t (xt ) + E
f¯t (xt ) f¯t (x? ) + E
f¯t (x? ) fˆt (x? ) .
(8)
|

t=1

{z
a

}

|

t=1

{z
b

}

|

t=1

{z
c

}

This decomposition essentially adds yet another layer of hallucination to the analysis: we pretend
that the loss functions are f¯1 , . . . , f¯T instead of fˆ1 , . . . , fˆT (which are themselves pretend loss
functions, as described above). Eq. (8b) is the regret in our new pretend scenario, while Eq. (8a) +
Eq. (8c) is the difference between this regret and the regret in Eq. (6b).
The following lemma bounds each of the terms in Eq. (8) separately, and summarizes the main
technical contribution of our paper.
Lemma 11. Under the conditions of Theorem 9, for any ✏ 2 (0, 1) it holds that (8c)  0 ,
p
12DLd⌘ kT
(8a) 
+ O (1 + ⌘)k , and
p
✓
◆
1
# log ✏
64d2 ⌘T
12HD2 d⌘ kT
T✏
(8b) 
+ 2
+
+ O
+ T ⌘k .
⌘
(k + 1)
4.1

Proof Sketch of Lemma 11

As mentioned above, the basic intuition of our technique is quite simple: average the gradients to
decrease their variance. Yet, applying this idea in the analysis is tricky. We begin by describing the
main source of difficulty in proving Lemma 11.
Recall that our strategy is to pretend that the loss functions are f¯1 , . . . , f¯T and to use the random
vector ḡ t as a biased estimator of rf¯t (xt ). Naturally, one of our goals is to show that this bias
is small. Recall that each ĝ s is an unbiased estimator of rfˆs (xs ) (conditioned on the history up
to round t). Specifically, note that each vector in the sequence ĝ t k , . . . , ĝ t is a gradient estimate
at a different point. Yet, we average these vectors and claim that they accurately estimate rf¯t at
the current point, xt . Luckily, fˆt is H-smooth, so rfˆt i (xt i ) should not be much different than
rfˆt i (xt ), provided that we show that xt i and xt are close to each other in Euclidean distance.
To show that xt i and xt are close, we exploit the stability of the dual averaging algorithm. Particularly, the first claim in Lemma 7 states that kxs xs+1 kxs is controlled by kḡ s kxs ,⇤ for all s, so now
we need to show that kḡ s kxs ,⇤ is small. However, ḡ t is the average of k + 1 gradient estimates taken
at different points; each ĝ t i is designed to have a small norm with respect to its own local norm
k · kxt i ,⇤ ; for all we know, it may be very large with respect to the current local norm k · kxt ,⇤ . So
now we need to show that the local norms at xt i and xt are similar. We could prove this if we knew
that xt i and xt are close to each other—which is exactly what we set out to prove in the beginning.
This chicken-and-egg situation complicates our analysis considerably.
Another non-trivial component of our proof is the variance reduction analysis. The motivation
to average ĝ t k , . . . , ĝ t is to generate new gradient estimates with a smaller variance. While the
random vectors ĝ 1 , . . . , ĝ T are not independent, we show that their randomness is uncorrelated.
Therefore, the variance of ḡ t is k + 1 times smaller than the variance of each ĝ t . However, to make
this argument formal, we again require the local norms at xt i and xt to be similar.
To make things more complicated, there is the recurring need to move back and forth between local
norms and the Euclidean norm, since the latter is used in the definition of Lipschitz and smoothness.
All of this has to do with bounding Eq. (8b), the regret with respect to the pretend loss functions
f¯1 , . . . , f¯T - an additional bias term appears in the analysis of Eq. (8a).
We conclude the paper by stating our main lemmas and sketching the proof Lemma 11. The full
technical proofs are all deferred to the supplementary material and replaced with some high level
commentary.
7

To break the chicken-and-egg situation described above, we begin with a crude bound on kḡ t kxt ,⇤ ,
which does not benefit at all from the averaging operation. We simultaneously prove that the local
norms at xt i and xt are similar.
Lemma 12. If the parameters k, ⌘, and are chosen such that 12k⌘d 
(i) kḡ t kxt ,⇤  2d/ ;
(ii) for any 0  i  k such that t

1 it holds that 12 kzkxt

i

i ,⇤

then for all t,

 kzkxt ,⇤  2kzkxt

i ,⇤

.

Lemma 12 itself has a chicken-and-egg aspect, which we resolve using an inductive proof technique.
Armed with the knowledge
⇥ that the⇤ local norms at xt i and xt are similar, we go on to prove the
more refined bound on Et kḡ t k2xt ,⇤ , which does benefit from averaging.
Lemma 13. If the parameters k, ⌘, and are chosen such that 12k⌘d 
⇥
⇤
Et kḡ t k2xt ,⇤  2D2 L2 +

then

32d2
.
2 (k + 1)

The proof constructs a martingale difference sequence and uses the fact that its increments are uncorrelated. Compare the above to Lemma 8, which proves that kĝ t i k2xt i ,⇤  d2 / 2 and note the
extra k + 1 in our denominator—all of our hard work was aimed at getting this factor.
Next, we set out to bound the expected Euclidean distance between xt i and xt . This bound is later
needed to exploit the L-Lipschitz and H-smooth assumptions. The crude bound on kḡ s kxs ,⇤ from
Lemma 12 is enough to satisfy the conditions of Lemma 7, which then tells us that E[kxs xs+1 kxs ]
is controlled by E[kḡ s kxs ,⇤ ]. The latter enjoys the improved bound due to Lemma 13. Integrating
the resulting bound over time, we obtain the following lemma.
Lemma 14. If the parameters k, ⌘, and
0  i  k such that t i 1, we have
E[kxt

are chosen such that 12k⌘d 

x t k2 ] 

i

p
12Dd⌘ k

then for all t and any

+ O(⌘k) .

Notice that xt i and xt may be k rounds apart, but the bound scales only with
the work of the averaging technique.

p

k. Again, this is

Finally, we have all the tools in place to prove our main result, Lemma 11.
Pk ˆ
1
Proof sketch. The first term, Eq. (8a), is bounded by rewriting f¯t (xt ) = k+1
i=0 f t i (xt ) and
then proving that fˆt i (xt ) is not very far from fˆt (xt ). This follows from the fact that fˆt is LLipschitz and from Lemma 14. To bound the second term, Eq. (8b), we use the convexity of each f¯t
to write
" T
#
" T
#
X
X
E
f¯t (xt ) f¯t (x? )  E
rf¯t (xt ) · (xt x? ) .
t=1

t=1

We relate the right-hand side above to

E

"

T
X
t=1

ḡ t · (xt

?

x )

#

,

using the fact that fˆt is H-smooth and again using Lemma 14. Then, we upper bound the above
using Lemma 7, Theorem 5, and Lemma 13.
⇤
Acknowledgments
We thank Jian Ding for several critical contributions during the early stages of this research. Parts
of this work were done while the second and third authors were at Microsoft Research, the support
of which is gratefully acknowledged.

8

References
[1] J. Abernethy and A. Rakhlin. Beating the adaptive bandit with high probability. In Information
Theory and Applications Workshop, 2009, pages 280–289. IEEE, 2009.
[2] J. Abernethy, E. Hazan, and A. Rakhlin. Competing in the dark: An efficient algorithm for
bandit linear optimization. In Proceedings of the 21st Annual Conference on Learning Theory
(COLT), 2008.
[3] A. Agarwal, O. Dekel, and L. Xiao. Optimal algorithms for online convex optimization with
multi-point bandit feedback. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT), 2010.
[4] A. Agarwal, D. P. Foster, D. Hsu, S. M. Kakade, and A. Rakhlin. Stochastic convex optimization with bandit feedback. In Advances in Neural Information Processing Systems (NIPS),
2011.
[5] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed
bandit problems. Foundations and Trends in Machine Learning, 5(1):1–122, 2012.
[6] S. Bubeck and R. Eldan. The entropic barrier: a simple and optimal universal self-concordant
barrier. arXiv preprint arXiv:1412.1587, 2015.
[7] S. Bubeck and R. Eldan. Multi-scale exploration of convex functions and bandit convex optimization. arXiv preprint arXiv:1507.06580, 2015.
p
[8] S. Bubeck, O. Dekel, T. Koren, and Y. Peres. Bandit convex optimization: T regret in one
dimension. In In Proceedings of the 28st Annual Conference on Learning Theory (COLT),
2015.
[9] V. Dani, T. Hayes, and S. M. Kakade. The price of bandit information for online optimization.
In Advances in Neural Information Processing Systems (NIPS), 2008.
[10] O. Dekel, J. Ding, T. Koren, and Y. Peres. Bandits with switching costs: T 2/3 regret. In
Proceedings of the 46th Annual Symposium on the Theory of Computing, 2014.
[11] A. D. Flaxman, A. Kalai, and H. B. McMahan. Online convex optimization in the bandit
setting: gradient descent without a gradient. In Proceedings of the sixteenth annual ACMSIAM symposium on Discrete algorithms, pages 385–394. Society for Industrial and Applied
Mathematics, 2005.
[12] E. Hazan and K. Levy. Bandit convex optimization: Towards tight bounds. In Advances in
Neural Information Processing Systems (NIPS), 2014.
[13] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical programming, 120(1):221–259, 2009.
[14] Y. Nesterov and A. Nemirovskii. Interior-point polynomial algorithms in convex programming,
volume 13. SIAM, 1994.
[15] A. Saha and A. Tewari. Improved regret guarantees for online smooth convex optimization with
bandit feedback. In International Conference on Artificial Intelligence and Statistics (AISTAT),
pages 636–642, 2011.
[16] S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends
in Machine Learning, 4(2):107–194, 2011.
[17] M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning (ICML’03), pages
928–936, 2003.

9

