Reflection, Refraction, and Hamiltonian Monte Carlo

Justin Domke
National ICT Australia (NICTA) &
Australian National University
Canberra, ACT 0200
Justin.Domke@nicta.com.au

Hadi Mohasel Afshar
Research School of Computer Science
Australian National University
Canberra, ACT 0200
hadi.afshar@anu.edu.au

Abstract
Hamiltonian Monte Carlo (HMC) is a successful approach for sampling from continuous densities. However, it has difficulty simulating Hamiltonian dynamics
with non-smooth functions, leading to poor performance. This paper is motivated
by the behavior of Hamiltonian dynamics in physical systems like optics. We introduce a modification of the Leapfrog discretization of Hamiltonian dynamics on
piecewise continuous energies, where intersections of the trajectory with discontinuities are detected, and the momentum is reflected or refracted to compensate for
the change in energy. We prove that this method preserves the correct stationary
distribution when boundaries are affine. Experiments show that by reducing the
number of rejected samples, this method improves on traditional HMC.

1

Introduction

Markov chain Monte Carlo sampling is among the most general methods for probabilistic inference.
When the probability distribution is smooth, Hamiltonian Monte Carlo (HMC) (originally called
hybrid Monte Carlo [4]) uses the gradient to simulate Hamiltonian dynamics and reduce random
walk behavior. This often leads to a rapid exploration of the distribution [7, 2]. HMC has recently
become popular in Bayesian statistical inference [13], and is often the algorithm of choice.
Some problems display piecewise smoothness, where the density is differentiable except at certain
boundaries. Probabilistic models may intrinsically have finite support, being constrained to some
region. In Bayesian inference, it might be convenient to state a piecewise prior. More complex and
highly piecewise distributions emerge in applications where the distributions are derived from other
distributions (e.g. the distribution of the product of two continuous random variables [5]) as well as
applications such as preference learning [1], or probabilistic programming [8].
While HMC is motivated by smooth distributions, the inclusion of an acceptance probability means
HMC does asymptotically sample correctly from piecewise distributions1 . However, since leapfrog
numerical integration of Hamiltonian dynamics (see [9]) relies on the assumption that the corresponding potential energy is smooth, such cases lead to high rejection probabilities, and poor performance. Hence, traditional HMC is rarely used for piecewise distributions.
In physical systems that follow Hamiltonian dynamics [6], a discontinuity in the energy can result
in two possible behaviors. If the energy decreases across a discontinuity, or the momentum is large
enough to overcome an increase, the system will cross the boundary with an instantaneous change
in momentum, known as refraction in the context of optics [3]. If the change in energy is too large to
be overcome by the momentum, the system will reflect off the boundary, again with an instantaneous
change in momentum.
1
Technically, here we assume the total measure of the non-differentiable points is zero so that, with probability one, none is ever encountered

1

6

6

6

4

4

2

2

0.

2

0.

2

0.25

4
25

0.

reflection

refraction

0.75

0.8

2

0.75

0.85

2

0

q

q2

0.25

0.8

0.95

0

0.75

q2

0.9

0

0.9

âˆ’2

0.8

0.85

0.

âˆ’2

âˆ’2

âˆ’4

âˆ’4

25

0.75

âˆ’4
0.25
0.

0.

2

âˆ’6
âˆ’6

âˆ’4

âˆ’2

0
q1

2

4

2

6

âˆ’6
âˆ’6

âˆ’4

(a)

âˆ’2

0
q1

(b)

2

4

6

âˆ’6
âˆ’6

âˆ’4

âˆ’2

0
q1

2

4

6

(c)

Figure 1: Example trajectories of baseline and reflective HMC. (a) Contours of the target distribution in
two dimensions, as defined in Eq. 18. (b) Trajectories of the rejected (red crosses) and accepted (blue dots)
proposals using baseline HMC. (c) The same with RHMC. Both use leapfrog parameters L = 25 and  = 0.1
In RHMC, the trajectory reflects or refracts on the boundaries of the internal and external polytope boundaries
and thus has far fewer rejected samples than HMC, leading to faster mixing in practice. (More examples in
supplementary material.)

Recently, Pakman and Paninski [11, 10] proposed methods for HMC-based sampling from piecewise
Gaussian distributions by exactly solving the Hamiltonian equations, and accounting for what we
refer to as refraction and reflection above. However, since Hamiltonian equations of motion can
rarely be solved exactly, the applications of this method are restricted to distributions whose logdensity is piecewise quadratic.
In this paper, we generalize this work to arbitrary piecewise continuous distributions, where each
region is a polytope, i.e. is determined by a set of affine boundaries. We introduce a modification to
the leapfrog numerical simulation of Hamiltonian dynamics, called Reflective Hamiltonian Monte
Carlo (RHMC), by incorporating reflection and refraction via detecting the first intersection of a
linear trajectory with a boundary. We prove that our method has the correct stationary distribution,
where the main technical difficulty is proving volume preservation of our dynamics to establish detailed balance. Numerical experiments confirm that our method is more efficient than baseline HMC,
due to having fewer rejected proposal trajectories, particularly in high dimensions. As mentioned,
the main advantage of this method over [11] and [10] is that it can be applied to arbitrary piecewise densities, without the need for a closed-form solution to the Hamiltonian dynamics, greatly
increasing the scope of applicability.

2

Exact Hamiltonian Dynamics

Consider a distribution P (q) âˆ exp(âˆ’U (q)) over Rn , where U is the potential energy.
HMC [9] is based on considering a joint distribution on momentum and position space
P (q, p) âˆ exp(âˆ’H(q, p)), where H(q, p) = U (q) + K(p), and K is a quadratic, meaning that
P (p) âˆ exp(âˆ’K(p)) is a normal distribution. If one could exactly simulate the dynamics, HMC
would proceed by (1) iteratively sampling p âˆ¼ P (p), (2) simulating the Hamiltonian dynamics
dqi
âˆ‚H
=
= pi
dt
âˆ‚pi

dpi
âˆ‚H
âˆ‚U
=âˆ’
=âˆ’
dt
âˆ‚qi
âˆ‚qi

(1)

(2)

for some period of time , and (3) reversing the final value p. (Only needed for the proof of correctness, since this will be immediately discarded at the start of the next iteration in practice.)
Since steps (1) and (2-3) both leave the distribution P (p, q) invariant, so does a Markov chain that
alternates between the two steps. Hence, the dynamics have P (p, q) as a stationary distribution. Of
course, the above differential equations are not well-defined when U has discontinuities, and are
typically difficult to solve in closed-form.
2

3

Reflection and Refraction with Exact Hamiltonian Dynamics

Take a potential function U (q) which is differentiable in all points except at some boundaries of
partitions. Suppose that, when simulating the Hamiltonian dynamics, (q, p) evolves over time as in
the above equations whenever these equations are differentiable. However, when the state reaches a
boundary, decompose the momentum vector p into a component pâŠ¥ perpendicular to the boundary
and a component pk parallel to the boundary. Let âˆ†U be the (signed) difference in potential energy
on the p
two sides of the discontinuity. If kpâŠ¥ k2 > 2âˆ†U then pâŠ¥ is instantaneously replaced by
p
0
pâŠ¥ := kpâŠ¥ k2 âˆ’ 2âˆ†U Â· kpâŠ¥ k . That is, the discontinuity is passed, but the momentum is changed
âŠ¥
in the direction perpendicular to the boundary (refraction). (If âˆ†U is positive, the momentum will
decrease, and if it is negative, the momentum will increase.) On the other hand, if kpâŠ¥ k2 â‰¤ 2âˆ†U ,
then pâŠ¥ is instantaneously replaced by âˆ’pâŠ¥ . That is, if the particleâ€™s momentum is insufficient
to climb the potential boundary, it bounces back by reversing the momentum component which is
perpendicular to the boundary.
Pakman and Paninski [11, 10] present an algorithm to exactly solve these dynamics for quadratic
U . However, for non-quadratic U , the Hamiltonian dynamics rarely have a closed-form solution,
and one must resort to numerical integration, the most successful method for which is known as the
leapfrog dynamics.

4

Reflection and Refraction with Leapfrog Dynamics

Informally, HMC with leapfrog dynamics iterates three steps. (1) Sample p âˆ¼ P (p). (2) Perform
leapfrog simulation, by discretizing the Hamiltonian equations into L steps using some small stepsize . Here, one interleaves a position step q â† q + p between two half momentum steps p â†
p âˆ’ âˆ‡U (q)/2. (3) Reverse the sign of p. If (q, p) is the starting point of the leapfrog dynamics,
and (q0 , p0 ) is the final point, accept the move with probability min(1, exp(H(p, q) âˆ’ H(p0 , q0 )))
See Algorithm 1.
It can be shown that this baseline HMC method has detailed balance with respect to P (p), even if
U (q) is discontinuous. However, discontinuities mean that large changes in the Hamiltonian may
occur, meaning many steps can be rejected. We propose a modification of the dynamics, namely,
reflective Hamiltonian Monte Carlo (RHMC), which is also shown in Algorithm 1.
The only modification is applied to the position steps: In RHMC, the first intersection of the trajectory with the boundaries of the polytope that contains q must be detected [11, 10]. The position step
is only taken up to this boundary, and reflection/refraction occurs, depending on the momentum and
change of energy at the boundary. This process continues until the entire amount of time  has been
simulated. Note that if there is no boundary in the trajectory to time , this is equivalent to baseline
HMC. Also note that several boundaries might be visited in one position step.
As with baseline HMC, there are two alternating steps, namely drawing a new momentum variable
p from P (p) âˆ exp(âˆ’K(p)) and proposing a move (p, q) â†’ (p0 , q0 ) and accepting or rejecting it
with a probability determined by a Metropolis-Hastings ratio. We can show that both of these steps
leave the joint distribution P invariant, and hence a Markov chain that also alternates between these
steps will also leave P invariant.
As it is easy to see, drawing p from P (p) will leave P (q, p) invariant, we concentrate on the second
step i.e. where a move is proposed according to the piecewise leapfrog dynamics shown in Alg. 1.
Firstly, it is clear that these dynamics are time-reversible, meaning that if the simulation takes state
(q, p) to (q0 , p0 ) it will also take state (q0 , p0 ) to (q, p). Secondly, we will show that these dynamics
are volume preserving. Formally, if D denotes the leapfrog dynamics, we will show that the absolute
value of the determinant of the Jacobian of D is one. These two properties together show that the
probability density of proposing a move from (q, p) to (q0 , p0 ) is the same of that proposing a move
from (q0 , p0 ) to (q, p). Thus, if the move (q, p) â†’ (q0 , p0 ) is accepted according to the standard
Metropolis-Hastings ratio, R ((q, p) â†’ (q0 , p0 )) = min(1, exp(H(q, p) âˆ’ H(q0 , p0 )), then detailed
balance will be satisfied. To see this, let Q denote the proposal distribution, Then, the usual proof of
correctness for Metropolis-Hastings applies, namely that
3

P (q, p)Q ((q, p) â†’ (q0 , p0 )) R ((q, p) â†’ (q0 , p0 ))
P (q0 , p0 )Q((q0 , p0 ) â†’ (q, p))R((q0 , p0 ) â†’ (q, p))
P (q, p) min(1, exp(H(q, p) âˆ’ H(q0 , p0 ))
= 1. (3)
=
P (q0 , p0 ) min(1, exp(H(q0 , p0 ) âˆ’ H(q, p))
(The final equality is easy to establish, considering the cases where H(q, p) â‰¥ H(q0 , p0 ) and
H(q0 , p0 ) â‰¤ H(q, p) separately.) This means that
detailed balance holds, and so P is a stationary distribution.

q0
0

p

x
The major difference in the analysis of RHMC, relap
tive to traditional HMC is that showing conservation
of volume is more difficult. With standard HMC
q
and leapfrog steps, volume conservation is easy to
show by observing that each part of a leapfrog step
q1 = c
is a shear transformation. This is not the case with
RHMC, and so we must resort to a full analysis of
the determinant of the Jacobian, as explored in the Figure 2: Transformation T : hq,pi â†’ hq0 ,p0 i
following section.
described by Lemma 1 (Refraction).
Algorithm 1: BASELINE & R EFLECTIVE HMC A LGORITHMS
input : q0 , current sample; U , potential function, L, # leapfrog steps; , leapfrog step size
output: next sample
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

begin
q â† q0 ; p âˆ¼ N (0, 1)
H0 â† kpk2 /2 + U (q)
for l = 1 to L do
p â† p âˆ’ âˆ‡U (q)/2

# Half-step evolution of momentum

if BASELINE HMC then
# Full-step evolution of position:
q â† q + p
# i.e. if R EFLECTIVE HMC:
else
t0 â† 0

while hx, tx , âˆ†U, Ï†i â† F IRST D ISCONTINUITY(q, p,  âˆ’ t0 , U ) 6= âˆ… do
qâ†x
t0 â† t0 + tx
hpâŠ¥ , pk i = DECOMPOSE(p, Ï†)
# Perpendicular/ parallel to boundary plane Ï†
if kpâŠ¥ k2 >p2âˆ†U then
p
# Refraction
pâŠ¥ â† kpâŠ¥ k2 âˆ’ 2âˆ†U Â· kpâŠ¥ k
âŠ¥
else
pâŠ¥ â† âˆ’pâŠ¥
# Reflection

18
19
20
21
22
23
24
25

p â† pâŠ¥ + pk
q â† q + ( âˆ’ t0 )p
p â† p âˆ’ âˆ‡U (q)/2
# Half-step evolution of momentum
p â† âˆ’p
# Not required in practice; for reversibility proof
H â† kpk2 /2 + U (q); âˆ†H â† H âˆ’ H0
if s âˆ¼ U(0, 1) < eâˆ’âˆ†H return q else return q0
end
note : F IRST D ISCONTINUITY(Â·) returns x, the position of the first intersection of a boundary plain
with line segment [q, q + ( âˆ’ t0 )p]; tx , the time it is visited; âˆ†U , the change in energy at the
discontinuity, and Ï†, the visited partition boundary. If no such point exists, âˆ… is returned.

4

5

Volume Conservation

5.1

Refraction

In our first result, we assume without loss of generality, that there is a boundary located at the
hyperplane q1 = c. This Lemma shows that, in the refractive case, volume is conserved. The setting
is visualized in Figure 2.
Lemma 1. Let T : hq, pi â†’ hq0 , p0 i be a transformation in Rn that takes a unit mass located at
q := (q1 , . . . , qn ) and moves it with constant momentum p := (p1 , . . . , pn ) till it reaches a plane
q1 = c (at some point x := (c, x2 , . . . , xn ) where
 c is a constant). Subsequently the momentum is
p
0
2
changed to p =
p1 âˆ’ 2âˆ†U (x), p2 , . . . , pn (where âˆ†U (Â·) is a function of x s.t. p21 > 2âˆ†U (x)).
The move is carried on for the total time period Ï„ till it ends in q0 . For all n âˆˆ N, T satisfies the
volume preservation property.
Proof. Since for i > 1, the momentum is not affected by the collision, qi0 = qi + Ï„ Â· pi and p0i = pi .
Thus,
âˆ‚qi0
âˆ‚qi0
âˆ‚p0i
âˆ‚p0i
âˆ€j âˆˆ {2, . . . , n}s.t. j 6= i,
=
=
=
= 0.
âˆ‚qj
âˆ‚pj
âˆ‚qj
âˆ‚pj
Therefore, if we explicitly write out the Jacobian determinant |J| of the transformation T , it is
 âˆ‚q0
 

âˆ‚q10
âˆ‚q10
âˆ‚q10
âˆ‚q10

  âˆ‚q10 âˆ‚q10
1
âˆ‚q10
âˆ‚q10
âˆ‚q10

Â· Â· Â· âˆ‚pkâˆ’1
 âˆ‚q1
  âˆ‚q
Â· Â· Â· âˆ‚pkâˆ’1
âˆ‚p1
âˆ‚qk
âˆ‚pk

âˆ‚p1
âˆ‚qk
âˆ‚pk
1
0
0
0
0
 âˆ‚p01

âˆ‚p
âˆ‚p
âˆ‚p
âˆ‚p
0
0
0
0
0


1
1
1
1
âˆ‚p1
âˆ‚p1
âˆ‚p1

  âˆ‚p1 âˆ‚p1 Â· Â· Â·
Â· Â· Â· âˆ‚pkâˆ’1

âˆ‚p1
âˆ‚qk
âˆ‚pk
 âˆ‚q10

âˆ‚q1
âˆ‚p1
âˆ‚pkâˆ’1
âˆ‚qk
âˆ‚pk


0
0
0
0
 âˆ‚q2
 
âˆ‚q2
âˆ‚q2
âˆ‚q2
âˆ‚q2

âˆ‚q20
âˆ‚q20
âˆ‚q20
 âˆ‚q1

Â·
Â·
Â·


0
0
Â·
Â·
Â·
âˆ‚p
âˆ‚p
âˆ‚q
âˆ‚p
1
kâˆ’1
k
k

 
âˆ‚pkâˆ’1
âˆ‚qk
âˆ‚pk



..
.
.
.


.
.
.
..
..
..
..

= .
..
..
.

.
.


0
0
0
0
0


âˆ‚pkâˆ’1
âˆ‚pkâˆ’1
âˆ‚pkâˆ’1 
 âˆ‚pkâˆ’1 âˆ‚pkâˆ’1
âˆ‚p0kâˆ’1 
âˆ‚p0kâˆ’1

Â·
Â·
Â·
 âˆ‚q1

0
0
Â·
Â·
Â·
1
âˆ‚p
âˆ‚p
âˆ‚q
âˆ‚p


1
kâˆ’1
k
k
 âˆ‚q0
 
âˆ‚qk
âˆ‚pk
0
0
0
0

0
âˆ‚qk
âˆ‚qk
âˆ‚qk
âˆ‚qk

 
k
âˆ‚qk

Â·
Â·
Â·
 âˆ‚q1
  0
0
Â·Â·Â·
0
1
âˆ‚p1
âˆ‚pkâˆ’1
âˆ‚qk
âˆ‚pk

âˆ‚pk
0
0
0
0
 âˆ‚p0k

âˆ‚p
âˆ‚p
âˆ‚p
âˆ‚p

k
k
k


0
0
Â·Â·Â·
0
0
1 
Â· Â· Â· âˆ‚p k
âˆ‚q
âˆ‚p
âˆ‚q
âˆ‚p
1

1

kâˆ’1

k

k

(4)



Now, using standard properties of the determinant, we have that |J| = 


âˆ‚q10
âˆ‚q1
âˆ‚p01
âˆ‚q1

âˆ‚q10
âˆ‚p1
âˆ‚p01
âˆ‚p1




.


We will now explicitly calculate these four derivatives. Due to the significance of the result, we carry
out the computations in detail. Nonetheless, as this is a largely mechanical process, for brevity, we
do not comment on the derivation.
Let t1 be the time to reach x and t2 be the period between reaching x and the last point q0 . Then:
def

t1 =

c âˆ’ q1
p1

x = q + t1 p

(5)

q10 = c + p01 Â· t2 (8)

def

p01 =

q

def

(6)

t2 = Ï„ âˆ’ t1 = Ï„ +

p21 âˆ’ 2âˆ†U (x) (9)

âˆ‚t2 by (7) 1
=
âˆ‚q1
p1

âˆ‚q10
âˆ‚q 0 âˆ‚p0
âˆ‚q 0 âˆ‚t2
= 10 Â· 1 + 1 Â·
âˆ‚q1
âˆ‚p1 âˆ‚q1
âˆ‚t2 âˆ‚q1
âˆ‚q10
âˆ‚q 0 âˆ‚p0
âˆ‚q 0 âˆ‚t2
= 10 Â· 1 + 1 Â·
âˆ‚p1
âˆ‚p1 âˆ‚p1
âˆ‚t2 âˆ‚p1

(8 & 10)

=

(7 & 8)

(10)

(11)

âˆ‚p01
c âˆ’ q1
+ p01 Â·
âˆ‚p1
p21

(12)


âˆ‚ p21 âˆ’ 2âˆ†U (x)
âˆ‚p01 (9)
1
p1 âˆ’ âˆ‚âˆ†U (x)/âˆ‚p1
= p 2
Â·
=
âˆ‚p1
âˆ‚p1
p01
2 p1 âˆ’ 2âˆ†U (x)
5

(7)

âˆ‚p01
1
+ p01 Â·
âˆ‚q1
p1

t2 Â·

= t2 Â·

q1 âˆ’ c
p1

(13)


âˆ‚ p21 âˆ’ 2âˆ†U (x)
âˆ‚p01
1
1 âˆ’âˆ‚âˆ†U (x)
Â·
= p 2
= 0 Â·
âˆ‚q1
âˆ‚q1
p1
âˆ‚q1
2 p1 âˆ’ 2âˆ†U (x)

(14)



câˆ’q1
âˆ‚x (5, 6) âˆ‚ q + p1 p
âˆ‚q
âˆ‚(p/p1 )
âˆ’1
=
=
+ (c âˆ’ q1 ) Â·
= (c âˆ’ q1 ) 2 Â· (0, p2 , p3 , . . . , pn ) (15)
âˆ‚p1
âˆ‚p1
âˆ‚p1
âˆ‚p1
p1
âˆ‚x (5, 6) âˆ‚q
p âˆ‚(c âˆ’ q1 )
q
p
p2
pn
âˆ’1
=
+
Â·
=
âˆ’
= (1, 0, . . . , 0) âˆ’ (1, , . . . , ) =
(0, p2 , . . . , pn )
âˆ‚q1
âˆ‚q1
p1
âˆ‚q1
q1
p1
p1
p1
p1
(16)
âˆ‚q 0 âˆ‚p0

Substituting the appropriate terms into |J| = | âˆ‚q11 âˆ‚p11 âˆ’

âˆ‚p01 âˆ‚q10
âˆ‚q1 âˆ‚p1 |,

we get that





âˆ‚q10 âˆ‚p01
âˆ‚q10 âˆ‚p01 (11 & 12)
âˆ‚p01
p01
âˆ‚p01
âˆ‚p01
âˆ‚p0
0 c âˆ’ q1
|J| =
Â·
âˆ’
Â·
=
t2
+
Â·
âˆ’ t2
+ p1 2
Â· 1
âˆ‚q1 âˆ‚p1
âˆ‚p1 âˆ‚q1
âˆ‚q1
p1
âˆ‚p1
âˆ‚p1
p1
âˆ‚q1
 0



0
0
p
âˆ‚p1
q1 âˆ’ c âˆ‚p1 (13 & 14) 1
âˆ‚âˆ†U (x) q1 âˆ’ c âˆ‚âˆ†U (x)
= 1
+
Â·
=
p1 âˆ’
âˆ’
Â·
p1 âˆ‚p1
p1
âˆ‚q1
p1
âˆ‚p1
p1
âˆ‚q1


q1 âˆ’ c âˆ‚âˆ†U (x) âˆ‚x
1 âˆ‚âˆ†U (x) âˆ‚x
Â·
+
Â·
Â·
=1âˆ’
p1
âˆ‚x
âˆ‚p1
p1
âˆ‚x
âˆ‚q1


1 âˆ‚âˆ†U (x) q1 âˆ’ c
q1 âˆ’ c âˆ’1
(15 & 16)
= 1âˆ’
Â·
(0,
p
,
p
,
.
.
.
,
p
)
+
Â·
Â·
(0,
p
,
.
.
.
,
p
)
= 1.
2 3
n
2
n
p1
âˆ‚x
p21
p1
p1
(4)

5.2

Reflection

Now, we turn to the reflective case, and again show that volume is conserved. Again, we assume
without loss of generality that there is a boundary located at the hyperplane q1 = c.
Lemma 2. Let T : hq, pi â†’ hq0 , p0 i be a transformation in Rn that takes a unit mass located at
q := (q1 , . . . , qn ) and moves it with the constant momentum p := (p1 , . . . , pn ) till it reaches a plane
q1 = c (at some point x := (c, x2 , . . . , xn ) where c is a constant). Subsequently the mass is bounced
back (reflected) with momentum p0 = (âˆ’p1 , p2 , . . . , pn ) The move is carried on for a total time
period Ï„ till it ends in q0 . For all n âˆˆ N, T satisfies the volume preservation property.
Proof. Similar to Lemma 1, for i > 1, qi0 = qi +Ï„ Â·pi and p0i = pi . Therefore, for any j âˆˆ {2, . . . , n}
âˆ‚q 0
âˆ‚q 0
âˆ‚p0
âˆ‚p0
s.t. j 6= i, âˆ‚qji = âˆ‚pji = âˆ‚qji = âˆ‚pji = 0. Consequently, by equation (4), and since p01 = âˆ’p1 ,

 

 âˆ‚q10 âˆ‚q10   âˆ‚q10 âˆ‚q10  âˆ’âˆ‚q 0
 âˆ‚q1 âˆ‚p1   âˆ‚q1 âˆ‚p1 
1
|J| =  âˆ‚p0 âˆ‚p0  = 
(17)
=
1
1 

 0

âˆ‚q
âˆ’1
1
âˆ‚q
âˆ‚p
1

1

As before, let t1 be the time to reach x and t2 be the period between reaching x and the last point
def
def
0 def
0
1
q0 . That is, t1 = câˆ’q
p1 and t2 = Ï„ âˆ’ t1 . It follows that q1 = c + p1 Â· t2 is equal to 2c âˆ’ Ï„ p1 âˆ’ q1 .
Hence, |J| = 1.
5.3

Reflective Leapfrog Dynamics

Theorem 1. In RHMC (Algorithm 1) for sampling from a continuous and piecewise distribution P
which has affine partitioning boundaries, leapfrog simulation preserves volume in (q, p) space.
Proof. We split the algorithm into several atomic transformations Ti . Each transformation is either
(a) a momentum step, (b) a full position step with no reflection/refraction or (c) a full or partial
position step where exactly one reflection or refraction occurs.
6

To prove that the total algorithm preserves volume, it is sufficient to show that the volume is preserved under each Ti (i.e. |JTi (q, p)| = 1) since:
|JT1 oT2 oÂ·Â·Â·oTm | = |JT1 | Â· |JT2 | Â· Â· Â· |JTm |
Transformations of kind (a) and (b) are shear mappings and therefore they preserve the volume [9].
Now consider a (full or partial) position step where a single refraction occurs. If the reflective plane
is in form q1 = c, by lemma 1, the volume preservation property holds. Otherwise, as long as
the reflective plane is affine, via a rotation of basis vectors, the problem is reduced to the former
case. Since volume is conserved under rotation, in this case the volume is also conserved. With
similar reasoning, by lemma 2, reflection on a affine reflective boundary preserves volume. Thus,
since all component transformations of RHMC leapfrog simulation preserve volume, the proof is
complete.
Along with the fact that the leapfrog dynamics are time-reversible, this shows that the algorithm
satisfies detailed balance, and so has the correct stationary distribution.

6

Experiment

Compared to baseline HMC, we expect that RHMC will simulate Hamiltonian dynamics more accurately and therefore leads to fewer rejected samples. On the other hand, this comes at the expense
of slower leapfrog position steps since intersections, reflections and refractions must be computed.
To test the trade off, we compare the RHMC to baseline HMC [9] and tuned Metroplis-Hastings
(MH) with a simple isotropic Normal proposal distribution. MH is automatically tuned after [12] by
testing 100 equidistant proposal variances in interval (0, 1] and accepting a variance for which the
acceptance rate is closest to 0.24. The baseline HMC and RHMC number of steps L and step size 
are chosen to be 100 and 0.1 respectively. (Many other choices are in the Appendix.) While HMC
performance is highly standard to these parameters [7] RHMC is consistently faster.
The comparison takes place on a heavy tail piecewise model with (non-normalized) negative log
probability
ï£±p
ï£´
if kqkâˆ â‰¤ 3
ï£² q>pA q
(18)
U (q) = 1 + q> A q if 3 < kqkâˆ â‰¤ 6
ï£´
ï£³+âˆ,
otherwise
where A is a positive definite matrix. We carry out the experiment on three choices of (position
space) dimensionalites, n = 2, 10 and 50.
Due to the symmetry of the model, the ground truth expected value of q is known to be 0. Therefore,
the absolute error of the expected value (estimated by a chain q(1) , . . . , q(k) of MCMC samples) in
each dimension d = 1, . . . , n is the absolute value of the mean of d-th element of the sample vectors.
The worst mean absolute error (WMAE) over all dimensions is taken as the error measurement of
the chain.
 k

X


 1

(1)
(k)
s
WMAE q , . . . , q
=
max 
qd 
(19)

k d=1,...n 
s=1

For each algorithm, 20 Markov chains are run and the mean WMAE and 99% confidence intervals
(as error bars) versus the number of iterations (i.e. Markov chain sizes) are time (milliseconds) are
depicted in figure 2. All algorithms are implemented in java and run on a single thread of a 3.40GHz
CPU.
For each of the 20 repetitions, some random starting point is chosen uniformly and used for all three
of the algorithms. We use a diagonal matrix for A where, for each repetition, each entry on the main
diagonal is either exp(âˆ’5) or exp(5) with equal probabilities.
As the results show, even in low dimensions, the extra cost of the position step is more or less
compensated by its higher effective sample size but as the dimensionality increases, the RHMC
significantly outperforms both baseline HMC and tuned MH.
7

6

6

Baseline.HMC
Tuned.MH
Reflective.HMC

5

4

Error

Error

4

3

3

2

2

1

1

0 0
10

Baseline.HMC
Tuned.MH
Reflective.HMC

5

1

10

2

10

3

10

Iteration (dim=2, L=100, Îµ =0.1)

0

4

10

1

10

2

(a)
6

Baseline.HMC
Tuned.MH
Reflective.HMC

5

4

Error

Error

Baseline.HMC
Tuned.MH
Reflective.HMC

5

4

3

3

2

2

1

1

1

10

2

10

3

10

Iteration (dim=10, L=100, Îµ =0.1)

0

4

10

1

10

2

10

(e)
6

5

5

4

4

Baseline.HMC
Tuned.MH
Reflective.HMC

3

Error

Error

3

10

Time (dim=10, L=100, Îµ =0.1)

(b)
6

3

2

2

1

1

0 0
10

10

(d)

6

0 0
10

3

10

Time (dim=2, L=100, Îµ =0.1)

1

10

2

10

3

10

Iteration (dim=50, L=100, Îµ =0.1)

0

4

10

Baseline.HMC
Tuned.MH
Reflective.HMC
1

10

(c)

2

3

10

10

Time (dim=50, L=100, Îµ =0.1)

4

10

(f)

Figure 3: Error (worst mean absolute error per dimension) versus (a-c) iterations and (e-f) time (ms).
Tuned.HM is Metropolis Hastings with a tuned isotropic Gaussian proposal distribution. (Many more examples
in supplementary material.)

7

Conclusion

We have presented a modification of the leapfrog dynamics for Hamiltonian Monte Carlo for piecewise smooth energy functions with affine boundaries (i.e. each region is a polytope), inspired by
physical systems. Though traditional Hamiltonian Monte Carlo can in principle be used on such
functions, the fact that the Hamiltonian will often be dramatically changed by the dynamics can result in a very low acceptance ratio, particularly in high dimensions. By better preserving the Hamiltonian, reflective Hamiltonian Monte Carlo (RHMC) accepts more moves and thus has a higher
effective sample size, leading to much more efficient probabilistic inference. To use this method,
one must be able to detect the first intersection of a position trajectory with polytope boundaries.
Acknowledgements
NICTA is funded by the Australian Government through the Department of Communications and
the Australian Research Council through the ICT Centre of Excellence Program.
8

References
[1] Hadi Mohasel Afshar, Scott Sanner, and Ehsan Abbasnejad. Linear-time gibbs sampling in
piecewise graphical models. In Association for the Advancement of Artificial Intelligence,
pages 665â€“673, 2015.
[2] Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of Markov Chain
Monte Carlo. CRC press, 2011.
[3] Hans Adolph Buchdahl. An introduction to Hamiltonian optics. Courier Corporation, 1993.
[4] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid monte
carlo. Physics letters B, 195(2):216â€“222, 1987.
[5] Andrew G Glen, Lawrence M Leemis, and John H Drew. Computing the distribution of
the product of two continuous random variables. Computational statistics & data analysis,
44(3):451â€“464, 2004.
[6] Donald T Greenwood. Principles of dynamics. Prentice-Hall Englewood Cliffs, NJ, 1988.
[7] Matthew D Homan and Andrew Gelman. The no-u-turn sampler: Adaptively setting path
lengths in hamiltonian monte carlo. The Journal of Machine Learning Research, 15(1):1593â€“
1623, 2014.
[8] David Lunn, David Spiegelhalter, Andrew Thomas, and Nicky Best. The bugs project: evolution, critique and future directions. Statistics in medicine, 28(25):3049â€“3067, 2009.
[9] Radford M Neal. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte
Carlo, 2, 2011.
[10] Ari Pakman and Liam Paninski. Auxiliary-variable exact hamiltonian monte carlo samplers
for binary distributions. In Advances in Neural Information Processing Systems, pages 2490â€“
2498, 2013.
[11] Ari Pakman and Liam Paninski. Exact hamiltonian monte carlo for truncated multivariate
gaussians. Journal of Computational and Graphical Statistics, 23(2):518â€“542, 2014.
[12] Gareth O Roberts, Andrew Gelman, Walter R Gilks, et al. Weak convergence and optimal
scaling of random walk metropolis algorithms. The annals of applied probability, 7(1):110â€“
120, 1997.
[13] Stan Development Team. Stan Modeling Language Users Guide and Reference Manual, Version 2.5.0, 2014.

9

