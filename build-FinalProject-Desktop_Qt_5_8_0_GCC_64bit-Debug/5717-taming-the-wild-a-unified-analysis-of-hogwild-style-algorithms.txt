Taming the Wild: A Unified Analysis of
H OGWILD !-Style Algorithms

Christopher De Sa, Ce Zhang, Kunle Olukotun, and Christopher ReÃÅ
cdesa@stanford.edu, czhang@cs.wisc.edu,
kunle@stanford.edu, chrismre@stanford.edu
Departments of Electrical Engineering and Computer Science
Stanford University, Stanford, CA 94309

Abstract
Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of machine learning problems. Researchers and industry have developed several techniques to optimize SGD‚Äôs runtime performance, including asynchronous execution and reduced precision. Our main result is a martingale-based analysis that
enables us to capture the rich noise models that may arise from such techniques.
Specifically, we use our new analysis in three ways: (1) we derive convergence
rates for the convex case (H OGWILD !) with relaxed assumptions on the sparsity
of the problem; (2) we analyze asynchronous SGD algorithms for non-convex
matrix problems including matrix completion; and (3) we design and analyze
an asynchronous SGD algorithm, called B UCKWILD !, that uses lower-precision
arithmetic. We show experimentally that our algorithms run efficiently for a variety of problems on modern hardware.

1

Introduction

Many problems in machine learning can be written as a stochastic optimization problem
minimize E[fÀú(x)] over x ‚àà Rn ,
where fÀú is a random objective function. One popular method to solve this is with stochastic gradient
descent (SGD), an iterative method which, at each timestep t, chooses a random objective sample fÀút
and updates
xt+1 = xt ‚àí Œ±‚àáfÀút (xt ),
(1)
where Œ± is the step size. For most problems, this update step is easy to compute, and perhaps
because of this SGD is a ubiquitous algorithm with a wide range of applications in machine learning [1], including neural network backpropagation [2, 3, 13], recommendation systems [8, 19], and
optimization [20]. For non-convex problems, SGD is popular‚Äîin particular, it is widely used in
deep learning‚Äîbut its success is poorly understood theoretically.
Given SGD‚Äôs success in industry, practitioners have developed methods to speed up its computation.
One popular method to speed up SGD and related algorithms is using asynchronous execution.
In an asynchronous algorithm, such as H OGWILD ! [17], multiple threads run an update rule such
as Equation 1 in parallel without locks. H OGWILD ! and other lock-free algorithms have been
applied to a variety of uses, including PageRank approximations (FrogWild! [16]), deep learning
(Dogwild! [18]) and recommender systems [24]. Many asynchronous versions of other stochastic
algorithms have been individually analyzed, such as stochastic coordinate descent (SGD) [14, 15]
and accelerated parallel proximal coordinate descent (APPROX) [6], producing rate results that are
similar to those of H OGWILD ! Recently, Gupta et al. [9] gave an empirical analysis of the effects of
a low-precision variant of SGD on neural network training. Other variants of stochastic algorithms
1

have been proposed [5, 11, 12, 21, 22, 23]; only a fraction of these algorithms have been analyzed in
the asynchronous case. Unfortunately, a new variant of SGD (or a related algorithm) may violate the
assumptions of existing analysis, and hence there are gaps in our understanding of these techniques.
One approach to filling this gap is to analyze each purpose-built extension from scratch: an entirely
new model for each type of asynchrony, each type of precision, etc. In a practical sense, this may
be unavoidable, but ideally there would be a single technique that could analyze many models. In
this vein, we prove a martingale-based result that enables us to treat many different extensions as
different forms of noise within a unified model. We demonstrate our technique with three results:
1. For the convex case, H OGWILD ! requires strict sparsity assumptions. Using our techniques, we are able to relax these assumptions and still derive convergence rates. Moreover,
under H OGWILD !‚Äôs stricter assumptions, we recover the previous convergence rates.
2. We derive convergence results for an asynchronous SGD algorithm for a non-convex matrix
completion problem. We derive the first rates for asynchronous SGD following the recent
(synchronous) non-convex SGD work of De Sa et al. [4].
3. We derive convergence rates in the presence of quantization errors such as those introduced by fixed-point arithmetic. We validate our results experimentally, and show that
B UCKWILD ! can achieve speedups of up to 2.3√ó over H OGWILD !-based algorithms for
logistic regression.
One can combine these different methods both theoretically and empirically. We begin with our
main result, which describes our martingale-based approach and our model.

2

Main Result

Analyzing asynchronous algorithms is challenging because, unlike in the sequential case where there
is a single copy of the iterate x, in the asynchronous case each core has a separate copy of x in its
own cache. Writes from one core may take some time to be propagated to another core‚Äôs copy of
x, which results in race conditions where stale data is used to compute the gradient updates. This
difficulty is compounded in the non-convex case, where a series of unlucky random events‚Äîbad
initialization, inauspicious steps, and race conditions‚Äîcan cause the algorithm to get stuck near a
saddle point or in a local minimum.
Broadly, we analyze algorithms that repeatedly update x by running an update step
xt+1 = xt ‚àí GÃÉt (xt ),

(2)

for some i.i.d. update function GÃÉt . For example, for SGD, we would have G(x) = Œ±‚àáfÀút (x). The
goal of the algorithm must be to produce an iterate in some success region S‚Äîfor example, a ball
centered at the optimum x‚àó . For any T , after running the algorithm for T timesteps, we say that the
algorithm has succeeded if xt ‚àà S for some t ‚â§ T ; otherwise, we say that the algorithm has failed,
and we denote this failure event as FT .
Our main result is a technique that allows us to bound the convergence rates of asynchronous SGD
and related algorithms, even for some non-convex problems. We use martingale methods, which
have produced elegant convergence rate results for both convex and some non-convex [4] algorithms.
Martingales enable us to model multiple forms of error‚Äîfor example, from stochastic sampling,
random initialization, and asynchronous delays‚Äîwithin a single statistical model. Compared to
standard techniques, they also allow us to analyze algorithms that sometimes get stuck, which is
useful for non-convex problems. Our core contribution is that a martingale-based proof for the
convergence of a sequential stochastic algorithm can be easily modified to give a convergence rate
for an asynchronous version.
A supermartingale [7] is a stochastic process Wt such that E[Wt+1 |Wt ] ‚â§ Wt . That is, the expected
value is non-increasing over time. A martingale-based proof of convergence for the sequential version of this algorithm must construct a supermartingale Wt (xt , xt‚àí1 , . . . , x0 ) that is a function of
both the time and the current and past iterates; this function informally represents how unhappy we
are with the current state of the algorithm. Typically, it will have the following properties.
Definition 1. For a stochastic algorithm as described above, a non-negative process Wt : Rn√ót ‚Üí R
is a rate supermartingale with horizon B if the following conditions are true. First, it must be a
2

supermartingale; that is, for any sequence xt , . . . , x0 and any t ‚â§ B,
E[Wt+1 (xt ‚àí GÃÉt (xt ), xt , . . . , x0 )] ‚â§ Wt (xt , xt‚àí1 , . . . , x0 ).

(3)

Second, for all times T ‚â§ B and for any sequence xT , . . . , x0 , if the algorithm has not succeeded
by time T (that is, xt ‚àà
/ S for all t < T ), it must hold that
WT (xT , xT ‚àí1 , . . . , x0 ) ‚â• T.

(4)

This represents the fact that we are unhappy with running for many iterations without success.
Using this, we can easily bound the convergence rate of the sequential version of the algorithm.
Statement 1. Assume that we run a sequential stochastic algorithm, for which W is a rate supermartingale. For any T ‚â§ B, the probability that the algorithm has not succeeded by time T is
P (FT ) ‚â§

E[W0 (x0 )]
.
T

Proof. In what follows, we let Wt denote the actual value taken on by the function in a process
defined by (2). That is, Wt = Wt (xt , xt‚àí1 , . . . , x0 ). By applying (3) recursively, for any T ,
E[WT ] ‚â§ E[W0 ] = E[W0 (x0 )].
By the law of total expectation applied to the failure event FT ,
E[W0 (x0 )] ‚â• E[WT ] = P (FT ) E[WT |FT ] + P (¬¨FT ) E[WT |¬¨FT ].
Applying (4), i.e. E[WT |FT ] ‚â• T , and recalling that W is nonnegative results in
E[W0 (x0 )] ‚â• P (FT ) T ;
rearranging terms produces the result in Statement 1.
This technique is very general; in subsequent sections we show that rate supermartingales can be
constructed for SGD on all convex problems and for some algorithms for non-convex problems.
2.1

Modeling Asynchronicity

The behavior of an asynchronous SGD algorithm depends both on the problem it is trying to solve
and on the hardware it is running on. For ease of analysis, we assume that the hardware has the
following characteristics. These are basically the same assumptions used to prove the original H OG WILD ! result [17].
‚Ä¢ There are multiple threads running iterations of (2), each with their own cache. At any point
in time, these caches may hold different values for the variable x, and they communicate
via some cache coherency protocol.
‚Ä¢ There exists a central store S (typically RAM) at which all writes are serialized. This
provides a consistent value for the state of the system at any point in real time.
‚Ä¢ If a thread performs a read R of a previously written value X, and then writes another
value Y (dependent on R), then the write that produced X will be committed to S before
the write that produced Y .
‚Ä¢ Each write from an iteration of (2) is to only a single entry of x and is done using an atomic
read-add-write instruction. That is, there are no write-after-write races (handling these is
possible, but complicates the analysis).
Notice that, if we let xt denote the value of the vector x in the central store S after t writes have
occurred, then since the writes are atomic, the value of xt+1 is solely dependent on the single thread
that produces the write that is serialized next in S. If we let GÃÉt denote the update function sample
that is used by that thread for that write, and vt denote the cached value of x used by that write, then
xt+1 = xt ‚àí GÃÉt (vÃÉt )
3

(5)

Our hardware model further constrains the value of vÃÉt : all the read elements of vÃÉt must have been
written to S at some time before t. Therefore, for some nonnegative variable œÑÃÉi,t ,
eTi vÃÉt = eTi xt‚àíœÑÃÉi,t ,
(6)
where ei is the ith standard basis vector. We can think of œÑÃÉi,t as the delay in the ith coordinate
caused by the parallel updates.
We can conceive of this system as a stochastic process with two sources of randomness: the noisy update function samples GÃÉt and the delays œÑÃÉi,t . We assume that the GÃÉt are independent and identically
distributed‚Äîthis is reasonable because they are sampled independently by the updating threads. It
would be unreasonable, though, to assume the same for the œÑÃÉi,t , since delays may very well be correlated in the system. Instead, we assume that the delays are bounded from above by some random
variable œÑÃÉ . Specifically, if Ft , the filtration, denotes all random events that occurred before timestep
t, then for any i, t, and k,
P (œÑÃÉi,t ‚â• k|Ft ) ‚â§ P (œÑÃÉ ‚â• k) .
(7)
We let œÑ = E[œÑÃÉ ], and call œÑ the worst-case expected delay.
2.2

Convergence Rates for Asynchronous SGD

Now that we are equipped with a stochastic model for the asynchronous SGD algorithm, we show
how we can use a rate supermartingale to give a convergence rate for asynchronous algorithms. To
do this, we need some continuity and boundedness assumptions; we collect these into a definition,
and then state the theorem.
Definition 2. An algorithm with rate supermartingale W is (H, R, Œæ)-bounded if the following
conditions hold. First, W must be Lipschitz continuous in the current iterate with parameter H; that
is, for any t, u, v, and sequence xt , . . . , x0 ,
kWt (u, xt‚àí1 , . . . , x0 ) ‚àí Wt (v, xt‚àí1 , . . . , x0 )k‚â§ Hku ‚àí vk.
(8)
Second, GÃÉ must be Lipschitz continuous in expectation with parameter R; that is, for any u, and v,
E[kGÃÉ(u) ‚àí GÃÉ(v)k] ‚â§ Rku ‚àí vk1 .
Third, the expected magnitude of the update must be bounded by Œæ. That is, for any x,

(9)

E[kGÃÉ(x)k] ‚â§ Œæ.
(10)
Theorem 1. Assume that we run an asynchronous stochastic algorithm with the above hardware
model, for which W is a (H, R, Œæ)-bounded rate supermartingale with horizon B. Further assume
that HRŒæœÑ < 1. For any T ‚â§ B, the probability that the algorithm has not succeeded by time T is
E[W (0, x0 )]
P (FT ) ‚â§
.
(1 ‚àí HRŒæœÑ )T
Note that this rate depends only on the worst-case expected delay œÑ and not on any other properties
of the hardware model. Compared to the result of Statement 1, the probability of failure has only
increased by a factor of 1 ‚àí HRŒæœÑ . In most practical cases, HRŒæœÑ  1, so this increase in
probability is negligible.
Since the proof of this theorem is simple, but uses non-standard techniques, we outline it here.
First, notice that the process Wt , which was a supermartingale in the sequential case, is not in the
asynchronous case because of the delayed updates. Our strategy is to use W to produce a new
process Vt that is a supermartingale in this case. For any t and x¬∑ , if xu ‚àà
/ S for all u < t, we define
‚àû
‚àû
X
X
Vt (xt , . . . , x0 ) = Wt (xt , . . . , x0 ) ‚àí HRŒæœÑ t + HR
kxt‚àík+1 ‚àí xt‚àík k
P (œÑÃÉ ‚â• m) .
k=1

m=k

Compared with W , there are two additional terms here. The first term is negative, and cancels out
some of the unhappiness from (4) that we ascribed to running for many iterations. We can interpret
this as us accepting that we may need to run for more iterations than in the sequential case. The
second term measures the distance between recent iterates; we would be unhappy if this becomes
large because then the noise from the delayed updates would also be large. On the other hand, if
xu ‚àà S for some u < t, then we define
Vt (xt , . . . , xu , . . . , x0 ) = Vu (xu , . . . , x0 ).
4

We call Vt a stopped process because its value doesn‚Äôt change after success occurs. It is straightforward to show that Vt is a supermartingale for the asynchronous algorithm. Once we know this, the
same logic used in the proof of Statement 1 can be used to prove Theorem 1.
Theorem 1 gives us a straightforward way of bounding the convergence time of any asynchronous
stochastic algorithm. First, we find a rate supermartingale for the problem; this is typically no
harder than proving sequential convergence. Second, we find parameters such that the problem is
(H, R, Œæ)-bounded, typically ; this is easily done for well-behaved problems by using differentiation
to bound the Lipschitz constants. Third, we apply Theorem 1 to get a rate for asynchronous SGD.
Using this method, analyzing an asynchronous algorithm is really no more difficult than analyzing
its sequential analog.

3

Applications

Now that we have proved our main result, we turn our attention to applications. We show, for
a couple of algorithms, how to construct a rate supermartingale. We demonstrate that doing this
allows us to recover known rates for H OGWILD ! algorithms as well as analyze cases where no
known rates exist.
3.1

Convex Case, High Precision Arithmetic

First, we consider the simple case of using asynchronous SGD to minimize a convex function f (x)
using unbiased gradient samples ‚àáfÀú(x). That is, we run the update rule
xt+1 = xt ‚àí Œ±‚àáfÀút (x).
(11)
We make the standard assumption that f is strongly convex with parameter c; that is, for all x and y
(x ‚àí y)T (‚àáf (x) ‚àí ‚àáf (y)) ‚â• ckx ‚àí yk2 .
We also assume continuous differentiability of ‚àáfÀú with 1-norm Lipschitz constant L,
E[k‚àáfÀú(x) ‚àí ‚àáfÀú(y)k] ‚â§ Lkx ‚àí yk1 .

(12)
(13)

We require that the second moment of the gradient sample is also bounded for some M > 0 by
E[k‚àáfÀú(x)k2 ] ‚â§ M 2 .
(14)
For some  > 0, we let the success region be
S = {x|kx ‚àí x‚àó k2 ‚â§ }.
Under these conditions, we can construct a rate supermartingale for this algorithm.
Lemma 1. There exists a Wt where, if the algorithm hasn‚Äôt succeeded by timestep t,



‚àó 2 ‚àí1
Wt (xt , . . . , x0 ) =
log
e
kx
‚àí
x
k

+ t,
t
2Œ±c ‚àí Œ±2 M 2
such that Wt is a rate submartingale for the above
‚àö algorithm with horizon B = ‚àû. Furthermore, it
is (H, R, Œæ)-bounded with parameters: H = 2 (2Œ±c ‚àí Œ±2 M 2 )‚àí1 , R = Œ±L, and Œæ = Œ±M .
Using this and Theorem 1 gives us a direct bound on the failure rate of convex H OGWILD ! SGD.
Corollary 1. Assume that we run an asynchronous version of the above SGD algorithm, where for
some constant œë ‚àà (0, 1) we choose step size
cœë
‚àö .
Œ±= 2
M + 2LM œÑ 
Then for any T , the probability that the algorithm has not succeeded by time T is
‚àö


M 2 + 2LM œÑ 
‚àó 2 ‚àí1
P (FT ) ‚â§
log
e
kx
‚àí
x
k

.
0
c2 œëT
This result is more general than the result in Niu et al. [17]. The main differences are: that we make
no assumptions about the sparsity structure of the gradient samples; and that our rate depends only
on the second moment of GÃÉ and the expected value of œÑÃÉ , as opposed to requiring absolute bounds
on their magnitude. Under their stricter assumptions, the result of Corollary 1 recovers their rate.
5

3.2

Convex Case, Low Precision Arithmetic

One of the ways B UCKWILD ! achieves high performance is by using low-precision fixed-point
arithmetic. This introduces additional noise to the system in the form of round-off error. We consider
this error to be part of the B UCKWILD ! hardware model. We assume that the round-off error can
be modeled by an unbiased rounding function operating on the update samples. That is, for some
chosen precision factor Œ∫, there is a random quantization function QÃÉ such that, for any x ‚àà R, it
holds that E[QÃÉ(x)] = x, and the round-off error is bounded by |QÃÉ(x) ‚àí x|< Œ±Œ∫M . Using this
function, we can write a low-precision asynchronous update rule for convex SGD as


xt+1 = xt ‚àí QÃÉt Œ±‚àáfÀút (vÃÉt ) ,
(15)
where QÃÉt operates only on the single nonzero entry of ‚àáfÀút (vÃÉt ). In the same way as we did in the
high-precision case, we can use these properties to construct a rate supermartingale for the lowprecision version of the convex SGD algorithm, and then use Theorem 1 to bound the failure rate of
convex B UCKWILD !
Corollary 2. Assume that we run asynchronous low-precision convex SGD, and for some œë ‚àà (0, 1),
we choose step size
cœë
‚àö ,
Œ±= 2
M (1 + Œ∫2 ) + LM œÑ (2 + Œ∫2 ) 
then for any T , the probability that the algorithm has not succeeded by time T is
‚àö


M 2 (1 + Œ∫2 ) + LM œÑ (2 + Œ∫2 ) 
‚àó 2 ‚àí1
log
e
kx
‚àí
x
k

P (FT ) ‚â§
.
0
c2 œëT
Typically, we choose a precision such that Œ∫  1; in this case, the increased error compared to the
result of Corollary 1 will be negligible and we will converge in a number of samples that is very
similar to the high-precision, sequential case. Since each B UCKWILD ! update runs in less time than
an equivalent H OGWILD ! update, this result means that an execution of B UCKWILD ! will produce
same-quality output in less wall-clock time compared with H OGWILD !
3.3

Non-Convex Case, High Precision Arithmetic

Many machine learning problems are non-convex, but are still solved in practice with SGD. In this
section, we show that our technique can be adapted to analyze non-convex problems. Unfortunately,
there are no general convergence results that provide rates for SGD on non-convex problems, so it
would be unreasonable to expect a general proof of convergence for non-convex H OGWILD ! Instead,
we focus on a particular problem, low-rank least-squares matrix completion,
minimize E[kAÃÉ ‚àí xxT k2F ]
(16)
subject to x ‚àà Rn ,
for which there exists a sequential SGD algorithm with a martingale-based rate that has already
been proven. This problem arises in general data analysis, subspace tracking, principle component
analysis, recommendation systems, and other applications [4]. In what follows, we let A = E[AÃÉ].
We assume that A is symmetric, and has unit eigenvectors u1 , u2 , . . . , un with corresponding eigenvalues Œª1 > Œª2 ‚â• ¬∑ ¬∑ ¬∑ ‚â• Œªn . We let ‚àÜ, the eigengap, denote ‚àÜ = Œª1 ‚àí Œª2 .
De Sa et al. [4] provide a martingale-based rate of convergence for a particular SGD algorithm,
Alecton, running on this problem. For simplicity, we focus on only the rank-1 version of the problem, and we assume that, at each timestep, a single entry of A is used as a sample. Under these
conditions, Alecton uses the update rule
xt+1 = (I + Œ∑n2 eiÃÉt eTiÃÉt AejÃÉt eTjÃÉt )xt ,

(17)

where iÃÉt and jÃÉt are randomly-chosen indices in [1, n]. It initializes x0 uniformly on the sphere of
some radius centered at the origin. We can equivalently think of this as a stochastic power iteration
algorithm. For any  > 0, we define the success set S to be
2

S = {x|(uT1 x)2 ‚â• (1 ‚àí ) kxk }.
(18)
That is, we are only concerned with the direction of x, not its magnitude; this algorithm only recovers
the dominant eigenvector of A, not its eigenvalue. In order to show convergence for this entrywise
sampling scheme, De Sa et al. [4] require that the matrix A satisfy a coherence bound [10].
6

Table 1: Training loss of SGD as a function of arithmetic precision for logistic regression.
Dataset
Reuters
Forest
RCV1
Music

Rows
8K
581K
781K
515K

Columns
18K
54
47K
91

Size
1.2GB
0.2GB
0.9GB
0.7GB

32-bit float
0.5700
0.6463
0.1888
0.8785

16-bit int
0.5700
0.6463
0.1888
0.8785

8-bit int
0.5709
0.6447
0.1879
0.8781

Definition 3. A matrix A ‚àà Rn√ón is incoherent with parameter ¬µ if for every standard basis vector
ej , and for all unit eigenvectors ui of the matrix, (eTj ui )2 ‚â§ ¬µ2 n‚àí1 .
They also require that the step size be set, for some constants 0 < Œ≥ ‚â§ 1 and 0 < œë < (1 + )‚àí1 as
Œ∑=

‚àÜŒ≥œë
2

2n¬µ4 kAkF

.

For ease of analysis, we add the additional assumptions that our algorithm runs in some bounded
space. That is, for some constant C, at all times t, 1 ‚â§ kxt k and kxt k1 ‚â§ C. As in the convex
case, by following the martingale-based approach of De Sa et al. [4], we are able to generate a rate
supermartinagle for this algorithm‚Äîto save space, we only state its initial value and not the full
expression.
Lemma 2. For the problem above, choose any horizon B such that Œ∑Œ≥‚àÜB ‚â§ 1. Then there exists
a function Wt such that Wt is a rate supermartingale for the above non-convex SGD algorithm with
1
parameters H = 8nŒ∑ ‚àí1 Œ≥ ‚àí1 ‚àÜ‚àí1 ‚àí 2 , R = Œ∑¬µ kAkF , and Œæ = Œ∑¬µ kAkF C, and
p
E [W0 (x0 )] ‚â§ 2Œ∑ ‚àí1 ‚àÜ‚àí1 log(enŒ≥ ‚àí1 ‚àí1 ) + B 2œÄŒ≥.
Note that the analysis parameter Œ≥ allows us to trade off between B, which determines how long we
can run the algorithm, and the initial value of the supermartingale E [W0 (x0 )]. We can now produce
a corollary about the convergence rate by applying Theorem 1 and setting B and T appropriately.
Corollary 3. Assume that we run H OGWILD ! Alecton under these conditions for T timesteps, as
defined below. Then the probability of failure, P (FT ), will be bounded as below.
2

T =

4n¬µ4 kAkF
‚àö
log
‚àÜ2 Œ≥œë 2œÄŒ≥



en
Œ≥

‚àö


P (FT ) ‚â§

,

¬µ2

8œÄŒ≥¬µ2
‚àö .
‚àí 4CœëœÑ 

The fact that we are able to use our technique to analyze a non-convex algorithm illustrates its
generality. Note that it is possible to combine our results to analyze asynchronous low-precision
non-convex SGD, but the resulting formulas are complex, so we do not include them here.

4

Experiments

We validate our theoretical results for both asynchronous non-convex matrix completion and B UCK WILD !, a H OGWILD ! implementation with lower-precision arithmetic. Like H OGWILD !, a B UCK WILD ! algorithm has multiple threads running an update rule (2) in parallel without locking. Compared with H OGWILD !, which uses 32-bit floating point numbers to represent input data, B UCK WILD ! uses limited-precision arithmetic by rounding the input data to 8-bit or 16-bit integers. This
not only decreases the memory usage, but also allows us to take advantage of single-instructionmultiple-data (SIMD) instructions for integers on modern CPUs.
We verified our main claims by running H OGWILD ! and B UCKWILD ! algorithms on the discussed
applications. Table 1 shows how the training loss of SGD for logistic regression, a convex problem,
varies as the precision is changed. We ran SGD with step size Œ± = 0.0001; however, results are
similar across a range of step sizes. We analyzed all four datasets reported in DimmWitted [25] that
favored H OGWILD !: Reuters and RCV1, which are text classification datasets; Forest, which arises
from remote sensing; and Music, which is a music classification dataset. We implemented all GLM
models reported in DimmWitted, including SVM, Linear Regression, and Logistic Regression, and
7

2

5
4
3

1

2
32-bit float
16-bit int
8-bit int

1
0

1

4

12
threads

24

(uT1 x)2 kxk‚àí2

6

Hogwild vs. Sequential Alecton for n = 106

speedup over 32-bit best H OGWILD !

speedup over 32-bit sequential

Performance of B UCKWILD ! for Logistic Regression

(a) Speedup of B UCKWILD ! for dense RCV1
dataset.

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0

0.2

0.4

sequential
12-thread hogwild
0.6 0.8
1
1.2 1.4
iterations (billions)

1.6

(b) Convergence trajectories for sequential versus H OGWILD ! Alecton.

Figure 1: Experiments compare the training loss, performance, and convergence of H OGWILD ! and
B UCKWILD ! algorithms with sequential and/or high-precision versions.
report Logistic Regression because other models have similar performance. The results illustrate
that there is almost no increase in training loss as the precision is decreased for these problems. We
also investigated 4-bit and 1-bit computation: the former was slower than 8-bit due to a lack of 4-bit
SIMD instructions, and the latter discarded too much information to produce good quality results.
Figure 1(a) displays the speedup of B UCKWILD ! running on the dense-version of the RCV1 dataset
compared to both full-precision sequential SGD (left axis) and best-case H OGWILD ! (right axis).
Experiments ran on a machine with two Xeon X650 CPUs, each with six hyperthreaded cores, and
24GB of RAM. This plot illustrates that incorporating low-precision arithmetic into our algorithm
allows us to achieve significant speedups over both sequential and H OGWILD ! SGD. (Note that we
don‚Äôt get full linear speedup because we are bound by the available memory bandwidth; beyond
this limit, adding additional threads provides no benefits while increasing conflicts and thrashing
the L1 and L2 caches.) This result, combined with the data in Table 1, suggest that by doing lowprecision asynchronous updates, we can get speedups of up to 2.3√ó on these sorts of datasets without
a significant increase in error.
Figure 1(b) compares the convergence trajectories of H OGWILD ! and sequential versions of the nonconvex Alecton matrix completion algorithm on a synthetic data matrix A ‚àà Rn√ón with ten random
eigenvalues Œªi > 0. Each plotted series represents a different run of Alecton; the trajectories differ
somewhat because of the randomness of the algorithm. The plot shows that the sequential and
asynchronous versions behave qualitatively similarly, and converge to the same noise floor. For this
dataset, sequential Alecton took 6.86 seconds to run while 12-thread H OGWILD ! Alecton took 1.39
seconds, a 4.9√ó speedup.

5

Conclusion

This paper presented a unified theoretical framework for producing results about the convergence
rates of asynchronous and low-precision random algorithms such as stochastic gradient descent. We
showed how a martingale-based rate of convergence for a sequential, full-precision algorithm can
be easily leveraged to give a rate for an asynchronous, low-precision version. We also introduced
B UCKWILD !, a strategy for SGD that is able to take advantage of modern hardware resources for
both task and data parallelism, and showed that it achieves near linear parallel speedup over sequential algorithms.
Acknowledgments
The B UCKWILD ! name arose out of conversations with Benjamin Recht. Thanks also to Madeleine Udell
for helpful conversations. The authors acknowledge the support of: DARPA FA8750-12-2-0335; NSF IIS1247701; NSF CCF-1111943; DOE 108845; NSF CCF-1337375; DARPA FA8750-13-2-0039; NSF IIS1353606; ONR N000141210041 and N000141310129; NIH U54EB020405; Oracle; NVIDIA; Huawei; SAP
Labs; Sloan Research Fellowship; Moore Foundation; American Family Insurance; Google; and Toshiba.

8

References
[1] LeÃÅon Bottou. Large-scale machine learning with stochastic gradient descent. In COMPSTAT‚Äô2010, pages
177‚Äì186. Springer, 2010.
[2] LeÃÅon Bottou. Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade, pages 421‚Äì436.
Springer, 2012.
[3] LeÃÅon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In J.C. Platt, D. Koller, Y. Singer,
and S. Roweis, editors, NIPS, volume 20, pages 161‚Äì168. NIPS Foundation, 2008.
[4] Christopher De Sa, Kunle Olukotun, and Christopher ReÃÅ. Global convergence of stochastic gradient
descent for some nonconvex matrix problems. ICML, 2015.
[5] John C Duchi, Peter L Bartlett, and Martin J Wainwright. Randomized smoothing for stochastic optimization. SIAM Journal on Optimization, 22(2):674‚Äì701, 2012.
[6] Olivier Fercoq and Peter RichtaÃÅrik. Accelerated, parallel and proximal coordinate descent. arXiv preprint
arXiv:1312.5799, 2013.
[7] Thomas R Fleming and David P Harrington. Counting processes and survival analysis. volume 169,
pages 56‚Äì57. John Wiley & Sons, 1991.
[8] Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, and Reza Zadeh. WTF: The who
to follow service at twitter. WWW ‚Äô13, pages 505‚Äì514, 2013.
[9] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. ICML, 2015.
[10] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating
minimization. In STOC, pages 665‚Äì674. ACM, 2013.
[11] BjoÃàrn Johansson, Maben Rabi, and Mikael Johansson. A randomized incremental subgradient method for
distributed optimization in networked systems. SIAM Journal on Optimization, 20(3):1157‚Äì1170, 2009.
[12] Jakub KonecnyÃÄ, Zheng Qu, and Peter RichtaÃÅrik. S2cd: Semi-stochastic coordinate descent. In NIPS
Optimization in Machine Learning workshop, 2014.
[13] Yann Le Cun, LeÃÅon Bottou, Genevieve B. Orr, and Klaus-Robert MuÃàller. Efficient backprop. In Neural
Networks, Tricks of the Trade. 1998.
[14] Ji Liu and Stephen J. Wright. Asynchronous stochastic coordinate descent: Parallelism and convergence
properties. SIOPT, 25(1):351‚Äì376, 2015.
[15] Ji Liu, Stephen J Wright, Christopher ReÃÅ, Victor Bittorf, and Srikrishna Sridhar. An asynchronous parallel
stochastic coordinate descent algorithm. JMLR, 16:285‚Äì322, 2015.
[16] Ioannis Mitliagkas, Michael Borokhovich, Alexandros G. Dimakis, and Constantine Caramanis. Frogwild!: Fast pagerank approximations on graph engines. PVLDB, 2015.
[17] Feng Niu, Benjamin Recht, Christopher Re, and Stephen Wright. Hogwild: A lock-free approach to
parallelizing stochastic gradient descent. In NIPS, pages 693‚Äì701, 2011.
[18] Cyprien Noel and Simon Osindero. Dogwild!‚ÄìDistributed Hogwild for CPU & GPU. 2014.
[19] Shameem Ahamed Puthiya Parambath. Matrix factorization methods for recommender systems. 2013.
[20] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly
convex stochastic optimization. ICML, 2012.
[21] Peter RichtaÃÅrik and Martin TakaÃÅcÃå. Parallel coordinate descent methods for big data optimization. Mathematical Programming, pages 1‚Äì52, 2012.
[22] Qing Tao, Kang Kong, Dejun Chu, and Gaowei Wu. Stochastic coordinate descent methods for regularized smooth and nonsmooth losses. In Machine Learning and Knowledge Discovery in Databases, pages
537‚Äì552. Springer, 2012.
[23] Rachael Tappenden, Martin TakaÃÅcÃå, and Peter RichtaÃÅrik. On the complexity of parallel coordinate descent.
arXiv preprint arXiv:1503.03033, 2015.
[24] Hsiang-Fu Yu, Cho-Jui Hsieh, Si Si, and Inderjit S Dhillon. Scalable coordinate descent approaches to
parallel matrix factorization for recommender systems. In ICDM, pages 765‚Äì774, 2012.
[25] Ce Zhang and Christopher Re. Dimmwitted: A study of main-memory statistical analytics. PVLDB,
2014.

9

