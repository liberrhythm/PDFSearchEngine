Unified View of Matrix Completion under General
Structural Constraints
Suriya Gunasekar
UT at Austin, USA
suriya@utexas.edu

Arindam Banerjee
UMN Twin Cities, USA
banerjee@cs.umn.edu

Joydeep Ghosh
UT at Austin, USA
ghosh@ece.utexas.edu

Abstract
In this paper, we present a unified analysis of matrix completion under general
low-dimensional structural constraints induced by any norm regularization. We
consider two estimators for the general problem of structured matrix completion,
and provide unified upper bounds on the sample complexity and the estimation
error. Our analysis relies on results from generic chaining, and we establish
two intermediate results of independent interest: (a) in characterizing the size
or complexity of low dimensional subsets in high dimensional ambient space, a
certain partial complexity measure encountered in the analysis of matrix completion problems is characterized in terms of a well understood complexity measure
of Gaussian widths, and (b) it is shown that a form of restricted strong convexity
holds for matrix completion problems under general norm regularization. Further,
we provide several non-trivial examples of structures included in our framework,
notably the recently proposed spectral k-support norm.

1

Introduction

The task of completing the missing entries of a matrix from an incomplete subset of (potentially
noisy) entries is encountered in many applications including recommendation systems, data imputation, covariance matrix estimation, and sensor localization among others. Traditionally ill‚Äìposed
high dimensional estimation problems, where the number of parameters to be estimated is much
higher than the number of observations, has been extensively studied in the recent literature. However, matrix completion problems are particularly ill‚Äìposed as the observations are both limited
(high dimensional), and the measurements are extremely localized, i.e., the observations consist of
individual matrix entries. The localized measurement model, in contrast to random Gaussian or
sub‚ÄìGaussian measurements, poses additional complications in high dimensional estimation.
For well‚Äìposed estimation in high dimensional problems including matrix completion, it is imperative that low dimensional structural constraints are imposed on the target. For matrix completion,
the special case of low‚Äìrank constraint has been widely studied. Several existing work propose
tractable estimators with near‚Äìoptimal recovery guarantees for (approximate) low‚Äìrank matrix completion [8, 7, 28, 26, 18, 19, 22, 11, 20, 21]. A recent work [16] addresses the extension to structures
with decomposable norm regularization. However, the scope of matrix completion extends for low
dimensional structures far beyond simple low‚Äìrankness or decomposable norm structures.
In this paper, we present a unified statistical analysis of matrix completion under a general set of low
dimensional structures that are induced by any suitable norm regularization. We provide statistical
analysis of two generalized matrix completion estimators, the constrained norm minimizer, and the
generalized matrix Dantzig selector (Section 2.2). The main results in the paper (Theorem 1a‚Äì1b)
provide unified upper bounds on the sample complexity and estimation error of these estimators for
matrix completion under any norm regularization. Existing results on matrix completion with low
rank or other decomposable structures can be obtained as special cases of our general results.
1

Our unified analysis of sample complexity is motivated by recent work on high dimensional estimation using global (sub) Gaussian measurements [10, 1, 35, 3, 37, 5]. A key ingredient in the recovery
analysis of high dimensional estimation involves establishing a certain variation of Restricted Isometry Property (RIP) [9] of the measurement operator. It has been shown that such properties are satisfied by Gaussian and sub‚ÄìGaussian measurement operators with high probability. Unfortunately,
as has been noted before by Candes et al. [8], owing to highly localized measurements, such conditions are not satisfied in the matrix completion problem, and the existing results based on global
(sub) Gaussian measurements are not directly applicable. In fact, a key question we consider is:
given the radically limited measurement model in matrix completion, by how much would the sample complexity of estimation increase beyond the known sample complexity bounds for global (sub)
Gaussian measurements. Our results upper bounds the sample complexity for matrix completion to
within a log d factor over that for estimation under global (sub) Gaussian measurements [10, 3, 5].
While the result was previously known for low rank matrix completion using nuclear norm minimization [26, 20], with a careful use of generic chaining, we show that the log d factor suffices
for structures induced by any norm! As a key intermediate result, we show that a useful form of
restricted strong convexity (RSC) [27] holds for the localized measurements encountered in matrix
completion under general norm regularized structures. The result substantially generalizes existing
RSC results for matrix completion under the special cases of nuclear norm and decomposable norm
regularization [26, 16].
For our analysis, we use tools from generic chaining [33] to characterize the main results (Theorem 1a‚Äì1b) in terms of the Gaussian width (Definition 1) of certain error sets. Gaussian widths
provide a powerful geometric characterization for quantifying the complexity of a structured low dimensional subset in a high dimensional ambient space. Numerous tools have been developed in the
literature for bounding the Gaussian width of structured sets. A unified characterization of results in
terms of Gaussian width has the advantage that this literature can be readily leveraged to derive new
recovery guarantees for matrix completion under suitable structural constraints (Appendix D.2).
In addition to the theoretical elegance of such a unified framework, identifying useful but potentially
non‚Äìdecomposable low dimensional structures is of significant practical interest. The broad class
of structures enforced through symmetric convex bodies and symmetric atomic sets [10] can be
analyzed under this paradigm (Section 2.1). Such specialized structures can capture the constraints
in certain applications better than simple low‚Äìrankness. In particular, we discuss in detail, a non‚Äì
trivial example of the spectral k‚Äìsupport norm introduced by McDonald et al. [25].
To summarize the key contributions of the paper:
‚Ä¢ Theorem 1a‚Äì1b provide unified upper bounds on sample complexity and estimation error for
matrix completion estimators using general norm regularization: a substantial generalization of the
existing results on matrix completion under structural constraints.
‚Ä¢ Theorem 1a is applied to derive statistical results for the special case of matrix completion under
spectral k‚Äìsupport norm regularization.
‚Ä¢ An intermediate result, Theorem 5 shows that under any norm regularization, a variant of Restricted Strong Convexity (RSC) holds in the matrix completion setting with extremely localized
measurements. Further, a certain partial measure of complexity of a set is encountered in matrix
completion analysis (12). Another intermediate result, Theorem 2 provides bounds on the partial complexity measures in terms of a better understood complexity measure of Gaussian width.
These intermediate results are of independent interest beyond the scope of the paper.
Notations and Preliminaries
Indexes i, j are typically used to index rows and columns respectively of matrices, and index k is
used to index the observations. ei , ej , ek , etc. denote the standard basis in appropriate dimensions‚àó .
Notation G and g are used to denote a matrix and vector respectively, with independent standard
Gaussian random variables. P(.) and E(.) denote the probability of an event and the expectation of
a random variable, respectively. Given
p an integer N , let [N ] = {1, 2, . . . , N }. Euclidean norm in a
vector space is denoted as kxk2 = hx, xi. For a matrix
‚â• . . .,
pPX with singular values œÉ1 ‚â• œÉ2 P
2 , the nuclear norm kXk =
common norms include the Frobenius norm kXkF =
œÉ
‚àó
i i
i œÉi ,
d1 d2 ‚àí1
the spectral norm kXkop = œÉ1 , and the maximum norm kXk‚àû = maxij |Xij |. Also let, S
=
‚àó

for brevity we omit the explicit dependence of dimension unless necessary

2

{X ‚àà Rd1√ód2 : kXkF = 1} and Bd1 d2 = {X ‚àà Rd1√ód2 : kXkF ‚â§ 1}. Finally, given a norm k.k
defined on a vectorspace V, its dual norm is given by kXk‚àó = supkY k‚â§1 hX, Y i.
Definition 1 (Gaussian Width). Gaussian width of a set S ‚äÇ Rd1√ód2 is a widely studied measure of
complexity of a subset in high dimensional ambient space and is given by:
wG (S) = EG sup hX, Gi,
(1)
X‚ààS

where recall that G is a matrix of independent standard Gaussian random variables. Some key results
on Gaussian width are discussed in Appendix D.2.
Definition 2 (Sub‚ÄìGaussian Random Variable [36]). The sub‚ÄìGaussian norm of a random variable
X is given by: kXkŒ®2 = supp‚â•1 p‚àí1/2 (E|X|p )1/p . X is b‚Äìsub‚ÄìGaussian if kXkŒ®2 ‚â§ b < ‚àû.
Equivalently, X is sub‚ÄìGaussian if one of the following conditions are satisfied for some constants
k1 , k2 , and k3 [Lemma 5.5 of [36]].
2
2 2
‚àö
(1) ‚àÄp ‚â• 1, (E|X|p )1/p ‚â§ b p,
(2) ‚àÄt > 0, P(|X| > t) ‚â§ e1‚àít /k1 b ,
2
2
2 2
(3) E[ek2 X /b ] ‚â§ e, or
(4) if EX = 0, then ‚àÄs > 0, E[esX ] ‚â§ ek3 s b /2 .
Definition 3 (Restricted Strong Convexity (RSC)). A function L is said to satisfy Restricted Strong
Convexity (RSC) at Œò with respect to a subset S, if for some RSC parameter Œ∫L > 0,
‚àÄ‚àÜ ‚àà S, L(Œò + ‚àÜ) ‚àí L(Œò) ‚àí h‚àáL(Œò), ‚àÜi ‚â• Œ∫L k‚àÜk2F .
(2)
1√ód2
Definition 4 (Spikiness Ratio [26]). For X ‚àà Rd‚àö
, a measure of its ‚Äúspikiness‚Äù is given by:
d1 d2 kXk‚àû
.
(3)
Œ±sp (X) =
kXkF
Definition 5 (Norm Compatibility Constant [27]). The compatibility constant of a norm R : V ‚Üí R
under a closed convex cone C ‚äÇ V is defined as follows:
R(X)
Œ®R (C) = sup
.
(4)
X‚ààC\{0} kXkF

2

Structured Matrix Completion

Denote the ground truth target matrix as Œò‚àó ‚àà Rd1√ód2 ; let d = d1 + d2 . In the noisy matrix completion, observations consists of individual entries of Œò‚àó observed through an additive noise channel.
Sub‚ÄìGaussian Noise: Given, a list of independently sampled standard basis ‚Ñ¶ = {Ek = eik e>
jk :
ik ‚àà [d1 ], jk ‚àà [d2 ]} with potential duplicates, observations (yk )k ‚àà R|‚Ñ¶| are given by:
yk = hŒò‚àó , Ek i + ŒæŒ∑k , for k = 1, 2, . . . , |‚Ñ¶|,
(5)
|‚Ñ¶|
where Œ∑ ‚àà R is the noise vector of independent sub‚ÄìGaussian random variables with E[Œ∑k ] = 0
and Var(Œ∑k ) = 1, and Œæ 2 is scaled variance of noise per observation. Further let kŒ∑k kŒ®2 ‚â§ b for a
constant b (recall k.kŒ®2 from Definition 2). Also, without loss of generality, assume normalization
kŒò‚àó kF = 1.
Uniform Sampling: Assume that the entries in ‚Ñ¶ are drawn independently and uniformly:
Ek ‚àº uniform{ei e>
(6)
j : i ‚àà [d1 ], j ‚àà [d2 ]}, for Ek ‚àà ‚Ñ¶.
Let {ek } be the standard basis of R|‚Ñ¶| . Given ‚Ñ¶, define P‚Ñ¶ : Rd1√ód2 ‚Üí R|‚Ñ¶| as:
P|‚Ñ¶|
P‚Ñ¶ (X) = k=1 hX, Ek iek
(7)
Structural Constraints For matrix completion with |‚Ñ¶| < d1 d2 , low dimensional structural constraints on Œò‚àó are necessary for well‚Äìposedness. We consider a generalized constraint setting
wherein for some low‚Äìdimensional model space M, Œò‚àó ‚àà M is enforced through a surrogate
norm regularizer R(.). We make no further assumptions on R other than it being a norm in Rd1√ód2 .
Low Spikiness In matrix completion under uniform sampling model, further restrictions on Œò‚àó (beyond low dimensional structure) are required to ensure that the most informative entries of the matrix
are observed with high probability [8]. Early work assumed stringent matrix incoherence conditions
for low‚Äìrank completion to preclude such matrices [7, 18, 19], while more recent work [11, 26], relax these assumptions to a more intuitive restriction of the spikiness ratio, defined in (3). However,
under this relaxation only an approximate recovery is typically guaranteed in low‚Äìnoise regime, as
opposed to near exact recovery under incoherence assumptions [26, 11].
Assumption 1 (Spikiness Ratio). There exists Œ±‚àó > 0, such that
‚àó
‚àó
kF
‚àö
‚â§ ‚àödŒ± d .

kŒò‚àó k‚àû = Œ±sp (Œò‚àó ) kŒò
d d
1 2

3

1 2

2.1

Special Cases and Applications

We briefly introduce some interesting examples of structural constraints with practical applications.
Example 1 (Low Rank and Decomposable Norms). Low‚Äìrankness is the most common structure
used in many matrix estimation problems including collaborative filtering, PCA, spectral clustering,
etc. Convex estimators using nuclear norm kŒòk‚àó regularization has been widely studied statistically
[8, 7, 28, 26, 18, 19, 22, 11, 20, 21]. A recent work [16] extends the analysis of low rank matrix
completion to general decomposable norms, i.e. R : ‚àÄX, Y ‚àà (M, M‚ä• ), R(X+Y ) = R(X)+R(Y ).
Example 2 (Spectral k‚Äìsupport Norm). A non‚Äìtrivial and significant example of norm regularization that is not decomposable is the spectral k‚Äìsupport norm recently introduced by McDonald et al. [25]. Spectral k‚Äìsupport norm is essentially the vector k‚Äìsupport norm [2] applied on the
singular values œÉ(Œò) of a matrix Œò ‚àà Rd1√ód2 . Without loss of generality, let d¬Ø = d1 = d2 .
¬Ø : |g| ‚â§ k} be the set of all subsets [d]
¬Ø of cardinality at most k, and let
Let Gk = {g ‚äÜ [d]
d¬Ø
V(Gk ) = {(vg )g‚ààGk : vg ‚àà R , supp(vg ) ‚äÜ g}. The spectral k‚Äìsupport norm is given by:
nX
o
X
kŒòkk‚Äìsp = inf
kvg k2 :
vg = œÉ(Œò) ,
(8)
v‚ààV(Gk )

g‚ààGk

g‚ààGk

McDonald et al. [25] showed that spectral k‚Äìsupport norm is a special case of cluster norm [17]. It
was further shown that in multi‚Äìtask learning, wherein the tasks (columns of Œò‚àó ) are assumed to be
clustered into dense groups, the cluster norm provides a trade‚Äìoff between intra‚Äìcluster variance,
(inverse) inter‚Äìcluster variance, and the norm of the task vectors. Both [17] and [25] demonstrate
superior empirical performance of cluster norms (and k‚Äìsupport norm) over traditional trace norm
and spectral elastic net minimization on bench marked matrix completion and multi‚Äìtask learning
datasets. However, statistical analysis of consistent matrix completion using spectral k‚Äìsupport
norm regularization has not been previously studied. In Section 3.2, we discuss the consequence of
our main theorem for this non‚Äìtrivial special case.
Example 3 (Additive Decomposition). Elementwise sparsity is a common structure often assumed
in high‚Äìdimensional estimation problems. However, in matrix completion, elementwise sparsity
conflicts with Assumption 1 (and more traditional incoherence assumptions). Indeed, it is easy to
see that with high probability most of the |‚Ñ¶|  d1 d2 uniformly sampled observations will be
zero, and an informed prediction is infeasible. However, elementwise sparse
P structures can often
be modelled within an additive decomposition framework, wherein Œò‚àó = k Œò(k) , such that each
component matrix Œò(k) is in turn structured (e.g. low rank+sparse used for robust PCA [6]). In
such structures, there is no scope for recovering sparse components outside the observed indices,
and it is assumed that: Œò(k) is sparse ‚áí supp(Œò(k) ) ‚äÜ ‚Ñ¶. In such cases, our results are applicable
under additional regularity assumptions that enforces non‚Äìspikiness on the superposed matrix. A
candidate norm regularizer for such structures is the weighted infimum convolution of individual
structure inducing norms [6, 39],
X
X
	
Rw (Œò) = inf
wk Rk (Œò(k) ) :
Œò(k) = Œò .
k

k

Example 4 (Other Applications). Other potential applications including cut matrices [30, 10], structures induced by compact convex sets, norms inducing structured sparsity assumptions on the spectrum of Œò‚àó , etc. can also be handled under the paradigm of this paper.
2.2

Structured Matrix Estimator

Let R be the norm surrogate for the structural constraints on Œò‚àó , and R‚àó denote its dual norm. We
propose and analyze two convex estimators for the task of structured matrix completion:
Constrained Norm Minimizer
b cn = argmin R(Œò)
Œò
s.t. kP‚Ñ¶ (Œò) ‚àí yk2 ‚â§ Œªcn .
‚àó
(9)
kŒòk‚àû ‚â§ ‚àöŒ±
d1 d2

Generalized Matrix Dantzig Selector
b ds =
Œò

argmin

R(Œò)

‚àö

d1 d2 ‚àó ‚àó
R P‚Ñ¶ (P‚Ñ¶ (Œò) ‚àí y) ‚â§ Œªds ,
|‚Ñ¶|

s.t.

Œ±‚àó

kŒòk‚àû ‚â§ ‚àö

d1 d2

4

(10)

where recall that P‚Ñ¶‚àó : R‚Ñ¶ ‚Üí Rd1√ód2 is the linear adjoint of P‚Ñ¶ , i.e. hP‚Ñ¶ (X), yi = hX, P‚Ñ¶‚àó (y)i.
Note: Theorem 1a‚Äì1b gives consistency results for (9) and (10), respectively, under certain conditions on the parameters Œªcn > 0, Œªds > 0, and Œ±‚àó > 1. In particular, these conditions assume
knowledge of the noise variance Œæ 2 and spikiness ratio Œ±sp (Œò‚àó ). In practice, typically Œæ and Œ±sp (Œò‚àó )
are unknown and the parameters are tuned by validating on held out data.

3

Main Results

We define the following ‚Äúrestricted‚Äù error cone and its subset:
TR = TR (Œò‚àó ) = cone{‚àÜ : R(Œò‚àó + ‚àÜ) ‚â§ R(Œò‚àó )}, and ER = TR ‚à© Sd1 d2 ‚àí1 .

(11)

b cn and Œò
b ds be the estimates from (9) and (10), respectively. If Œªcn and Œªds are chosen such that
Let Œò
‚àó
b cn = Œò
b cn ‚àí Œò‚àó
Œò belongs to the feasible sets in (9) and (10), respectively, then the error matrices ‚àÜ
‚àó
b ds = Œò
b ds ‚àí Œò are contained in TR .
and ‚àÜ
b cn = Œò‚àó +
Theorem 1a (Constrained Norm Minimizer).pUnder the problem setup in Section 2, let Œò
2 2
b
‚àÜcn be the estimate from (9) with Œªcn = 2Œæ |‚Ñ¶|. For large enough
c0 
, if |‚Ñ¶| > c0 wG (ER ) log d,

1
‚àö
then there exists an RSC parameter Œ∫c0 > 0 with Œ∫c0 ‚âà 1 ‚àí o log d , and constants c1 and c2
2
2
(ER ) log d),
(ER ))‚àí2 exp(‚àíc2 wG
such that, with probability greater than 1‚àíexp(‚àíc1 wG
s
(
)
2 (E ) log d
c20 wG
1 b 2
Œæ 2 Œ±‚àó2
R
k‚àÜcn kF ‚â§ 4 max
,
.
d1 d2
Œ∫c0 d1 d2
|‚Ñ¶|

b ds =
Theorem 1b (Matrix Dantzig Selector). Under the ‚àöproblem setup in Section 2, let Œò
d1 d2 ‚àó ‚àó
‚àó
b
Œò + ‚àÜds be the estimate from (10) with Œªds ‚â• 2Œæ |‚Ñ¶| R P‚Ñ¶ (Œ∑). For large enough c0 , if


1
2
(ER ) log d, then there exists an RSC parameter Œ∫c0 > 0 with Œ∫c0 ‚âà 1 ‚àí o ‚àölog
|‚Ñ¶| > c20 wG
,
d
2
(ER )),
and a constant c1 such that, with probability greater than 1‚àíexp(‚àíc1 wG
s
)
(
2 (E ) log d
c20 wG
1 b 2
Œª2ds Œ®2R (TR ) Œ±‚àó2
R
,
.
k‚àÜds kF ‚â§ 16 max
d1 d2
Œ∫2c0
d1 d2
|‚Ñ¶|

Recall Gaussian width wG and subspace compatibility constant Œ®R from (1) and (4), respectively.
Remarks:
2
(ER ) ‚â§ 3dr, Œ®R (TR ) ‚â§ 2r and
1. If R(Œò) = kŒòk‚àó q
and rank(Œò‚àó ) = r, then wG
‚àö

log d
‚â§ 2 d |‚Ñ¶|
w.h.p [10, 14, 26]. Using these bounds in Theorem 1b recovers
near‚Äìoptimal results for low rank matrix completion under spikiness assumption [26].
For both estimators, upper bound on sample complexity is dominated by the square of Gaussian
width which is often considered the effective dimension of a subset in high dimensional space
and plays a key role in high dimensional estimation under Gaussian measurement ensembles.
The results show that, independent of R(.), the upper bound on sample complexity for consistent
matrix completion with highly localized measurements is within a log d factor of the known
2
sample complexity of ‚àº wG
(ER ) for estimation from Gaussian measurements [3, 10, 37, 5].
First term in estimation error bounds in Theorem 1a‚Äì1b scales with Œæ 2 which is the per observation noise variance (upto constant). The second term is an upper bound on error that arises due
to unidentifiability of Œò‚àó within a certain radius under the spikiness constraints [26]; in contrast
[7] show exact recovery when Œæ = 0 using more stringent matrix incoherence conditions.
b cn from Theorem 1a is comparable to the result by CandeÃÅs et al. [7] for low rank
Bound on ‚àÜ
matrix completion under non‚Äìlow‚Äìnoise regime, where the first term dominates, and those of [10,
2
35] for high dimensional estimation under Gaussian measurements. With a bound on wG
(ER ), it
is easy to specialize this result for new structural constraints. However, this bound is potentially
loose and asymptotically converges to a constant error proportional to the noise variance Œæ 2 .
The estimation error bound in Theorem 1b is typically sharper than that in Theorem 1a. However,
for specific structures, using application of Theorem 1b requires additional bounds on R‚àó P‚Ñ¶‚àó (Œ∑)
2
and Œ®R (TR ) besides wG
(ER ).
d1 d2
‚àó
|‚Ñ¶| kP‚Ñ¶ (Œ∑)k2

2.

3.

4.

5.

5

3.1

Partial Complexity Measures

Recall that for wG (S) = E supX‚ààS hX, Gi and R|‚Ñ¶| 3 g ‚àº N (0, I|‚Ñ¶| ) is a standard normal vector.
Definition 6 (Partial Complexity Measures). Given a randomly sampled ‚Ñ¶ = {Ek ‚àà Rd1√ód2 }, and
a centered random vector Œ∑ ‚àà R|‚Ñ¶| , the partial Œ∑‚Äìcomplexity measure of S is given by:
w‚Ñ¶,Œ∑ (S) = E‚Ñ¶,Œ∑ sup hX, P‚Ñ¶‚àó (Œ∑)i.
(12)
X‚ààS‚àíS

Special cases of Œ∑ being a vector of standard Gaussian g, or standard Rademacher  (i.e. k ‚àà
{‚àí1, 1} w.p. 1/2) variables, are of particular interest.
Note: In the case of symmetric Œ∑, like g and , w‚Ñ¶,Œ∑ (S) = 2E‚Ñ¶,Œ∑ supX‚ààS hX, P‚Ñ¶‚àó (Œ∑)i, and the later
expression will be used interchangeably ignoring the constant term.

Theorem 2 (Partial Gaussian Complexity). Let S ‚äÜ Bd1 d2 with non‚Äìempty interior, and let ‚Ñ¶ be
sampled according to (6). ‚àÉ universal constants k1 , k2 , K1 and K2 such that:
s
r
|‚Ñ¶|
w‚Ñ¶,g (S) ‚â§ k1
wG (S) + k2 E‚Ñ¶ sup kP‚Ñ¶ (X ‚àí Y )k22
d1 d2
X,Y ‚ààS
(13)
s
|‚Ñ¶|
w‚Ñ¶,g (S) ‚â§ K1
wG (S) + K2 sup kX ‚àí Y k‚àû .
d1 d2
X,Y ‚ààS
Also, for centered i.i.d. sub‚ÄìGaussian vector Œ∑ ‚àà R|‚Ñ¶| , ‚àÉ constant K3 s.t. w‚Ñ¶,Œ∑ (S) ‚â§ K3 w‚Ñ¶,g (S).
Note: For ‚Ñ¶ ( [d1 ] √ó [d2 ], the second term in (13) is a consequence of the localized measurements.
3.2

Spectral k‚ÄìSupport Norm

We introduced spectral k‚Äìsupport norm in Section 2.1. The estimators from (9) and (10) for spectral
k‚Äìsupport norm can be efficiently solved via proximal methods using the proximal operators derived
in [25]. We are interested in the statistical guarantees for matrix completion using spectral k‚Äìsupport
norm regularization. We extend the analysis for upper bounding the Gaussian width of the descent
cone for the vector k‚Äìsupport norm by [29] to the case of spectral k‚Äìsupport norm. WLOG let
¬Ø Let œÉ ‚àó ‚àà Rd¬Ø be the vector of singular values of Œò‚àó sorted in non‚Äìascending order.
d1 = d2 = d.
Pp
1
‚àó
‚àó
‚àó
Let r ‚àà {0, 1, 2, . . . , k ‚àí 1} be the unique integer satisfying: œÉk‚àír‚àí1
> r+1
i=k‚àír œÉi ‚â• œÉk‚àír .
¬Ø
Denote I2 = {1, 2, . . . , k ‚àí r ‚àí 1} and I1 = {k ‚àí r, k ‚àí r + 1, . . . , s}. Finally, for I ‚äÜ [d],
‚àó
c
‚àó
‚àó
(œÉI )i = 0 ‚àÄi ‚àà I , and (œÉI )i = œÉi ‚àÄi ‚àà I.
Lemma 3. If rank of Œò‚àó is s and ER is the error set for R(Œò) = kŒòkk‚Äìsp , then
 (r + 1)2 kœÉ ‚àó k2

I2 2
2
wG
(ER ) ‚â§ s(2d¬Ø ‚àí s) +
+
|I
|
(2d¬Ø ‚àí s).
1
kœÉI‚àó1 k21
Proof of the above lemma is provided in the appendix. Lemma 3 can be combined with Theorem 1a
to obtain recovery guarantees for matrix completion under spectral k‚Äìsupport norm.

4

Discussions and Related Work

Sample Complexity: For consistent recovery in high dimensional convex estimation, it is desirable
that the descent cone at the target parameter Œò‚àó is ‚Äúsmall‚Äù relative to the feasible set (enforced by the
observations) of the estimator. Thus, it is not surprising that the sample complexity and estimation
error bounds of an estimator depends on some measure of complexity/size of the error cone at
Œò‚àó . Results in this paper are largely characterized in terms of a widely used complexity measure
of Gaussian width wG (.), and can be compared with the literature on estimation from Gaussian
measurements.
Error Bounds: Theorem 1a provides estimation error bounds that depends only on the Gaussian
width of the descent cone. In non‚Äìlow‚Äìnoise regime, this result is comparable to analogous results
of constrained norm minimization [6, 10, 35]. However, this bound is potentially loose owing to
mismatched data‚Äìfit term using squared loss, and asymptotically converges to a constant error proportional to the noise variance Œæ 2 .
6

A tighter analysis on the estimation error can be obtained for the matrix Dantzig selector (10) from
Theorem 1b. However, application of Theorem 1b requires computing high probability upper bound
on R‚àó P‚Ñ¶‚àó (Œ∑). The literature on norms of random matrices [13, 24, 36, 34] can be exploited in computing such bounds. Beside, in special cases: if R(.) ‚â• Kk.k‚àó , then KR‚àó (.) ‚â§ k.kop can be used
to obtain asymptotically consistent results.
Finally, under near zero‚Äìnoise, the second term in the results of Theorem 1 dominates, and bounds
are weaker than that of [6, 19] owing to the relaxation of stronger incoherence assumption.
Related Work and Future Directions: The closest related work is the result on consistency of
matrix completion under decomposable norm regularization by [16]. Results in this paper are a strict
generalization to general norm regularized (not necessarily decomposable) matrix completion. We
provide non‚Äìtrivial examples of application where structures enforced by such non‚Äìdecomposable
norms are of interest. Further, in contrast to our results that are based on Gaussian width, the RSC
parameter in [16] depends on a modified complexity measure Œ∫R (d, |‚Ñ¶|) (see definition in [16]). An
advantage of results based on Gaussian width is that, application of Theorem 1 for special cases can
greatly benefit from the numerous tools in the literature for the computation of wG (.).
Another closely related line of work is the non‚Äìasymptotic analysis of high dimensional estimation
under random Gaussian or sub‚ÄìGaussian measurements [10, 1, 35, 3, 37, 5]. However, the analysis
from this literature rely on variants of RIP of the measurement ensemble [9], which is not satisfied by
the the extremely localized measurements encountered in matrix completion[8]. In an intermediate
result, we establish a form of RSC for matrix completion under general norm regularization: a result
that was previously known only for nuclear norm and decomposable norm regularization.
In future work, it is of interest to derive matching lower bounds on estimation error for matrix
completion under general low dimensional structures, along the lines of [22, 5] and explore special
case applications of the results in the paper. We also plan to derive explicit characterization of Œªds
in terms of Gaussian width of unit balls by exploiting generic chaining results for general Banach
spaces [33].

5

Proof Sketch

Proofs of the lemmas are provided in the Appendix.
5.1 Proof of Theorem 1
Define the following set of Œ≤‚Äìnon‚Äìspiky
matrices in‚àöRd1√ód2 for constant
(
) c0 from Theorem 1:
d1 d2 kXk‚àû
A(Œ≤) = X : Œ±sp (X) =
<Œ≤ .
(14)
kXkF
s
|‚Ñ¶|
2
Define,
Œ≤c0 =
(15)
2 (E ) log d
c20 wG
R
Case 1: Spiky Error Matrix When the error matrix from (9) or (10) has large spikiness ratio,
‚àö
b ‚àû ‚â§ kŒòk
b ‚àû +kŒò‚àó k‚àû‚â§2Œ±‚àó / d1 d2 in (3).
following bound on error is immediate using k‚àÜk
b cn ) ‚àà
Proposition 4 (Spiky Error
For the constant c0 in Theorem 1a, if Œ±sp (‚àÜ
/ A(Œ≤c0 ), then
q Matrix).
2 w 2 (E ) log d
‚àó2
c
R
4Œ±
2
‚àó2
0 G
b cn k ‚â§ 2 = 4Œ±
b ds .
k‚àÜ
. An analogous result also holds for ‚àÜ

F
Œ≤
|‚Ñ¶|
c0

b ds , ‚àÜ
b cn ‚àà A(Œ≤). Recall from (5), that y ‚àí P‚Ñ¶ (Œò‚àó ) = ŒæŒ∑,
Case 2: Non‚ÄìSpiky Error Matrix Let ‚àÜ
|‚Ñ¶|
where Œ∑ ‚àà R consists of independent sub‚ÄìGaussian random variables with E[Œ∑k ] = 0, Var(Œ∑k ) =
1, and kŒ∑k kŒ®2 ‚â§ b for a constant b.
5.1.1

Restricted Strong Convexity (RSC)

Recall TR and ER from (11). The most significant step in the proof of Theorem 1 involves showing
that over a useful subset of TR , a form of RSC (2) is satisfied by a squared loss penalty.
2
Theorem 5 (Restricted Strong Convexity). Let |‚Ñ¶| > c20 wG
(ER ) log
 d, for large enough constant
c0 . There exists a RSC parameter Œ∫c0 > 0 with Œ∫c0 ‚âà 1 ‚àí o

the following holds w.p. greater that 1 ‚àí

2
exp(‚àíc1 wG
(ER )),

7

‚àö1
log d

, and a constant c1 such that,

‚àÄX ‚àà TR ‚à© A(Œ≤c0 ),

d1 d2
kP‚Ñ¶ (X)k22 ‚â• Œ∫c0 kXk2F .
|‚Ñ¶|

Proof in Appendix A combines empirical process tools along with Theorem 2.
5.1.2



Constrained Norm Minimizer

Lemma 6. Under the conditions of Theorem 1, letp
b be a constant such that ‚àÄk, kŒ∑k kŒ®2 ‚â§ b. There
exists a universal constant c2 such that, if Œªcn ‚â• 2Œæ |‚Ñ¶|, then w.p. greater than 1 ‚àí 2 exp (‚àíc2 |‚Ñ¶|),
b ds ‚àà TR , and (b) kP‚Ñ¶ (‚àÜ
b cn )k2 ‚â§ 2Œªcn .
(a) ‚àÜ

p
b cn ‚àà A(Œ≤c ), then using Theorem 5 and Lemma 6, w.h.p.
Using Œªcn = 2Œæ |‚Ñ¶| in (9), if ‚àÜ
0
b cn k2
b cn )k2 4Œæ 2
k‚àÜ
1 kP‚Ñ¶ (‚àÜ
2
F
‚â§
‚â§
.
d1 d2
Œ∫c0
|‚Ñ¶|
Œ∫c0
5.1.3

(16)

Matrix Dantzig Selector
‚àö

‚àö

Proposition 7. Œªds ‚â• Œæ

d1 d2 ‚àó ‚àó
|‚Ñ¶| R P‚Ñ¶ (Œ∑)

b ds ‚àà TR ; (b)
‚áí w.h.p. (a) ‚àÜ

d1 d2 ‚àó ‚àó
b
|‚Ñ¶| R P‚Ñ¶ (P‚Ñ¶ (‚àÜds )) ‚â§ 2Œªds .

b ds and triangle inequality. Also,
Above result follows from optimality of Œò
‚àö
‚àö
d1 d2
b ds )k2 ‚â§ d1 d2 R‚àó P ‚àó (P‚Ñ¶ (‚àÜ
b ds ))R(‚àÜ
b ds ) ‚â§ 2Œªds Œ®R (TR )k‚àÜ
b ds kF ,
kP‚Ñ¶ (‚àÜ
2
‚Ñ¶
|‚Ñ¶|
|‚Ñ¶|
where recall norm compatibility constant Œ®R (TR ) from (4). Finally, using Theorem 5, w.h.p.
b k
b ds k2
b ds )k2 4Œªds Œ®R (TR ) k‚àÜ
k‚àÜ
1 kP‚Ñ¶ (‚àÜ
2
F
‚àö ds F .
‚â§
‚â§
d1 d2
|‚Ñ¶|
Œ∫c0
Œ∫c0
d1 d2
5.2

(17)

Proof of Theorem 2

|‚Ñ¶|
Let the entries of ‚Ñ¶ = {Ek = eik e>
jk : k = 1, 2, . . . , |‚Ñ¶|} be sampled as in (6). Recall that g ‚àà R
is a standard normal vector. For a compact S ‚äÜ Rd1√ód2 , it suffices to prove Theorem 2 for a dense
countable subset of S. Overloading S to such a countable subset, define following random process:
P
(X‚Ñ¶,g (X))X‚ààS , where X‚Ñ¶,g (X) = hX, P‚Ñ¶‚àó (g)i = k hX, Ek igk .
(18)

We start with a key lemma in the proof of Theorem 2. Proof of this lemma, provided in Appendix B,
uses tools from the broad topic of generic chaining developed in recent works [31, 33].
Lemma 8. For a compact subset Ss‚äÜ Rd1√ód2 with non‚Äìempty interior, ‚àÉ constants k1 , k2 such that:
r
|‚Ñ¶|
wG (S) + k2 E sup kP‚Ñ¶ (X ‚àí Y )k22 .

w‚Ñ¶,g (S) = E sup X‚Ñ¶,g (X) ‚â§ k1
d1 d2
X,Y ‚ààS
X‚ààS
Lemma 9. There exists constants k3 , k4 , such that for compact S ‚äÜ Bd1 d2 with non‚Äìempty interior
E sup kP‚Ñ¶ (X ‚àí Y )k22 ‚â§ k3
X,Y ‚ààS

|‚Ñ¶| 2
w (S) + k4 ( sup kX ‚àí Y k‚àû )w‚Ñ¶,g (S)
d1 d2 G
X,Y ‚ààS

Theorem
2 follows by combining Lemma 8 and Lemma 9, and simple algebraic manipulations using
‚àö
ab ‚â§ a/2 + b/2 and triangle inequality (See Appendix B.4).
The statement in Theorem 2 about partial sub‚ÄìGaussian complexity follows from a standard result
in empirical process given in Lemma 11 in the appendix.

Acknowledgments We thank the anonymous reviewers for helpful comments and suggestions. S.
Gunasekar and J. Ghosh acknowledge funding from NSF grants IIS-1421729, IIS-1417697, and
IIS1116656. A. Banerjee acknowledges NSF grants IIS-1447566, IIS-1422557, CCF-1451986,
CNS-1314560, IIS-0953274, IIS-1029711, and NASA grant NNX12AQ39A.

8

References
[1] D. Amelunxen, M. Lotz, M. B. McCoy, and J. A. Tropp. Living on the edge: A geometric theory of phase
transitions in convex optimization. Inform. Inference, 2014.
[2] A. Argyriou, R. Foygel, and N. Srebro. Sparse prediction with the k-support norm. In NIPS, 2012.
[3] A. Banerjee, S. Chen, F. Fazayeli, and V. Sivakumar. Estimation with norm regularization. In NIPS, 2014.
[4] A. Banerjee, S. Merugu, I. S. Dhillon, and J. Ghosh. Clustering with bregman divergences. JMLR, 2005.
[5] T. Cai, T. Liang, and A. Rakhlin. Geometrizing local rates of convergence for linear inverse problems.
arXiv preprint, 2014.
[6] E. J. CandeÃÅs, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? ACM, 2011.
[7] E. J. CandeÃÅs and Y. Plan. Matrix completion with noise. Proceedings of the IEEE, 2010.
[8] E. J. CandeÃÅs and B. Recht. Exact matrix completion via convex optimization. FoCM, 2009.
[9] Emmanuel J Candes and Terence Tao. Decoding by linear programming. Information Theory, IEEE
Transactions on, 2005.
[10] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse
problems. Foundations of Computational Mathematics, 2012.
[11] M. A. Davenport, Y. Plan, E. Berg, and M. Wootters. 1-bit matrix completion. Inform. Inference, 2014.
[12] R. M. Dudley. The sizes of compact subsets of hilbert space and continuity of gaussian processes. Journal
of Functional Analysis, 1967.
[13] A. Edelman. Eigenvalues and condition numbers of random matrices. Journal on Matrix Analysis and
Applications, 1988.
[14] M. Fazel, H Hindi, and S. P. Boyd. A rank minimization heuristic with application to minimum order
system approximation. In American Control Conference, 2001.
[15] J. Forster and M. Warmuth. Relative expected instantaneous loss bounds. Journal of Computer and
System Sciences, 2002.
[16] S. Gunasekar, P. Ravikumar, and J. Ghosh. Exponential family matrix completion under structural constraints. In ICML, 2014.
[17] L. Jacob, J. P. Vert, and F. R. Bach. Clustered multi-task learning: A convex formulation. In NIPS, 2009.
[18] R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. IEEE Trans. IT, 2010.
[19] R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. JMLR, 2010.
[20] O. Klopp. Noisy low-rank matrix completion with general sampling distribution. Bernoulli, 2014.
[21] O. Klopp. Matrix completion by singular value thresholding: sharp bounds. arXiv preprint arXiv, 2015.
[22] Vladimir Koltchinskii, Karim Lounici, Alexandre B Tsybakov, et al. Nuclear-norm penalization and
optimal rates for noisy low-rank matrix completion. The Annals of Statistics, 2011.
[23] M. Ledoux and M. Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer, 1991.
[24] A. E. Litvak, A. Pajor, M. Rudelson, and N. Tomczak-Jaegermann. Smallest singular value of random
matrices and geometry of random polytopes. Advances in Mathematics, 2005.
[25] A. M. McDonald, M. Pontil, and D. Stamos. New perspectives on k-support and cluster norms. arXiv
preprint, 2014.
[26] S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal
bounds with noise. JMLR, 2012.
[27] S. Negahban, B. Yu, M. J. Wainwright, and P. Ravikumar. A unified framework for high-dimensional
analysis of m-estimators with decomposable regularizers. In NIPS, 2009.
[28] B. Recht. A simpler approach to matrix completion. JMLR, 2011.
[29] E. Richard, G. Obozinski, and J.-P. Vert. Tight convex relaxations for sparse matrix factorization. In
ArXiv e-prints, 2014.
[30] N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. In Learning Theory. Springer, 2005.
[31] M. Talagrand. Majorizing measures: the generic chaining. The Annals of Probability, 1996.
[32] M. Talagrand. Majorizing measures without measures. Annals of probability, 2001.
[33] M. Talagrand. Upper and Lower Bounds for Stochastic Processes. Springer, 2014.
[34] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational
Mathematics, 2012.
[35] J. A. Tropp. Convex recovery of a structured signal from independent random linear measurements. arXiv
preprint, 2014.
[36] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. Compressed sensing,
pages 210‚Äì268, 2012.
[37] R. Vershynin. Estimation in high dimensions: a geometric perspective. ArXiv e-prints, 2014.
[38] A. G. Watson. Characterization of the subdifferential of some matrix norms. Linear Algebra and its
Applications, 1992.
[39] E. Yang and P. Ravikumar. Dirty statistical models. In NIPS, 2013.

9

