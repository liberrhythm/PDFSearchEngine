Multi-Layer Feature Reduction for Tree Structured
Group Lasso via Hierarchical Projection

Jie Wang1 , Jieping Ye1,2
Computational Medicine and Bioinformatics
2
Department of Electrical Engineering and Computer Science
University of Michigan, Ann Arbor, MI 48109
{jwangumi, jpye}@umich.edu
1

Abstract
Tree structured group Lasso (TGL) is a powerful technique in uncovering the tree
structured sparsity over the features, where each node encodes a group of features.
It has been applied successfully in many real-world applications. However, with
extremely large feature dimensions, solving TGL remains a significant challenge
due to its highly complicated regularizer. In this paper, we propose a novel MultiLayer Feature reduction method (MLFre) to quickly identify the inactive nodes
(the groups of features with zero coefficients in the solution) hierarchically in a
top-down fashion, which are guaranteed to be irrelevant to the response. Thus, we
can remove the detected nodes from the optimization without sacrificing accuracy. The major challenge in developing such testing rules is due to the overlaps
between the parents and their children nodes. By a novel hierarchical projection algorithm, MLFre is able to test the nodes independently from any of their
ancestor nodes. Moreover, we can integrate MLFre‚Äîthat has a low computational cost‚Äîwith any existing solvers. Experiments on both synthetic and real data
sets demonstrate that the speedup gained by MLFre can be orders of magnitude.

1

Introduction

Tree structured group Lasso (TGL) [13, 30] is a powerful regression technique in uncovering the
hierarchical sparse patterns among the features. The key of TGL, i.e., the tree guided regularization,
is based on a pre-defined tree structure and the group Lasso penalty [29], where each node represents
a group of features. In recent years, TGL has achieved great success in many real-world applications
such as brain image analysis [10, 18], gene data analysis [14], natural language processing [27, 28],
and face recognition [12]. Many algorithms have been proposed to improve the efficiency of TGL
[1, 6, 11, 7, 16]. However, the application of TGL to large-scale problems remains a challenge due
to its highly complicated regularizer.
As an emerging and promising technique in scaling large-scale problems, screening has received
much attention in the past few years. Screening aims to identify the zero coefficients in the sparse
solutions by simple testing rules such that the corresponding features can be removed from the
optimization. Thus, the size of the data matrix can be significantly reduced, leading to substantial
savings in computational cost and memory usage. Typical examples include TLFre [25], FLAMS
[22], EDPP [24], Sasvi [17], DOME [26], SAFE [8], and strong rules [21]. We note that strong rules
are inexact in the sense that features with nonzero coefficients may be mistakenly discarded, while
the others are exact. Another important direction of screening is to detect the non-support vectors for
support vector machine (SVM) and least absolute deviation (LAD) [23, 19]. Empirical studies have
shown that the speedup gained by screening methods can be several orders of magnitude. Moreover,
the exact screening methods improve the efficiency without sacrificing optimality.
However, to the best of our knowledge, existing screening methods are only applicable to sparse
models with simple structures such as Lasso, group Lasso, and sparse group Lasso. In this paper, we
1

propose a novel Multi-Layer Feature reduction method, called MLFre, for TGL. MLFre is exact and
it tests the nodes hierarchically from the top level to the bottom level to quickly identify the inactive
nodes (the groups of features with zero coefficients in the solution vector), which are guaranteed to
be absent from the sparse representation. To the best of our knowledge, MLFre is the first screening
method that is applicable to TGL with the highly complicated tree guided regularization.
The major technical challenges in developing MLFre for TGL lie in two folds. The first is that
most existing exact screening methods are based on evaluating the norm of the subgradients of the
sparsity-inducing regularizers with respect to the variables or groups of variables of interests. However, for TGL, we only have access to a mixture of the subgradients due to the overlaps between
parents and their children nodes. Therefore, our first major technical contribution is a novel hierarchical projection algorithm that is able to exactly and efficiently recover the subgradients with
respect to every node from the mixture (Sections 3 and 4). The second technical challenge is that
most existing exact screening methods need to estimate an upper bound involving the dual optimum.
This turns out to be a complicated nonconvex optimization problem for TGL. Thus, our second major
technical contribution is to show that this highly nontrivial nonconvex optimization problem admits
closed form solutions (Section 5). Experiments on both synthetic and real data sets demonstrate that
the speedup gained by MLFre can be orders of magnitude (Section 6). Please see supplements for
detailed proofs of the results in the main text.
Notation: Let k¬∑k be the `2 norm, [p] = {1, . . . , p} for a positive integer p, G ‚äÜ [p], and GÃÑ = [p]\G.
For u ‚àà Rp , let ui be its ith component. For G ‚äÜ [p], we denote uG = [u]G = {v : vi = ui if i ‚àà
G, vi = 0 otherwise} and HG = {u ‚àà Rp : uGÃÑ = 0}. If G1 , G2 ‚äÜ [n] and G1 ‚äÇ G2 , we
emphasize that G2 \ G1 6= ‚àÖ. For a set C, let int C, ri C, bd C, and rbd C be its interior, relative
interior, boundary, and relative boundary, respectively [5]. If C is closed and convex, the projection
operator is PC (z) := argminu‚ààC kz ‚àí uk, and its indicator function is IC (¬∑), which is 0 on C and ‚àû
elsewhere. Let Œì0 (Rp ) be the class of proper closed convex functions on Rp . For f ‚àà Œì0 (Rp ), let
‚àÇf be its subdifferential and dom f := {z : f (z) < ‚àû}. We denote by Œ≥+ = max(Œ≥, 0).

2

Basics

We briefly review some basics of TGL. First, we introduce the so-called index tree.
Definition 1. [16] For an index tree T of depth d, we denote the node(s) of depth i by Ti =
{Gi1 , . . . , Gini }, where n0 = 1, G01 = [p], Gij ‚äÇ [p], and ni ‚â• 1, ‚àÄ i ‚àà [d]. We assume that
(i): Gij1 ‚à© Gij2 = ‚àÖ, ‚àÄ i ‚àà [d] and j1 6= j2 (different nodes of the same depth do not overlap).
i+1
(ii): If Gij is a parent node of Gi+1
‚äÇ Gij .
` , then G`
When the tree structure is available (see supplement for an example), the TGL problem is
Xd Xni
min 21 ky ‚àí XŒ≤k2 + Œª
wji kŒ≤Gij k,
(TGL)
i=0

Œ≤

j=1

N √óp

N

where y ‚àà R is the response vector, X ‚àà R
is the data matrix, Œ≤Gij and wji are the coefficients
vector and positive weight corresponding to node Gij , respectively, and Œª > 0 is the regularization
parameter. We derive the Lagrangian dual problem of TGL as follows.
Pd Pni
Theorem 2. For the TGL problem, let œÜ(Œ≤) = i=0 j=1
wji kŒ≤Gij k. The following hold:
(i): Let œÜij (Œ≤) = kŒ≤Gij k and Bji = {Œ∂ ‚àà HGij : kŒ∂k ‚â§ wji }. We can write ‚àÇœÜ(0) as
Xd Xni
Xd Xni
‚àÇœÜ(0) =
wji ‚àÇœÜij (0) =
Bji .
(1)
i=0

j=1

i=0

j=1

T

(ii): Let F = {Œ∏ : X Œ∏ ‚àà ‚àÇœÜ(0)}. The Lagrangian dual of TGL is
	

sup 21 kyk2 ‚àí 12 k yŒª ‚àí Œ∏k2 : Œ∏ ‚àà F .

(2)

Œ∏

(iii): Let Œ≤ ‚àó (Œª) and Œ∏‚àó (Œª) be the optimal solution of problems (TGL) and (2), respectively. Then,
y = XŒ≤ ‚àó (Œª) + ŒªŒ∏‚àó (Œª),
(3)
Xd Xni
XT Œ∏‚àó (Œª) ‚àà
wji ‚àÇœÜij (Œ≤ ‚àó (Œª)).
(4)
i=0

j=1

The dual problem of TGL in (2) is equivalent to a projection problem, i.e., Œ∏‚àó (Œª) = PF (y/Œª). This
geometric property plays a fundamentally important role in developing MLFre (see Section 5).
2

3

Testing Dual Feasibility via Hierarchical Projection

Although the dual problem in (2) has nice geometric properties, it is challenging to determine the
feasibility of a given Œ∏ due to the complex dual feasible set F. An alternative approach is to test
if XT Œ∏ = P‚àÇœÜ(0) (XT Œ∏). Although ‚àÇœÜ(0) is very complicated, we show that P‚àÇœÜ(0) (¬∑) admits a
closed form solution by hierarchically splitting P‚àÇœÜ(0) (¬∑) into a sum of projection operators with
respect to a collection of simpler sets. We first introduce some notations. For an index tree T , let
nX
o
Aij =
Bkt : Gtk ‚äÜ Gij , ‚àÄ i ‚àà 0 ‚à™ [d], j ‚àà [ni ],
(5)
t,k
nX
o
Cji =
(6)
Bkt : Gtk ‚äÇ Gij , ‚àÄ i ‚àà 0 ‚à™ [d], j ‚àà [ni ].
t,k

For a node Gij , the set Aij is the sum of Bkt corresponding to all its descendant nodes and itself, and
the set Cji the sum excluding itself. Therefore, by the definitions of Aij , Bji , and Cji , we have
‚àÇœÜ(0) = A01 ,
Aij = Bji + Cji , ‚àÄ non-leaf node Gij ,
Aij = Bji , ‚àÄ leaf node Gij ,
(7)
which implies that P‚àÇœÜ(0) (¬∑) = PA01 (¬∑) = PB10 +C10 (¬∑). This motivates the first pillar of this paper,
i.e., Lemma 3, which splits PB10 +C10 (¬∑) into the sum of two projections onto B10 and C10 , respectively.
Lemma 3. Let G ‚äÜ [p], B = {u ‚àà HG : kuk ‚â§ Œ≥} with Œ≥ > 0, C ‚äÜ HG a nonempty closed convex
set, and z an arbitrary point in HG . Then, the following hold:
(i): [2] PB (z) = min{1, Œ≥/kzk}z if z 6= 0. Otherwise, PB (z) = 0.
(ii): IB+C (z) = IB (z ‚àí PC (z)), i.e., PC (z) ‚àà argminu‚ààC IB (z ‚àí u).
(iii): PB+C (z) = PC (z) + PB (z ‚àí PC (z)).
By part (iii) of Lemma 3, we can split PA01 (XT Œ∏) in the following form:
PA01 (XT Œ∏) = PC10 (XT Œ∏) + PB10 (XT Œ∏ ‚àí PC10 (XT Œ∏)).

(8)
T

As PB10 (¬∑) admits a closed form solution by part (i) of Lemma 3, we can compute PA01 (X Œ∏) if we
have PC10 (XT Œ∏) computed. By Eq. (5) and Eq. (6), for a non-leaf node Gij , we note that
X
i+1
i
Cji =
Ai+1
‚äÇ Gij }.
(9)
k , where Ic (Gj ) = {k : Gk
i
k‚ààIc (Gj )

Inspired by (9), we have the following result.
Lemma 4. Let {G` ‚äÇ [p]}` be a setPof nonoverlapping index
P sets, {C` ‚äÜ HG` }` be a set of
nonempty closed convex sets, and C = ` C` . Then, PC (z) = ` PC` (zG` ) for z ‚àà Rp .
Remark 1. For Lemma 4, if all C` are balls centered at 0, then PC (z) admits a closed form solution.
By Lemma 4 and Eq. (9), we can further splits PC10 (XT Œ∏) in Eq. (8) in the following form.
X
PC10 (XT Œ∏) =
PA1k ([XT Œ∏]G1k ), where Ic (G01 ) = {k : G1k ‚äÇ G01 }.
0
k‚ààIc (G1 )

(10)

Consider the right hand side of Eq. (10). If G1k is a leaf node, Eq. (7) implies that A1k = Bk1 and
thus PA1k (¬∑) admits a closed form solution by part (i) of Lemma 3. Otherwise, we continue to split
PA1k (¬∑) by Lemmas (3) and (4). This procedure will always terminate as we reach the leaf nodes
[see the last equality in Eq. (7)]. Therefore, by a repeated application of Lemmas (3) and (4), the
following algorithm computes the closed form solution of PA01 (¬∑).
Algorithm 1 Hierarchical Projection: PA01 (¬∑).
Input: z ‚àà Rp , the index tree T as in Definition 1, and positive weights wji for all nodes Gij in T .
Output: u0 = PA01 (z), vi for ‚àÄ i ‚àà 0 ‚à™ [d].
1: Set ui ‚Üê 0 ‚àà Rp , ‚àÄ i ‚àà 0 ‚à™ [d + 1], vi ‚Üê 0 ‚àà Rp , ‚àÄ i ‚àà 0 ‚à™ [d].
2: for i = d to 0 do
/*hierarchical projection*/
3:
for j = 1 to ni do
i+1
i
vG
(11)
4:
i = PB i (zGi ‚àí u i ),
G
j
j
j

j

i
uiGi ‚Üê ui+1
+ vG
i.
Gi
j

j

5:
end for
6: end for

3

j

(12)

The time complexity of Algorithm 1 is similar to that of solving its proximal operator [16],
Pd Pni
|Gij |), where |Gij | is the number of features contained in the node Gij . As
i.e., O( i=0 j=1
Pni
i
j=1 |Gj | ‚â§ p by Definition 1, the time complexity of Algorithm 1 is O(pd), and thus O(p log p)
for a balanced tree, where d = O(log p). The next result shows that u0 returned by Algorithm 1 is
the projection of z onto A01 . Indeed, we have more general results as follows.
Theorem 5. ForAlgorithm
1, the following hold:

i
(i): uGi = PAij zGij , ‚àÄ i ‚àà 0 ‚à™ [d], j ‚àà [ni ].
j


= PCji zGij , for any non-leaf node Gij .
(ii): ui+1
Gi
j

4

MLFre Inspired by the KKT Conditions and Hierarchical Projection

In this section, we motivate MLFre via the KKT condition in Eq. (4) and the hierarchical projection
in Algorithm 1. Note that for any node Gij , we have
(
{Œ∂ ‚àà HGij : kŒ∂k ‚â§ wji },
if [Œ≤ ‚àó (Œª)]Gij = 0,
i
i
‚àó
(13)
wj ‚àÇœÜj (Œ≤ (Œª)) =
wji [Œ≤ ‚àó (Œª)]Gij /k[Œ≤ ‚àó (Œª)]Gij k, otherwise.
Moreover, the KKT condition in Eq. (4) implies that
‚àÉ {Œæji ‚àà wji ‚àÇœÜij (Œ≤ ‚àó (Œª)) : ‚àÄ i ‚àà 0 ‚à™ [d], j ‚àà [ni ]} such that XT Œ∏‚àó (Œª) =

Xd
i=0

Xn i
j=1

Œæji .

(14)

Thus, if kŒæji k < wji , we can see that [Œ≤ ‚àó (Œª)]Gij = 0. However, we do not have direct access to Œæji
even if Œ∏‚àó (Œª) is known, because XT Œ∏‚àó (Œª) is a mixture (sum) of all Œæji as shown in Eq. (14). Indeed,
Algorithm 1 turns out to be much more useful than testing the feasibility of a given Œ∏: it is able to
split all Œæji ‚àà wji ‚àÇœÜij (Œ≤ ‚àó (Œª)) from XT Œ∏‚àó (Œª). This will serve as a cornerstone in developing MLFre.
Theorem 6 rigorously shows this property of Algorithm 1.
Theorem 6. Let vi , i ‚àà 0 ‚à™ [d] be the output of Algorithm 1 with input XT Œ∏‚àó (Œª), and {Œæji : i ‚àà
0 ‚à™ [d], j ‚àà [ni ]} be the set of vectors that satisfy Eq. (14). Then, the following hold.
(i) If [Œ≤ ‚àó (Œª)]Gij = 0, and [Œ≤ ‚àó (Œª)]Glr 6= 0 for all Glr ‚äÉ Gij , then

 P
PAij [XT Œ∏‚àó (Œª)]Gij = {(k,t):Gt ‚äÜGi } Œækt .
k

j

(ii) If Gij is a non-leaf node, and [Œ≤ ‚àó (Œª)]Gij 6= 0, then

 P
PCji [XT Œ∏‚àó (Œª)]Gij = {(k,t):Gt ‚äÇGi } Œækt .
k

j

i
i
i
‚àó
(iii) vG
i ‚àà wj ‚àÇœÜj (Œ≤ (Œª)), ‚àÄ i ‚àà 0 ‚à™ [d], j ‚àà [ni ].
j

Combining Eq. (13) and part (iii) of Theorem 6, we can see that
i
i
‚àó
kvG
i k < wj ‚áí [Œ≤ (Œª)]Gi = 0.
j
j

By plugging Eq. (11) and part (ii) of Theorem 5 into (15), we have [Œ≤ ‚àó (Œª)]Gij = 0 if






(a): PBji [XT Œ∏‚àó (Œª)]Gij ‚àí PCji [XT Œ∏‚àó (Œª)]Gij  < wji , if Gij is a non-leaf node,





(b): PBji [XT Œ∏‚àó (Œª)]Gij  < wji ,
if Gij is a leaf node.

(15)

(R1)
(R2)

Moreover, the definition of PBji implies that we can simplify (R1) and (R2) to the following form:



 T ‚àó

[X Œ∏ (Œª)]Gij ‚àí PCji [XT Œ∏‚àó (Œª)]Gij  < wji ‚áí [Œ≤ ‚àó (Œª)]Gij = 0, if Gij is a non-leaf node, (R1‚Äô)


 T ‚àó

if Gij is a leaf node.
(R2‚Äô)
[X Œ∏ (Œª)]Gij  < wji ‚áí [Œ≤ ‚àó (Œª)]Gij = 0,
However, (R1‚Äô) and (R2‚Äô) are not applicable to detect inactive nodes as they involve Œ∏‚àó (Œª). Inspired
by SAFE [8], we first estimate a set Œò containing Œ∏‚àó (Œª). Let [XT Œò]Gij = {[XT Œ∏]Gij : Œ∏ ‚àà Œò} and


Sij (z) = zGij ‚àí PCji zGij .
(16)
4

Then, we can relax (R1‚Äô) and (R2‚Äô) as
n
o

supŒ∂ Sij (Œ∂) : Œ∂Gij ‚àà Œûij ‚äá [XT Œò]Gij < wji ‚áí [Œ≤ ‚àó (Œª)]Gij = 0, if Gij is a non-leaf node, (R1‚àó )
o
n 
 
if Gij is a leaf node.
(R2‚àó )
supŒ∂ Œ∂Gij  : Œ∂Gij ‚àà [XT Œò]Gij < wji ‚áí [Œ≤ ‚àó (Œª)]Gij = 0,
In view of (R1‚àó ) and (R2‚àó ), we sketch the procedure to develop MLFre in the following three steps.
Step 1 We estimate a set Œò that contains Œ∏‚àó (Œª).
Step 2 We solve for the supreme values in (R1‚àó ) and (R2‚àó ), respectively.
Step 3 We develop MLFre by plugging the supreme values obtained in Step 2 to (R1‚àó ) and (R2‚àó ).
4.1 The Effective Interval of the Regularization Parameter Œª
The geometric property of the dual problem in (2), i.e., Œ∏‚àó (Œª) = PF (y/Œª), implies that Œ∏‚àó (Œª) =
y/Œª if y/Œª ‚àà F. Moreover, (R1) for the root node G01 leads to Œ≤ ‚àó (Œª) = 0 if y/Œª is an interior point
of F. Indeed, the following theorem presents stronger results.
Theorem 7. For TGL, let Œªmax = max {Œª : y/Œª ‚àà F} and S01 (¬∑) be defined by Eq. (16). Then,
(i): Œªmax = {Œª : kS01 (XT y/Œª)k = w10 }.
(ii): yŒª ‚àà F ‚áî Œª ‚â• Œªmax ‚áî Œ∏‚àó (Œª) = yŒª ‚áî Œ≤ ‚àó (Œª) = 0.
For more discussions on Œªmax , please refer to Section H in the supplements.

5

The Proposed Multi-Layer Feature Reduction Method for TGL

We follow the three steps in Section 4 to develop MLFre. Specifically, we first present an accurate
estimation of the dual optimum in Section 5.1, then we solve for the supreme values in (R1‚àó ) and
(R2‚àó ) in Section 5.2, and finally we present the proposed MLFre in Section 5.3.
5.1 Estimation of the Dual Optimum
We estimate the dual optimum by the geometric properties of projection operators [recall that
Œ∏‚àó (Œª) = PF (y/Œª)]. We first introduce a useful tool to characterize the projection operators.
Definition 8. [2] For a closed convex set C and a point z0 ‚àà C, the normal cone to C at z0 is
NC (z0 ) = {Œ∂ : hŒ∂, z ‚àí z0 i ‚â§ 0, ‚àÄ z ‚àà C}.
Theorem 7 implies that Œ∏‚àó (Œª) is known with Œª ‚â• Œªmax . Thus, we can estimate Œ∏‚àó (Œª) in terms of a
known Œ∏‚àó (Œª0 ). This leads to Theorem 9 that bounds the dual optimum by a small ball.
Theorem 9. For TGL, suppose that Œ∏‚àó (Œª0 ) is known with Œª0 ‚â§ Œªmax . For Œª ‚àà (0, Œª0 ), we define
(y
Œ∏‚àó (Œª0 ), 
if Œª0 < Œªmax ,
Œª0 ‚àí
n(Œª0 ) =
0
T y
XS1 X Œªmax , if Œª0 = Œªmax ,
r(Œª, Œª0 ) =

y
Œª

‚àí Œ∏‚àó (Œª0 ),
hr(Œª,Œª0 ),n(Œª0 )i
n(Œª0 ).
kn(Œª0 )k2

r‚ä• (Œª, Œª0 ) = r(Œª, Œª0 ) ‚àí

Then, the following hold:
(i): n(Œª0 ) ‚àà NF (Œ∏‚àó (Œª0 )).
(ii): kŒ∏‚àó (Œª) ‚àí (Œ∏‚àó (Œª0 ) + 21 r‚ä• (Œª, Œª0 ))k ‚â§ 12 kr‚ä• (Œª, Œª0 )k.
Theorem 9 indicates that Œ∏‚àó (Œª) lies inside the ball of radius 21 kr‚ä• (Œª, Œª0 )k centered at
o(Œª, Œª0 ) = Œ∏‚àó (Œª0 ) + 12 r‚ä• (Œª, Œª0 ).
5.2 Solving the Nonconvex Optimization Problems in (R1‚àó ) and (R2‚àó )
We solve for the supreme values in (R1‚àó ) and (R2‚àó ). For notational convenience, let
Œò = {Œ∏ : kŒ∏ ‚àí o(Œª, Œª0 )k ‚â§ 12 kr‚ä• (Œª, Œª0 )k},
Œûij

T

= {Œ∂ : Œ∂ ‚àà HGij , kŒ∂ ‚àí [X o(Œª, Œª0 )]Gij k ‚â§
‚àó

T

‚ä•
1
i
2 kr (Œª, Œª0 )kkXGj k2 }.

Œûij

Theorem 9 implies that Œ∏ (Œª) ‚àà Œò, and thus [X Œò]Gij ‚äÜ
for all non-leaf nodes
‚àó
‚àó
MLFre by (R1 ) and (R2 ), we need to solve the following optimization problems:
sij (Œª, Œª0 ) = supŒ∂ {kSij (Œ∂)k : Œ∂ ‚àà Œûij }, if Gij is a non-leaf node,
sij (Œª, Œª0 )

= supŒ∂ {kŒ∂k : Œ∂ ‚àà

Œûij },
5

if

Gij

is a leaf node.

Gij .

(17)
(18)
To develop
(19)
(20)

Before we solve problems (19) and (20), we first introduce some notations.
Definition 10. For a non-leaf node Gij of an index tree T , let Ic (Gij ) = {k : Gi+1
‚äÇ Gij }. If
k
i+1
i+1
i
i
i
for
Gj \ ‚à™k‚ààIc (Gij ) Gk 6= ‚àÖ, we define a virtual child node of Gj by Gj 0 = Gj \ ‚à™k‚ààIc (Gij ) Gi+1
k
0
0
0
j ‚àà {ni+1 + 1, ni+1 + 2, . . . , ni+1 + ni+1 }, where ni+1 is the number of virtual nodes of depth
i + 1. We set the weights wji 0 = 0 for all virtual nodes Gij 0 .
Another useful concept is the so-called unique path between the nodes in the tree.
Lemma 11. [16] For any non-root node Gij , we can find a unique path from Gij to the root G01 . Let
the nodes on this path be Glrl , where l ‚àà 0 ‚à™ [i], r0 = 1, and ri = j. Then, the following hold:
Gij ‚äÇ Glrl , ‚àÄ l ‚àà 0 ‚à™ [i ‚àí 1].

(21)

Gij

(22)

‚à©

Glr

= ‚àÖ, ‚àÄ r 6= rl , l ‚àà [i ‚àí 1], r ‚àà [ni ].

Solving Problem (19) We consider the following equivalent problem of (19).
2
1 i
2 (sj (Œª, Œª0 ))

= supŒ∂ { 12 kSij (Œ∂)k2 : Œ∂ ‚àà Œûij }, if Gij is a non-leaf node.

(23)

Although both the objective function and feasible set of problem (23) are convex, it is nonconvex as
we need to find the supreme value. We derive the closed form solutions of (19) and (23) as follows.
Theorem 12. Let c = [XT o(Œª, Œª0 )]Gij , Œ≥ =
output of Algorithm 1 with input XT o(Œª, Œª0 ).

1
‚ä•
i
2 kr (Œª, Œª0 )kkXGj k2 ,

and vi , i ‚àà 0 ‚à™ [d] be the

i
(i): Suppose that c ‚àà
/ Cji . Then, sij (Œª, Œª0 ) = kvG
i k + Œ≥.
j

(ii): Suppose that node Gij has a virtual child node. Then, for any c ‚àà Cji , sij (Œª, Œª0 ) = Œ≥.
(iii): Suppose that node Gij has no virtual child node. Then, the following hold.
(iii.a): If c ‚àà rbd Cji , then sij (Œª, Œª0 ) = Œ≥.
(iii.b): If c ‚àà ri Cji , then, for any node Gtk ‚äÇ Gij , where t ‚àà {i + 1, . . . , d} and k ‚àà [nt + n0t ], let
the nodes on the path from Gtk to Gij be Glrl , where l = i, . . . , t, ri = j, and rt = k, and


Xt
t
l
Œì(Gi+1
wrl l ‚àí kvG
.
(24)
l k
ri+1 , Gk ) =
r
l=i+1

l




t
Then, sij (Œª, Œª0 ) = Œ≥ ‚àí min{(k,t):Gtk ‚äÇGij } Œì(Gi+1
.
ri+1 , Gk )
+

Solving Problem (20) We can solve problem (20) by the Cauchy-Schwarz inequality.
Theorem 13. For problem (20), we have sij (Œª, Œª0 ) = k[XT o(Œª, Œª0 )]Gij k + 21 kr‚ä• (Œª, Œª0 )kkXGij k2 .
5.3 The Multi-Layer Screening Rule
In real-world applications, the optimal parameter values are usually unknown. Commonly used
approaches to determine an appropriate parameter value, such as cross validation and stability selection, solve TGL many times along a grid of parameter values. This process can be very time
consuming. Motivated by this challenge, we present MLFre in the following theorem by plugging
the supreme values found by Theorems 12 and 13 into (R1‚àó ) and (R2‚àó ), respectively.
Theorem 14. For the TGL problem, suppose that we are given a sequence of parameter values
Œªmax = Œª0 > Œª1 > ¬∑ ¬∑ ¬∑ > ŒªK . For each integer k = 0, . . . , K ‚àí 1, we compute Œ∏‚àó (Œªk ) from a given
Œ≤ ‚àó (Œªk ) via Eq. (3). Then, for i = 1, . . . , d, MLFre takes the form of
sij (Œªk+1 , Œªk ) < wji ‚áí [Œ≤ ‚àó (Œª)]Gij = 0, ‚àÄ j ‚àà [ni ].

(MLFre)

Remark 2. We apply MLFre to identify inactive nodes hierarchically in a top-down fashion. Note
that, we do not need to apply MLFre to node Gij if one of its ancestor nodes passes the rule.
Remark 3. To simplify notations, we consider TGL with a single tree, in the proof. However, all
major results are directly applicable to TGL with multiple trees, as they are independent from each
other. We note that, many sparse models, such as Lasso, group Lasso, and sparse group Lasso, are
special cases of TGL with multiple trees.
6

(a) synthetic 1, p = 20000

(b) synthetic 1, p = 50000

(c) synthetic 1, p = 100000

(d) synthetic 2, p = 20000

(e) synthetic 2, p = 50000

(f) synthetic 2, p = 100000

Figure 1: Rejection ratios of MLFre on two synthetic data sets with different feature dimensions.

6

Experiments

We evaluate MLFre on both synthetic and real data sets by two measurements. The first measure is
the rejection ratios of MLFre for each level of the tree. Let p0 be the number of zero coefficients in
the solution vector and G i be the index set of the inactive nodes withPdepth i identified by MLFre.
i

|Gi |

The rejection ratio of the ith layer of MLFre is defined by ri = k‚ààGp0 k , where |Gik | is the
number of features contained in node Gik . The second measure is speedup, namely, the ratio of the
running time of the solver without screening to the running time of solver with MLFre.
For each data set, we run the solver combined with MLFre along a sequence of 100 parameter values
equally spaced on the logarithmic scale of Œª/Œªmax from 1.0 to 0.05. The solver for TGL is from the
SLEP package [15]. It also provides an efficient routine to compute Œªmax .

6.1 Simulation Studies
We perform experiments on two synthetic data Table 1: Running time (in seconds) for solving
sets, named synthetic 1 and synthetic 2, which TGL along a sequence of 100 tuning parameare commonly used in the literature [21, 31]. ter values of Œª equally spaced on the logarithmic
The true model is y = XŒ≤ ‚àó + 0.01,  ‚àº scale of Œª/Œªmax from 1.0 to 0.05 by (a): the solver
N (0, 1). For each of the data set, we fix N = [15] without screening (see the third column); (b):
250 and select p = 20000, 50000, 100000. We the solver with MLFre (see the fifth column).
create a tree with height 4, i.e., d = 3. The
Dataset
p
solver MLFre MLFre+solver speedup
average sizes of the nodes with depth 1, 2 and
20000
483.96
1.03
30.17
16.04
3 are 50, 10, and 1, respectively. Thus, if
synthetic 1 50000 1175.91 2.95
39.49
29.78
p = 100000, we have roughly n1 = 2000,
100000 2391.43 6.57
58.91
40.60
n2 = 10000, and n3 = 100000. For synthet20000
470.54
1.19
37.87
12.43
ic 1, the entries of the data matrix X are i.i.d.
synthetic 2 50000 1122.30 3.13
43.97
25.53
standard Gaussian with zero pair-wise correla100000 2244.06 6.18
60.96
36.81
tion, i.e., corr (xi , xj ) = 0 for the ith and j th
ADNI+GMV 406262 20911.92 81.14
492.08
42.50
columns of X with i 6= j. For synthetic 2,
ADNI+WMV 406262 21855.03 80.83
556.19
39.29
the entries of X are drawn from standard GausADNI+WBV 406262 20812.06 82.10
564.36
36.88
sian with pair-wise correlation corr (xi , xj ) =
0.5|i‚àíj| . To construct Œ≤ ‚àó , we first randomly select 50% of the nodes with depth 1, and then randomly select 20% of the children nodes (with depth 2) of the remaining nodes with depth 1. The
components of Œ≤ ‚àó corresponding to the remaining nodes are populated from a standard Gaussian,
and the remaining ones are set to zero.
7

(a) ADNI+GMV

(b) ADNI+WMV

(c) ADNI+WBV

Figure 2: Rejection ratios of MLFre on ADNI data set with grey matter volume (GMV), white mater
volume (WMV), and whole brain volume (WBV) as response vectors, respectively.
Fig. 1 shows the rejection ratios of all three layers of MLFre. We can see that MLFre identifies
P3
almost all of the inactive nodes, i.e., i=1 ri ‚â• 90%, and the first layer contributes the most.
Moreover, Fig. 1 also indicates that, as the feature dimension (and the number of nodes in each level)
P3
increases, MLFre identifies more inactive nodes, i.e., i=1 ri ‚âà 100%. Thus, we can expect a more
significant capability of MLFre in identifying inactive nodes on data sets with higher dimensions.
Table 1 shows the running time of the solver with and without MLFre. We can observe significant
speedups gained by MLFre, which are up to 40 times. Take synthetic 1 with p = 100000 for
example. The solver without MLFre takes about 40 minutes to solve TGL at 100 parameter values.
Combined with MLFre, the solver only needs less than one minute for the same task. Table 1 also
shows that the computational cost of MLFre is very low‚Äîthat is negligible compared to that of the
solver without MLFre. Moreover, as MLFre identifies more inactive nodes with increasing feature
dimensions, Table 1 shows that the speedup gained by MLFre becomes more significant as well.
6.2 Experiments on ADNI data set
We perform experiments on the Alzheimers Disease Neuroimaging Initiative (ADNI) data set
(http://adni.loni.usc.edu/). The data set consists of 747 patients with 406262 single
nucleotide polymorphisms (SNPs). We create the index tree such that n1 = 4567, n2 = 89332,
and n3 = 406262. Fig. 2 presents the rejection ratios of MLFre on the ADNI data set with grey
matter volume (GMV), white matter volume (WMV), and whole brain volume (WBV) as response,
P3
respectively. We can see that MLFre identifies almost all inactive nodes, i.e., i=1 ri ‚âà 100%. As
a result, we observe significant speedups gained by MLFre‚Äîthat are about 40 times‚Äîfrom Table
1. Specifically, with GMV as response, the solver without MLFre takes about six hours to solve
TGL at 100 parameter values. However, combined with MLFre, the solver only needs about eight
minutes for the same task. Moreover, Table 1 also indicates that the computational cost of MLFre is
very low‚Äîthat is negligible compared to that of the solver without MLFre.

7

Conclusion

In this paper, we propose a novel multi-layer feature reduction (MLFre) method for TGL. Our major
technical contributions lie in two folds. The first is the novel hierarchical projection algorithm that
is able to exactly and efficiently recover the subgradients of the tree-guided regularizer with respect
to each node from their mixture. The second is that we show a highly nontrivial nonconvex problem
admits a closed form solution. To the best of our knowledge, MLFre is the first screening method
that is applicable to TGL. An appealing feature of MLFre is that it is exact in the sense that the
identified inactive nodes are guaranteed to be absent from the sparse representations. Experiments
on both synthetic and real data sets demonstrate that MLFre is very effective in identifying inactive
nodes, leading to substantial savings in computational cost and memory usage without sacrificing
accuracy. Moreover, the capability of MLFre in identifying inactive nodes on higher dimensional
data sets is more significant. We plan to generalize MLFre to more general and complicated sparse
models, e.g., over-lapping group Lasso with logistic loss. In addition, we plan to apply MLFre to
other applications, e.g., brain image analysis [10, 18] and natural language processing [27, 28].

Acknowledgments
This work is supported in part by research grants from NIH (R01 LM010730, U54 EB020403) and
NSF (IIS- 0953662, III-1539991, III-1539722).

8

References
[1] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization with sparsity-inducing penalties. Foundations and Trends in Machine Learning, 4(1):1‚Äì106, Jan. 2012.
[2] H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces.
Springer, 2011.
[3] M. Bazaraa, H. Sherali, and C. Shetty. Nonlinear Programming: Theory and Algorithms. WileyInterscience, 2006.
[4] J. Borwein and A. Lewis. Convex Analysis and Nonlinear Optimization, Second Edition. Canadian
Mathematical Society, 2006.
[5] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[6] X. Chen, Q. Lin, S. Kim, J. Carbonell, and E. Xing. Smoothing proximal gradient method for general
structured sparse regression. Annals of Applied Statistics, pages 719‚Äì752, 2012.
[7] W. Deng, W. Yin, and Y. Zhang. Group sparse optimization by alternating direction method. Technical
report, Rice CAAM Report TR11-06, 2011.
[8] L. El Ghaoui, V. Viallon, and T. Rabbani. Safe feature elimination in sparse supervised learning. Pacific
Journal of Optimization, 8:667‚Äì698, 2012.
[9] J.-B. Hiriart-Urruty. From convex optimization to nonconvex optimization. necessary and sufficient conditions for global optimality. In Nonsmooth optimization and related topics. Springer, 1988.
[10] R. Jenatton, A. Gramfort, V. Michel, G. Obozinski, E. Eger, F. Bach, and B. Thirion. Multiscale mining of
fmri data with hierarchical structured sparsity. SIAM Journal on Imaging Science, pages 835‚Äì856, 2012.
[11] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for hierarchical sparse coding.
Journal of Machine Learning Research, 12:2297‚Äì2334, 2011.
[12] K. Jia, T. Chan, and Y. Ma. Robust and practical face recognition via structured sparsity. In European
Conference on Computer Vision, 2012.
[13] S. Kim and E. Xing. Tree-guided group lasso for multi-task regression with structured sparsity. In
International Conference on Machine Learning, 2010.
[14] S. Kim and E. Xing. Tree-guided group lasso for multi-response regression with structured sparsity, with
an application to eqtl mapping. The Annals of Applied Statistics, 2012.
[15] J. Liu, S. Ji, and J. Ye. SLEP: Sparse Learning with Efficient Projections. Arizona State University, 2009.
[16] J. Liu and J. Ye. Moreau-Yosida regularization for grouped tree structure learning. In Advances in neural
information processing systems, 2010.
[17] J. Liu, Z. Zhao, J. Wang, and J. Ye. Safe screening with variational inequalities and its application to
lasso. In International Conference on Machine Learning, 2014.
[18] M. Liu, D. Zhang, P. Yap, and D. Shen. Tree-guided sparse coding for brain disease classification. In
Medical Image Computing and Computer-Assisted Intervention, 2012.
[19] K. Ogawa, Y. Suzuki, and I. Takeuchi. Safe screening of non-support vectors in pathwise SVM computation. In ICML, 2013.
[20] A. RuszczynÃÅski. Nonlinear Optimization. Princeton University Press, 2006.
[21] R. Tibshirani, J. Bien, J. Friedman, T. Hastie, N. Simon, J. Taylor, and R. Tibshirani. Strong rules for
discarding predictors in lasso-type problems. Journal of the Royal Statistical Society Series B, 74:245‚Äì
266, 2012.
[22] J. Wang, W. Fan, and J. Ye. Fused lasso screening rules via the monotonicity of subdifferentials. IEEE
Transactions on Pattern Analysis and Machine Intelligence, PP(99):1‚Äì1, 2015.
[23] J. Wang, P. Wonka, and J. Ye. Scaling svm and least absolute deviations via exact data reduction. In
International Conference on Machine Learning, 2014.
[24] J. Wang, P. Wonka, and J. Ye. Lasso screening rules via dual polytope projection. Journal of Machine
Learning Research, 16:1063‚Äì1101, 2015.
[25] J. Wang and J. Ye. Two-Layer feature reduction for sparse-group lasso via decomposition of convex sets.
Advances in neural information processing systems, 2014.
[26] Z. J. Xiang, H. Xu, and P. J. Ramadge. Learning sparse representation of high dimensional data on large
scale dictionaries. In NIPS, 2011.
[27] D. Yogatama, M. Faruqui, C. Dyer, and N. Smith. Learning word representations with hierarchical sparse
coding. In International Conference on Machine Learning, 2015.
[28] D. Yogatama and N. Smith. Linguistic structured sparsity in text categorization. In Proceedings of the
Annual Meeting of the Association for Computational Linguistics, 2014.
[29] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the
Royal Statistical Society Series B, 68:49‚Äì67, 2006.
[30] P. Zhao, G. Rocha, and B. Yu. The composite absolute penalties family for grouped and hierarchical
variable selection. Annals of Statistics, 2009.
[31] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal
Statistical Society Series B, 67:301‚Äì320, 2005.

9

