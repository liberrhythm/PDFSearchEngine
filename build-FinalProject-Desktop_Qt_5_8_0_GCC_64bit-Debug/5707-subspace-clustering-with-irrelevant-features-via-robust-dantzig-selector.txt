Subspace Clustering with Irrelevant Features via
Robust Dantzig Selector

Chao Qu
Department of Mechanical Engineering
National University of Singapore

Huan Xu
Department of Mechanical Engineering
National University of Singapore

A0117143@u.nus.edu

mpexuh@nus.edu.sg

Abstract
This paper considers the subspace clustering problem where the data contains
irrelevant or corrupted features. We propose a method termed â€œrobust Dantzig selectorâ€ which can successfully identify the clustering structure even with the presence of irrelevant features. The idea is simple yet powerful: we replace the inner
product by its robust counterpart, which is insensitive to the irrelevant features
given an upper bound of the number of irrelevant features. We establish theoretical guarantees for the algorithm to identify the correct subspace, and demonstrate
the effectiveness of the algorithm via numerical simulations. To the best of our
knowledge, this is the first method developed to tackle subspace clustering with
irrelevant features.

1

Introduction

The last decade has witnessed fast growing attention in research of high-dimensional data: images,
videos, DNA microarray data and data from many other applications all have the property that the
dimensionality can be comparable or even much larger than the number of samples. While this
setup appears ill-posed in the first sight, the inference and recovery is possible by exploiting the fact
that high-dimensional data often possess low dimensional structures [3, 14, 19]. On the other hand,
in this era of big data, huge amounts of data are collected everywhere, and such data is generally
heterogeneous. Clean data and irrelevant or even corrupted information are often mixed together,
which motivates us to consider the high-dimensional, big but dirty data problem. In particular, we
study the subspace clustering problem in this setting.
Subspace clustering is an important subject in analyzing high-dimensional data, inspired by many
real applications[15]. Given data points lying in the union of multiple linear spaces, subspace clustering aims to identify all these linear spaces, and cluster the sample points according to the linear
spaces they belong to. Here, different subspaces may correspond to motion of different objects in
video sequence [11, 17, 20], different rotations, translations and thickness in handwritten digit or
the latent communities for the social graph [15, 5].
A variety of algorithms of subspace clustering have been proposed in the last several years including algebraic algorithms [16], iterative methods [9, 1], statistical methods [11, 10], and spectral
clustering-based methods [6, 7]. Among them, sparse subspace clustering (SSC) not only achieves
state-of-art empirical performance, but also possesses elegant theoretical guarantees. In [12], the
authors provide a geometric analysis of SSC which explains rigorously why SSC is successful even
when the subspaces are overlapping [12]. [18] and [13] extend SSC to the noisy case, where data are
contaminated by additive Gaussian noise. Different from these work, we focus on the case where
some irrelevant features are involved.
1

Mathematically, SSC indeed solves for each sample a sparse linear regression problem with the
dictionary being all other samples. Many properties of sparse linear regression problem are well
understood in the clean data case. However, the performance of most standard algorithms deteriorates (e.g. LASSO and OMP) even only a few entries are corrupted. As such, it is well expected
that standard SSC breaks for subspace clustering with irrelevant or corrupted features (see Section 5
for numerical evidences). Sparse regression under corruption is a hard problem, and few work has
addressed this problem [8][21] [4].
Our contribution: Inspired by [4], we use a simple yet powerful tool called robust inner product
and propose the robust Dantzig selector to solve the subspace clustering problem with irrelevant
features. While our work is based upon the robust inner product developed to solve robust sparse
regression, the analysis is quite different from the regression case since both the data structures and
the tasks are completely different: for example, the RIP condition â€“ essential for sparse regression â€“
is hardly satisfied for subspace clustering [18]. We provide sufficient conditions to ensure that
the Robust Dantzig selector can detect the true subspace clustering. We further demonstrate via
numerical simulation the effectiveness of the proposed method. To the best of our knowledge, this
is the first attempt to perform subspace clustering with irrelevant features.

2
2.1

Problem setup and method
Notations and model

The clean data matrix is denoted by XA âˆˆ RDÃ—N , where each column corresponds to a data point,
normalized to a unit vector. The data points are lying on a union of L subspace S = âˆªL
l=1 Sl .
Each subspace Sl is of dimension dl which is smaller than D and contains Nl data samples with
N1 +N2 +Â· Â· Â·+NL = N . We denote the observed dirty data matrix by X âˆˆ R(D+D1 )Ã—N . Out of the
T
T T
D + D1 features, up to D1 of them are irrelevant. Without loss of generality, let X = [XO
, XA
] ,
where XO âˆˆ RD1 Ã—N denotes the irrelevant data. The subscript A and O denote the set of row
indices corresponding to true and irrelevant features and the superscript T denotes the transpose.
Notice that we do not know O a priori except its cardinality is D1 . The model is illustrated in Figure
(l)
1. Let XA âˆˆ RDÃ—Nl denote the selection of columns in XA that belongs to Sl . Similarly, denote
the corresponding columns in X by X (l) . Without loss of generality, let X = [X (1) , X (2) , ..., X (L) ]
be ordered. Further more, we use the subscript â€œâˆ’iâ€to describe a matrix that excludes the column
(l)
(l)
(l)
(l)
(l)
i, e.g., (XA )âˆ’i = [(xA )1 , ..., (xA )iâˆ’1 , (xA )i+1 , ..., (xA )Nl ]. We use the superscript lc to describe
c

(1)

(lâˆ’1)

(l+1)

(L)

a matrix that excludes column in subspace l, e.g., (XA )l = [XA , ..., XA , XA , ..., XA ].
For a matrix Î£, we use Î£s,Î· to denote the submatrix with row indices in set s and column indices
in set Î·. For any matrix Z, P (Z) denotes the symmetrized convex hull of its column, i.e., P (Z) =
(l)
l
conv(Â±z1 , Â±z2 , ...., Â±zN ) . We define Pâˆ’i
:= P ((XA )âˆ’i ) for simplification, i.e., the symmetrized
convex hull of clean data in subspace l except data i. Finally we use k Â· k2 to denote the l2 norm of
a vector and k Â· kâˆ to denote infinity norm of a vector or a matrix. Caligraphic letters such as X , Xl
represent the set containing all columns of the corresponding clean data matrix.

Figure 1: Illustration of the model of irrelevant features in the subspace clustering problem. The
left one is the model addressed in this paper: Among total D + D1 features, up tp D1 of them are
irrelevant. The right one illustrates a more general case, where the value of any D1 element of each
column can be arbitrary (e.g., due to corruptions). It is a harder case and left for future work.

2

Figure 2: Illustration of the Subspace Detection Property. Here, each figure corresponds to a matrix
where each column is ci , and non-zero entries are in white. The left figure satisfies this property.
The right one does not.
2.2

Method

In this secion we present our method as well as the intuition that derives it. When all observed data
are clean, to solve the subspace clustering problem, the celebrated SSC [6] proposes to solve the
following convex programming
min kci k1
ci

s.t.

xi = Xâˆ’i ci ,

(1)

for each data point xi . When data are corrupted by noise of small magnitude such as Gaussian noise,
a straightforward extension of SSC is the Lasso type method called â€œLasso-SSCâ€ [18, 13]
min kci k1 +
ci

Î»
kxi âˆ’ Xâˆ’i ci k22 .
2

(2)

Note that while Formulation (2) has the same form as Lasso, it is used to solve the subspace clustering task. In particular, the support recovery analysis of Lasso does not extend to this case, as Xâˆ’i
typically does not satisfy the RIP condition [18].
This paper considers the case where X contains irrelevant/gross corrupted features. As we discussed above, Lasso is not robust to such corruption. An intuitive idea is to consider the following
formulation first proposed for sparse linear regression [21].
min kci k1 +
ci ,E

Î»
kxi âˆ’ (Xâˆ’i âˆ’ E)ci k22 + Î·kEkâˆ— ,
2

(3)

where k Â· kâˆ— is some norm corresponding to the sparse type of E. One major challenge of this
formulation is that it is not convex. As such, it is not clear how to efficiently find the optimal
solution, and how to analyze the property of the solution (typically done via convex analysis) in the
subspace clustering task.
Our method is based on the idea of robust inner product. The robust inner product ha, bik is defined
as follows: For vector a âˆˆ RD , b âˆˆ RD , we compute qi = ai bi , i = 1, ..., N . Then {|qi |} are
P sorted
and the smallest (D âˆ’ k) are selected. Let â„¦ be the set of selected indices, then ha, bik = iâˆˆâ„¦ qi ,
i.e., the largest k terms are truncated. Our main idea is to replace all inner products involved by
robust counterparts ha, biD1 , where D1 is the upper bound of the number of irrelevant features.
The intuition is that the irrelevant features with large magnitude may affect the correct subspace
clustering. This simple truncation process will avoid this. We remark that we do not need to know
the exact number of irrelevant feature, but instead only an upper bound of it.
Extending (2) using the robust inner product leads the following formulation:
min kci k1 +
ci

Î» T
c Î£Ì‚ci âˆ’ Î»Î³Ì‚ T ci ,
2 i

(4)

T
T
where Î£Ì‚ and Î³Ì‚ are robust counterparts of Xâˆ’i
Xâˆ’i and Xâˆ’i
xi . Unfortunately, Î£Ì‚ may not be a
positive semidefinite matrix, thus (4) is not a convex program. Unlike the work [4][8] which studies

3

non-convexity in linear regression, the difficulty of non-convexity in the subspace clustering task
appears to be hard to overcome.
Instead we turn to the Dantzig Selector, which is essentially a linear program (and hence no positive
semidefiniteness is required):
T
min kci k1 + Î»kXâˆ’i
(Xâˆ’i ci âˆ’ xi )kâˆ .

(5)

ci

Replace all inner product by its robust counterpart, we propose the following Robust Dantzig Selector, which can be easily recast as a linear program:
min kci k1 + Î»kÎ£Ì‚ci âˆ’ Î³Ì‚kâˆ ,

Robust Dantzig Selector:

(6)

ci

Subspace Detection Property: To measure whether the algorithm is successful, we define the
criterion Subspace Detection Property following [18]. We say that the Subspace Detection Property holds, if and only if for all i, the optimal solution to the robust Dantzig Selector satisfies (1)
Non-triviality: ci is not a zero vector; (2) Self-Expressiveness Property: nonzeros entries of ci
correspond to only columns of X sampled from the same subspace as xi . See Figure 2 for illustrations.

3

Main Results

To avoid repetition and cluttered notations, we denote the following primal convex problem by
P (Î£, Î³)
min kck1 + Î»kÎ£c âˆ’ Î³kâˆ .
c

Its dual problem, denoted by D(Î£, Î³), is
maxhÎ¾, Î³i subject to kÎ¾k1 = Î»
Î¾

kÎ£Î¾kâˆ â‰¤ 1.

(7)

Before we presents our results, we define some quantities.
The dual direction is an important geometric term introdcued in analyzing SSC [12]. Here we define
similarly the dual direction of the robust Dantzig selector: Notice that the dual of robust Dantzig
T
T
problem is D(Î£Ì‚, Î³Ì‚), where Î³Ì‚ and Î£Ì‚ are robust counterparts of Xâˆ’i
xi and Xâˆ’i
Xâˆ’i respectively
(recall that Xâˆ’i and xi are the dirty data). We decompose Î£Ì‚ into two parts Î£Ì‚ = (XA )Tâˆ’i (XA )âˆ’i + Î£Ìƒ,
where the first term corresponds to the clean data, and the second term is due to the irrelevant features
and truncation from the robust inner product. Thus, the second constraint of the dual problem
becomes k((XA )Tâˆ’i (XA )âˆ’i + Î£Ìƒ)Î¾kâˆ â‰¤ 1. Let Î¾ be the optimal solution to the above optimization
(l)

problem, we define v(xi , Xâˆ’i , Î») := (XA )âˆ’i Î¾ and the dual direction as v l =

v(xli ,Xâˆ’i ,Î»)
(l)
kv(xli ,Xâˆ’i ,Î»)k2

.

l
Similarly as SSC [12], we define the subspace incoherence. Let V l = [v1l , v2l , ..., vN
]. The incoherl
(k)

ence of a point set Xl to other clean data points is defined as Âµ(Xl ) = maxk:k6=l k(XA )T V l kâˆ .
Recall that we decompose Î£Ì‚ and Î³Ì‚ as Î£Ì‚ = (XA )Tâˆ’i (XA )âˆ’i + Î£Ìƒ and Î³Ì‚ = (XA )Tâˆ’i (xA )i + Î³Ìƒ.
Intuitively, for robust Dantzig selecter to succeed, we want Î£Ìƒ and Î³Ìƒ not too large. Particularly, we
assume k(xA )i kâˆ â‰¤ 1 and k(XA )âˆ’i kâˆ â‰¤ 2 .
l
Theorem 1 (Deterministic Model). Denote Âµl := Âµ(Xl ), rl := mini:xi âˆˆXl r(Pâˆ’i
), r :=
minl=1,...,L rl and suppose Âµl < rl for all l. If

rl âˆ’ ul
1
< min
,
l 2D1 2
r2 âˆ’ 4D1 1 2 r âˆ’ 2D1 22
2 (ul + rl )

(8)

then the subspace detection property holds for all Î» in the range
1
rl âˆ’ ul
< Î» < min
.
l 2D1 2
r2 âˆ’ 4D1 1 2 r âˆ’ 2D1 22
2 (ul + rl )
4

(9)

In an ideal case when D1 = 0, the condition of the upper bound of Î» reduces to rl > ul , similar to
the condition for SSC in the noiseless case [12].
Based on Condition (8), under a randomized generative model, we can derive how many irrelevant
features can be tolerated.
Theorem 2 (Random model). Suppose there are L subspaces and for simplicity, all subspaces have
same dimension d and are chosen uniformly at random. For each subspace there are Ïd + 1 points
chosen independently and uniformly at random. Up to D1 features of data are irrelevant. Each
data point (including true and irrelevant features) is independent from other data points. Then for
some universal constants
C1 , C2 , the subspace detection property holds with probability at least
âˆš
1 âˆ’ N4 âˆ’ N exp(âˆ’ Ïd) if
dâ‰¤

Dc2 (Ï) log(Ï)
,
12 log N

and
log Ï
1 2
2 c (Ï) d

1
1âˆ’Îº
D
q
<Î»<
,
âˆš
C1 D1 (log D+C2 log N )
1
+
Îº
C
D
(log
D
+ C2 log N )
log Ï
1
1
âˆ’ ( 2c(Ï)
+
1)
d
D

q
12d log N
where Îº =
Dc2 (Ï) log Ï ; c(Ï) is a constant only depending on the density of data points on
subspace and satisfies (1) c(Ï) >
âˆš 0 for all Ï > 1, (2) there is a numerical value Ï0 , such that for all
Ï > Ï0 , one can take c(Ï) = 1/ 8.
Simplifying the above conditions, we can determine the number of irrelevant features that can be
tolerated. In particular, if d â‰¥ 2c2 (Ï) log Ï and we choose the Î» as
Î»=

4d
,
c2 (Ï) log Ï

then the maximal number of irrelevant feature D1 that can be torelated is
1âˆ’Îº
C0 Dc2 (Ï) log Ï
c(Ï)D log Ï
,
},
8C1 d(log(D) + C2 log N ) 1 + Îº C1 d(log(D) + C2 log N )
âˆš
with probability at least 1 âˆ’ N4 âˆ’ N exp(âˆ’ Ïd).
D1 = min{

If d â‰¤ 2c2 (Ï) log Ï, and we choose the same Î», then the number of irrelevant feature we can tolerate
is
q
Dc(Ï) logd Ï
1âˆ’Îº
C0 Dc2 (Ï) log Ï
D1 = min{ âˆš
,
},
4 2C1 (log(D) + C2 log N ) 1 + Îº C1 d(log(D) + C2 log N )
âˆš
with probability at least 1 âˆ’ N4 âˆ’ N exp(âˆ’ Ïd).
Remark 1. If D is much larger than D1 , the lower bound of Î» is proportional to the subspace
dimension d. When d increases, the upper bound of Î» decreases, since 1âˆ’Îº
1+Îº decreases. Thus the
valid range of Î» shrinks when d increases.
Remark 2. Ignoring the logarithm terms, when d is large, the tolerable D1 is proportional
to
âˆš
D
D
1âˆ’Îº D
min(C1 1âˆ’Îº
,
C
).
When
d
is
small,
D
is
proportional
to
min(C
,
C
D/
d)
.
2
1
1
2
1+Îº d
d
1+Îº d

4

Roadmap of the Proof

In this section, we lay out the roadmap of proof. In specific we want to establish the condition with
the number of irrelevant features, and the structure of data (i.e., the incoherence Âµ and inradius r)
for the algorithm to succeed. Indeed, we provide a lower bound of Î» such that the optimal solution
ci is not trivial; and an upper bound of Î» so that the Self-Expressiveness Property holds. Combining
them together established the theorems.
5

4.1

Self-Expressiveness Property

The Self-Expressiveness Property is related to the upper bound of Î». The proof technique is inspired
by [18] and [12], we first establish the following lemma, which provides a sufficient condition such
that Self-Expressiveness Property holds of the problem 6.
Lemma 1. Consider a matrix Î£ âˆˆ RN Ã—N and Î³ âˆˆ RN Ã—1 , If there exist a pair (cÌƒ, Î¾) such that cÌƒ
has a support S âŠ† T and
sgn(cÌƒs ) + Î£s,Î· Î¾Î· = 0,
kÎ£sc âˆ©T,Î· Î¾Î· kâˆ â‰¤ 1,
kÎ¾k1 = Î»,
kÎ£T c ,Î· Î¾Î· kâˆ < 1,

(10)

where Î· is the set of indices of entry i such that |(Î£cÌƒ âˆ’ Î³)i | = kÎ£cÌƒ âˆ’ Î³kâˆ , then for all optimal
solution câˆ— to the problem P (Î£, Î³), we have câˆ—T c = 0.
The variable Î¾ in Lemma 1 is often termed the â€œdual certificateâ€. We next consider an oracle problem
Ë† to construct such a dual certificate. This
P (Î£Ì‚l,l , Î³Ì‚l ), and use its dual optimal variable denoted by Î¾,
candidate satisfies all conditions in the Lemma 1 automatically except to show
kÎ£Ì‚lc ,Î·Ì‚ Î¾Ë†Î·Ì‚ kâˆ < 1,

(11)

c

where l denotes the set of indices expect the ones corresponding to subspace l. We can compare
c
this condition with the corresponding one in analyzing SSC, in which one need k(X)(l )T vkâˆ < 1,
c
where v is the dual certificate. Recall that we can decompose Î£Ì‚lc ,Î·Ì‚ = (XA )(l )T (XA )Î·Ì‚ + Î£Ìƒlc ,Î·Ì‚ .
Thus Condition 11 becomes
k(XA )(l

c

)T

((XA )Î·Ì‚ Î¾Ë†Î·Ì‚ ) + Î£Ìƒlc ,Î·Ì‚ Î¾Ë†Î·Ì‚ kâˆ < 1.

(12)

To show this holds, we need to bound two terms k(XA )Î·Ì‚ Î¾Ë†Î·Ì‚ k2 and kÎ£Ìƒlc ,Î·Ì‚ Î¾Ë†Î·Ì‚ kâˆ .
Bounding kÎ£Ìƒkâˆ , kÎ³Ìƒkâˆ
The following lemma relates D1 with kÎ£Ìƒkâˆ and kÎ³Ìƒkâˆ .
T
T
Lemma 2. Suppose Î£Ì‚ and Î³Ì‚ are robust counterparts of Xâˆ’i
Xâˆ’i and Xâˆ’i
xi respectively and among
D + D1 features, up to D1 are irrelevant. We can decompose Î£Ì‚ and Î³Ì‚ into following form Î£Ì‚ =
(XA )Tâˆ’i (XA )âˆ’i + Î£Ìƒ and Î³Ì‚ = (XA )Tâˆ’i (xA )i + Î³Ìƒ. We define Î´1 := kÎ³Ìƒkâˆ and Î´2 := kÎ£Ìƒkâˆ .If
k(xA )i kâˆ â‰¤ 1 and k(XA )âˆ’i kâˆ â‰¤ 2 , then Î´2 â‰¤ 2D1 22 , Î´1 â‰¤ 2D1 1 2 .

We then bound 1 and 2 in the random model
cap [2].
âˆš using the upper bound of the spherical
âˆš
Indeed we have 1 â‰¤ C1 (log D + C2 log N )/ D and 2 â‰¤ C1 (log D + C2 log N )/ D with high
probability.
Bounding kXÎ·Ì‚ Î¾Ë†Î·Ì‚ k2
By exploiting the feasible condition in the dual of the oracle problem, we obtain the following
bound:
1 + 2D1 Î»22
kXÎ·Ì‚ Î¾Ë†Î·Ì‚ k2 â‰¤
.
l )
r(Pâˆ’i
q
log Ï
l
âˆš
Furthermore, r(Pâˆ’i
) can be lower bound by c(Ï)
d and 2 can be upper bounded by C1 (log D+
2
âˆš
C2 log N )/ D in the random model with high probability. Thus the RHS can be upper bounded.
Plugging this upper bound into (12), we obtain the upper bound of Î».
4.2

Non-triviality with sufficiently large Î»

To ensure that the solution is not trivial (i.e., not all-zero), we need a lower bound on Î».
6

If Î» satisfies the following condition, the optimal solution to problem 6 can not be zero
Î»>

l )
r2 (Pâˆ’i

âˆ’

2D1 22

1
.
l )D  
âˆ’ 4r(Pâˆ’i
1 1 2

(13)

The proof idea is to show when Î» is large enough, the trivial solution c = 0 can not be optimal. In
particular, if c = 0, the corresponding value in the primal problem is Î»kÎ³Ì‚l kâˆ . We then establish a
lower bound of kÎ³Ì‚l kâˆ and a upper bound of kck1 + Î»kÎ£Ì‚l,l c âˆ’ Î³Ì‚l kâˆ so that the following inequality
always holds by some carefully choosen c.
kck1 + Î»kÎ£Ì‚l,l c âˆ’ Î³Ì‚l kâˆ < Î»kÎ³Ì‚l kâˆ .

(14)

l
). Notice
We then further lower bound the RHS of Equation (13) using the bound of 1 , 2 and r(Pâˆ’i
that condition (14) requires that Î» > A and condition (11) requires Î» < B, where A and B are some
terms depending on the number of irrelevant features. Thus we require A < B to get the maximal
number of irrelevant features that can be tolerated.

5

Numerical simulations

In this section, we use three numerical experiments to demonstrate the effectiveness of our method
to handle irrelevant/corrupted features. In particular, we test the performance of our method and
effect of number of irrelevant features and dimension subspaces d with respect to different Î». In
all experiments, the ambient dimension D = 200, sample density Ï = 5, the subspace are drawn
uniformly at random. Each subspace has Ïd+1 points chosen independently and uniformly random.
We measure the success of the algorithms using the relative violation of the subspace detection
property defined as follows,
P
|C|i,j
(i,j)âˆˆM
/
RelV iolation(C, M) = P
,
(i,j)âˆˆM |C|i,j
where C = [c1 , c2 , ..., cN ], M is the ground truth mask containing all (i, j) such that xi , xj belong
to a same subspace. If RelV iolation(C, M) = 0, then the subspace detection property is satisfied.
We also check whether we obtain a trivial solution, i.e., if any column in C is all-zero.
We first compare the robust Dantzig selector(Î» = 2) with SSC and LASSO-SSC ( Î» = 10). The
results are shown in Figure 3. The X-axis is the number of irrelevant features and the Y-axis is the
Relviolation defined above. The ambient dimension D = 200, L = 3, d = 5, the relative sample
density Ï = 5. The values of irrelevant features are independently sampled from a uniform distribution in the region [âˆ’2.5, 2.5] in (a) and [âˆ’10, 10] in (b). We observe from Figure 3 that both SSC
and Lasso SSC are very sensitive to irrelevant information. (Notice that RelViolation=0.1 is pretty
large and can be considered as clustering failure.) Compared with that, the proposed Robust Dantzig
Selector performs very well. Even when D1 = 20, it still detects the true subspaces perfectly. In
the same setting, we do some further experiments, our method breaks when D1 is about 40. We
also do further experiment for Lasso-SSC with different Î» in the supplementary material to show
Lasso-SSC is not robust to irrelevant features.
We also examine the relation of Î» to the performance of the algorithm. In Figure 4a, we test the
subspace detection property with different Î» and D1 . When Î» is too small, the algorithm gives a
trivial solution (the black region in the figure). As we increase the value of Î», the corresponding
solutions satisfy the subspace detection property (represented as the white region in the figure).
When Î» is larger than certain upper bound, RelV iolation becomes non-zero, indicating errors in
subspace clustering. In Figure 4b, we test the subspace detection property with different Î» and d.
Notice we rescale Î» with d, since by Theorem 3, Î» should be proportional to d. We observe that the
valid region of Î» shrinks with increasing d which matches our theorem.

6

Conclusion and future work

We studied subspace clustering with irrelevant features, and proposed the â€œrobust Dantzig selectorâ€
based on the idea of robust inner product, essentially a truncated version of inner product to avoid
7

Original SSC
Lasso SSC
Robust Dantzig Selector

1

0.8
RelViolation

RelViolation

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

Original SSC
Lasso SSC
Robust Dantzig Selector

1

0

5

10
Number of irrelevant features

15

0

20

0

5

10
Number of irrelevant features

(a)

15

20

(b)

2.5

10.5
10
9.5
9
8.5
8
7.5
7
6.5
6
5.5
5
4.5
4
3.5
3
2.5
2
1.5
1

2.3
2.1
1.9
1.7
1.5

Î»/d

Î»

Figure 3: Relviolation with different D1 . Simulated with D = 200, d = 5, L = 3, Ï = 5, Î» = 2,
and D1 from 1 to 20.

1.3
1.1
0.9
0.7
0.5
0.3
0.1

1

2

3

4

5

6

7

8

9

10

4

Number of irrelevant features D1

(a) Exact recovery with different number of
irrelevant features. Simulated with D =
200, d = 5, L = 3, Ï = 5 with an increasing D1 from 1 to 10. Black region:
trivial solution. White region: Non-trivial
solution with RelViolation=0. Gray region:
RelViolation> 0.02.

6

8

10

12

14

16

Subspace dimension d

(b) Exact recovery with different subspace
dimension d. Simulated with D = 200,
L = 3, Ï = 5, D1 = 5 and an increasing d from 4 to 16. Black region: trivial solution. White region: Non-trivial solution with RelViolation=0. Gray region:
RelViolation> 0.02.

Figure 4: Subspace detection property with different Î», D1 , d.
any single entry having too large influnce on the result. We established the sufficient conditions
for the algorithm to exactly detect the true subspace under the deterministic model and the random
model. Simulation results demonstrate that the proposed method is robust to irrelevant information
whereas the performance of original SSC and LASSO-SSC significantly deteriorates.
We now outline some directions of future research. An immediate future work is to study theoretical
guarantees of the proposed method under the semi-random model, where each subspace is chosen
deterministically, while samples are randomly distributed on the respective subspace. The challenge
here is to bound the subspace incoherence, previous methods uses the rotation invariance of the data,
which is not possible in our case as the robust inner product is invariant to rotations.
Acknowledgments
This work is partially supported by the Ministry of Education of Singapore AcRF Tier Two grant
R-265-000-443-112, and A*STAR SERC PSF grant R-265-000-540-305.

References
[1] Pankaj K Agarwal and Nabil H Mustafa. k-means projective clustering. In Proceedings of the
23rd ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages
8

155â€“165, 2004.
[2] Keith Ball. An elementary introduction to modern convex geometry. Flavors of geometry,
31:1â€“58, 1997.
[3] Emmanuel J CandeÌ€s, Xiaodong Li, Yi Ma, and John Wright. Robust principal component
analysis? Journal of the ACM, 58(3):11, 2011.
[4] Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust sparse regression under adversarial corruption. In Proceedings of the 30th International Conference on Machine Learning, pages 774â€“782, 2013.
[5] Yudong Chen, Ali Jalali, Sujay Sanghavi, and Huan Xu. Clustering partially observed graphs
via convex optimization. The Journal of Machine Learning Research, 15(1):2213â€“2238, 2014.
[6] Ehsan Elhamifar and ReneÌ Vidal. Sparse subspace clustering. In CVPR 2009, pages 2790â€“
2797.
[7] Guangcan Liu, Zhouchen Lin, and Yong Yu. Robust subspace segmentation by low-rank representation. In Proceedings of the 27th International Conference on Machine Learning, pages
663â€“670, 2010.
[8] Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing
data: Provable guarantees with non-convexity. In Advances in Neural Information Processing
Systems, pages 2726â€“2734, 2011.
[9] Le Lu and ReneÌ Vidal. Combined central and subspace clustering for computer vision applications. In Proceedings of the 23rd international conference on Machine learning, pages
593â€“600, 2006.
[10] Yi Ma, Harm Derksen, Wei Hong, and John Wright. Segmentation of multivariate mixed data
via lossy data coding and compression. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 29(9):1546â€“1562, 2007.
[11] Shankar R Rao, Roberto Tron, ReneÌ Vidal, and Yi Ma. Motion segmentation via robust subspace separation in the presence of outlying, incomplete, or corrupted trajectories. In CVPR
2008.
[12] Mahdi Soltanolkotabi, Emmanuel J Candes, et al. A geometric analysis of subspace clustering
with outliers. The Annals of Statistics, 40(4):2195â€“2238, 2012.
[13] Mahdi Soltanolkotabi, Ehsan Elhamifar, Emmanuel J Candes, et al. Robust subspace clustering. The Annals of Statistics, 42(2):669â€“699, 2014.
[14] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical
Society. Series B, pages 267â€“288, 1996.
[15] ReneÌ Vidal. A tutorial on subspace clustering. IEEE Signal Processing Magazine, 28(2):52â€“
68, 2010.
[16] Rene Vidal, Yi Ma, and Shankar Sastry. Generalized principal component analysis (gpca).
IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(12):1945â€“1959, 2005.
[17] ReneÌ Vidal, Roberto Tron, and Richard Hartley. Multiframe motion segmentation with missing
data using powerfactorization and gpca. International Journal of Computer Vision, 79(1):85â€“
105, 2008.
[18] Yu-Xiang Wang and Huan Xu. Noisy sparse subspace clustering. In Proceedings of The 30th
International Conference on Machine Learning, pages 89â€“97, 2013.
[19] H. Xu, C. Caramanis, and S. Sanghavi. Robust PCA via outlier pursuit. IEEE Transactions on
Information Theory, 58(5):3047â€“3064, 2012.
[20] Jingyu Yan and Marc Pollefeys. A general framework for motion segmentation: Independent,
articulated, rigid, non-rigid, degenerate and non-degenerate. In ECCV 2006, pages 94â€“106.
2006.
[21] Hao Zhu, Geert Leus, and Georgios B Giannakis. Sparsity-cognizant total least-squares for
perturbed compressive sampling. IEEE Transactions on Signal Processing, 59(5):2002â€“2016,
2011.

9

